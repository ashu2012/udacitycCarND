{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In my opinion this project truly highlights the importance of choosing proper training data as the model is very sensitive to the training data choice.  \n",
    "\n",
    "     In the begining I spent trying simple models with the data set provided by Udacity. I incorporated, left and right images , changing brigtness and flipping images using training generator as told in lectures but car always seems to go out of road after crossing the bridge.  \n",
    "     \n",
    "    In 2nd part i tried Nvidia model with the udacity data. Model is taking too much time so i added max pooling layer in Nvida model to do faster training but still car is going off road after crosing bridge. Model is able to run quite well in straight road with not so sharp turn but it fails to perform well for sharp turns. \n",
    "    \n",
    "    To solve above issue i collected data using simulator for sharp turns and as well as training data from mountain track. In training data i changed logic to add images which as steering angle more than 2 so that model should be able to learn important turns etc.  \n",
    "    \n",
    "    Also during final testing i observed car has velocity set to 9 but i was collecting simulator data at max speed around 30 . despite model is not learning steering angle with speed but i feeel it is important at what speed simulator data is collected as at higher speed less steering angle is sufficient at sharp turns while at slow speed higher steering angle is required at low speed. So itried collecting above data at speed 9 .    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "\n",
    "## Link to Udacity's sample data\n",
    "paths = \"data/\"\n",
    "os.chdir(r\"data/data\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/udacity/udacitycCarND/Behavioral Cloning/data/data\n",
      "/media/ashutosh/unix-extra1/udacity/udacitycCarND/Behavioral Cloning/data/data/IMG/*.jpg\n",
      "(320, 160) RGB\n",
      "(320, 160) RGB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Link to my collected  data\n",
    "path_mydata = \"data/\"\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "#Check size of images\n",
    "new_path = os.path.join(cwd, \"IMG/\", \"*.jpg\")\n",
    "print(new_path)\n",
    "for infile in glob.glob(new_path)[:2]:\n",
    "    im = Image.open(infile)\n",
    "    print(im.size, im.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import as a dataframe and plot steering\n",
    "df = pd.read_csv('driving_log.csv', header=0)\n",
    "df.columns = [\"center_image\", \"left_image\", \"right_image\", \"steering\", \"throttle\", \"break\", \"speed\"]\n",
    "df.drop(['throttle', 'break', 'speed'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f51cc113780>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51cc113048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.distplot(df['steering'], kde = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characteristics of the data:\n",
    "* All images are 320x160 pixels\n",
    "* The data is heavily skewed to zero steering as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8036\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftoList=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into training and validation samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(dftoList, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6428 1608\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_samples), len(validation_samples))\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We built training and validation generator using left , right and straight images. Also added random brightness and flipped images to have equal data for left as well as right steering data . Also most of the time car is going straight so i added logic that in every batch to model training add 50% images having angle less tha 2 and rest traing data having steering angle more than 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with train generator shared in the class and add image augmentations\n",
    "def train_generator(samples, batch_size=batch_size):\n",
    "    num_samples = len(samples)\n",
    "    print(num_samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        from sklearn.utils import shuffle\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            straight_count=0\n",
    "            images = []\n",
    "            angles = []\n",
    "            #print(batch_samples[0])\n",
    "            # Read center, left and right images from a folder containing Udacity data and my data\n",
    "            for sample_index,batch_sample in enumerate(batch_samples):\n",
    "                cwd = os.getcwd()\n",
    "                #print(cwd)\n",
    "                #print(batch_sample)\n",
    "                \n",
    "                #print(batch_sample)\n",
    "                center_angle = float(batch_sample[3])                \n",
    "                # Limit angles of less than absolute value of .1 to no more than 1/2 of data\n",
    "                # to reduce bias of car driving straight\n",
    "                if abs(center_angle) < .1:\n",
    "                    straight_count += 1\n",
    "                if straight_count > (batch_size * .5):\n",
    "                    while abs(batch_samples[sample_index][3]) < .1:\n",
    "                        sample_index = random.randrange(len(batch_samples))\n",
    "                    \n",
    "                    batch_sample=batch_samples[sample_index]\n",
    "                    \n",
    "                center_name = cwd+'/'+batch_sample[0].strip()#.split('/')[-1]\n",
    "                if Path(center_name).exists():\n",
    "                    center_image = cv2.imread(center_name)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(center_name))\n",
    "                    continue\n",
    "                #print(center_image.shape)\n",
    "                #center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                left_name = cwd+'/'+batch_sample[1].strip()#.split('/')[-1]\n",
    "                if Path(left_name).exists():\n",
    "                    left_image = cv2.imread(left_name)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(left_name))\n",
    "                    continue\n",
    "                #left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                right_name = cwd+'/'+batch_sample[2].strip()#.split('/')[-1]\n",
    "                if Path(right_name).exists():\n",
    "                    right_image = cv2.imread(right_name)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(batch_sample))\n",
    "                    continue\n",
    "                #right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "                \n",
    "                # Apply correction for left and right steering\n",
    "                correction = 0.20\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "\n",
    "                # Randomly include either center, left or right image\n",
    "                num = random.random()\n",
    "                #print(num)\n",
    "                if num <= 0.33:\n",
    "                    select_image = center_image\n",
    "                    select_angle = center_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                elif num>0.33 and num<=0.66:\n",
    "                    select_image = left_image\n",
    "                    select_angle = left_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                    #print(select_image)\n",
    "                else:\n",
    "                    select_image = right_image\n",
    "                    select_angle = right_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                    #print(select_image)\n",
    "\n",
    "                # Randomly horizontally flip selected images with 80% probability\n",
    "                keep_prob = random.random()\n",
    "                if keep_prob >0.20:\n",
    "                    #print(select_image)\n",
    "                    flip_image = np.fliplr(select_image)\n",
    "                    flip_angle = -1*select_angle\n",
    "                    images.append(flip_image)\n",
    "                    angles.append(flip_angle)\n",
    "\n",
    "                # Augment with images of different brightness\n",
    "                # Randomly select a percent change\n",
    "                change_pct = random.uniform(0.4, 1.2)\n",
    "\n",
    "                # Change to HSV to change the brightness V\n",
    "                hsv = cv2.cvtColor(select_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "                hsv[:, :, 2] = hsv[:, :, 2] * change_pct\n",
    "                # Convert back to RGB and append\n",
    "\n",
    "                bright_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "                images.append(bright_img)\n",
    "                angles.append(select_angle)\n",
    "\n",
    "                ## Randomly shear image with 80% probability\n",
    "                shear_prob = random.random()\n",
    "                if shear_prob >=0.20:\n",
    "                    shear_range = 40\n",
    "                    rows, cols, ch = select_image.shape\n",
    "                    dx = np.random.randint(-shear_range, shear_range + 1)\n",
    "                    #    print('dx',dx)\n",
    "                    random_point = [cols / 2 + dx, rows / 2]\n",
    "                    pts1 = np.float32([[0, rows], [cols, rows], [cols / 2, rows / 2]])\n",
    "                    pts2 = np.float32([[0, rows], [cols, rows], random_point])\n",
    "                    dsteering = dx / (rows / 2) * 360 / (2 * np.pi * 25.0) / 10.0\n",
    "                    M = cv2.getAffineTransform(pts1, pts2)\n",
    "                    shear_image = cv2.warpAffine(center_image, M, (cols, rows), borderMode=1)\n",
    "                    shear_angle = select_angle + dsteering\n",
    "                    images.append(shear_image)\n",
    "                    angles.append(shear_angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            print(X_train.shape)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "def valid_generator(samples, batch_size=batch_size):\n",
    "        num_samples = len(samples)\n",
    "        print(cwd)\n",
    "        while 1:  # Loop forever so the generator never terminates\n",
    "            from sklearn.utils import shuffle\n",
    "            shuffle(samples)\n",
    "            for offset in range(0, num_samples, batch_size):\n",
    "                batch_samples = samples[offset:offset + batch_size]\n",
    "\n",
    "                images = []\n",
    "                angles = []\n",
    "               \n",
    "                #Validation generator only has center images and no augmentations\n",
    "                for batch_sample in batch_samples:\n",
    "                    \n",
    "                    #print(batch_sample)\n",
    "                    center_name = cwd+'/' + batch_sample[0].strip()#.split('/')[-1]\n",
    "                    if Path(center_name).exists():\n",
    "                        center_image = cv2.imread(center_name)\n",
    "                    else:\n",
    "                        print(\"not Found:-\"+str(center_name))\n",
    "                        continue\n",
    "                    \n",
    "                    #center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    center_angle = float(batch_sample[3])\n",
    "\n",
    "                    images.append(center_image)\n",
    "                    angles.append(center_angle)\n",
    "\n",
    "                X_train = np.array(images)\n",
    "                y_train = np.array(angles)\n",
    "                print(X_train.shape)\n",
    "                yield shuffle(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_normalize(image):\n",
    "    import cv2\n",
    "    from keras.backend import tf as ktf   \n",
    "    \n",
    "    # resize to width 200 and high 66 liek recommended\n",
    "    # in the nvidia paper for the used CNN\n",
    "    # image = cv2.resize(image, (66, 200)) #first try\n",
    "    resized = ktf.image.resize_images(image, (64, 64))\n",
    "    #normalize 0-1\n",
    "    resized = resized/255.0 - 0.5\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Params\n",
    "row, col, ch = 160, 320, 3\n",
    "nb_classes = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(row, col, ch)))\n",
    "# Crop pixels from top and bottom of image\n",
    "model.add(Cropping2D(cropping=((60, 20), (0, 0))))\n",
    "\n",
    "# Resise data within the neural network\n",
    "model.add(Lambda(resize_normalize))\n",
    "\n",
    "# First convolution layer so the model can automatically figure out the best color space for the hypothesis\n",
    "model.add(Convolution2D(3, 1, 1, border_mode='same', name='color_conv'))\n",
    "\n",
    "# CNN model\n",
    "\n",
    "model.add(Convolution2D(32, 3,3 ,border_mode='same', subsample=(2,2), name='conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1), name='pool1'))\n",
    "\n",
    "model.add(Convolution2D(64, 3,3 ,border_mode='same',subsample=(2,2), name='conv2'))\n",
    "model.add(Activation('relu',name='relu2'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), name='pool2'))\n",
    "\n",
    "model.add(Convolution2D(128, 3,3,border_mode='same',subsample=(1,1), name='conv3'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), name='pool3'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, name='dense1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, name='dense2'))\n",
    "\n",
    "model.add(Dense(1,name='output'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr= 0.0001), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 162, 322, 3)   0           zeropadding2d_input_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)        (None, 82, 322, 3)    0           zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)               (None, 64, 64, 3)     0           cropping2d_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "color_conv (Convolution2D)       (None, 64, 64, 3)     12          lambda_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Convolution2D)            (None, 32, 32, 32)    896         color_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 32, 32, 32)    0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 31, 31, 32)    0           activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Convolution2D)            (None, 16, 16, 64)    18496       pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu2 (Activation)               (None, 16, 16, 64)    0           conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 8, 8, 64)      0           relu2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Convolution2D)            (None, 8, 8, 128)     73856       pool2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 8, 8, 128)     0           conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)             (None, 4, 4, 128)     0           activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 2048)          0           pool3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 2048)          0           flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense1 (Dense)                   (None, 128)           262272      dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 128)           0           dense1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 128)           0           activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense2 (Dense)                   (None, 128)           16512       dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             129         dense2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 372,173\n",
      "Trainable params: 372,173\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save every model using Keras checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"check-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_epoch = 5\n",
    "samples_per_epoch = 2000\n",
    "nb_val_samples = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = train_generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = valid_generator(validation_samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6428\n",
      "Epoch 1/15\n",
      "(469, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      "(472, 160, 320, 3)\n",
      "  469/20000 [..............................] - ETA: 108s - loss: 0.0521(458, 160, 320, 3)\n",
      "  925/20000 [>.............................] - ETA: 87s - loss: 0.0590 (452, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      " 1397/20000 [=>............................] - ETA: 80s - loss: 0.0575(465, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 1855/20000 [=>............................] - ETA: 76s - loss: 0.0554(464, 160, 320, 3)\n",
      "(461, 160, 320, 3)\n",
      " 2307/20000 [==>...........................] - ETA: 72s - loss: 0.0545(448, 160, 320, 3)\n",
      " 2764/20000 [===>..........................] - ETA: 69s - loss: 0.0544(470, 160, 320, 3)\n",
      "(450, 160, 320, 3)\n",
      " 3229/20000 [===>..........................] - ETA: 83s - loss: 0.0538(471, 160, 320, 3)\n",
      "(459, 160, 320, 3)\n",
      " 3685/20000 [====>.........................] - ETA: 101s - loss: 0.0519(451, 160, 320, 3)\n",
      " 4149/20000 [=====>........................] - ETA: 114s - loss: 0.0506(467, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      " 4610/20000 [=====>........................] - ETA: 120s - loss: 0.0495(466, 160, 320, 3)\n",
      "(455, 160, 320, 3)\n",
      " 5058/20000 [======>.......................] - ETA: 124s - loss: 0.0487(463, 160, 320, 3)\n",
      " 5528/20000 [=======>......................] - ETA: 127s - loss: 0.0473(460, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      " 5978/20000 [=======>......................] - ETA: 127s - loss: 0.0467(461, 160, 320, 3)\n",
      " 6449/20000 [========>.....................] - ETA: 127s - loss: 0.0460(467, 160, 320, 3)\n",
      " 6908/20000 [=========>....................] - ETA: 126s - loss: 0.0460(466, 160, 320, 3)\n",
      " 7359/20000 [==========>...................] - ETA: 125s - loss: 0.0457(450, 160, 320, 3)\n",
      " 7826/20000 [==========>...................] - ETA: 125s - loss: 0.0459(459, 160, 320, 3)\n",
      " 8288/20000 [===========>..................] - ETA: 122s - loss: 0.0464(458, 160, 320, 3)\n",
      " 8754/20000 [============>.................] - ETA: 120s - loss: 0.0464(464, 160, 320, 3)\n",
      " 9209/20000 [============>.................] - ETA: 117s - loss: 0.0460(462, 160, 320, 3)\n",
      " 9672/20000 [=============>................] - ETA: 113s - loss: 0.0455(468, 160, 320, 3)\n",
      "10132/20000 [==============>...............] - ETA: 109s - loss: 0.0454(462, 160, 320, 3)\n",
      "10594/20000 [==============>...............] - ETA: 105s - loss: 0.0447(469, 160, 320, 3)\n",
      "11055/20000 [===============>..............] - ETA: 101s - loss: 0.0446(456, 160, 320, 3)\n",
      "11522/20000 [================>.............] - ETA: 96s - loss: 0.0442 (462, 160, 320, 3)\n",
      "11988/20000 [================>.............] - ETA: 91s - loss: 0.0439(461, 160, 320, 3)\n",
      "12438/20000 [=================>............] - ETA: 87s - loss: 0.0436(464, 160, 320, 3)\n",
      "12897/20000 [==================>...........] - ETA: 82s - loss: 0.0436(461, 160, 320, 3)\n",
      "13355/20000 [===================>..........] - ETA: 77s - loss: 0.0433(472, 160, 320, 3)\n",
      "13819/20000 [===================>..........] - ETA: 72s - loss: 0.0433(464, 160, 320, 3)\n",
      "14281/20000 [====================>.........] - ETA: 67s - loss: 0.0431(463, 160, 320, 3)\n",
      "14749/20000 [=====================>........] - ETA: 62s - loss: 0.0432(469, 160, 320, 3)\n",
      "15211/20000 [=====================>........] - ETA: 57s - loss: 0.0429(468, 160, 320, 3)\n",
      "15680/20000 [======================>.......] - ETA: 51s - loss: 0.0432(461, 160, 320, 3)\n",
      "16136/20000 [=======================>......] - ETA: 45s - loss: 0.0432(465, 160, 320, 3)\n",
      "16598/20000 [=======================>......] - ETA: 39s - loss: 0.0431(466, 160, 320, 3)\n",
      "17059/20000 [========================>.....] - ETA: 33s - loss: 0.0430(461, 160, 320, 3)\n",
      "17523/20000 [=========================>....] - ETA: 27s - loss: 0.0432(459, 160, 320, 3)\n",
      "17984/20000 [=========================>....] - ETA: 22s - loss: 0.0430(456, 160, 320, 3)\n",
      "18456/20000 [==========================>...] - ETA: 16s - loss: 0.0427(105, 160, 320, 3)\n",
      "18920/20000 [===========================>..] - ETA: 11s - loss: 0.0424(466, 160, 320, 3)\n",
      "19383/20000 [============================>.] - ETA: 6s - loss: 0.0423 (465, 160, 320, 3)\n",
      "19852/20000 [============================>.] - ETA: 1s - loss: 0.0423(448, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/udacity/udacitycCarND/Behavioral Cloning/data\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00000: saving model to check-00-0.0128.hdf5\n",
      "20320/20000 [==============================] - 214s - loss: 0.0421 - val_loss: 0.0128\n",
      "Epoch 2/15\n",
      "(458, 160, 320, 3)\n",
      "  461/20000 [..............................] - ETA: 77s - loss: 0.0313(471, 160, 320, 3)\n",
      "  926/20000 [>.............................] - ETA: 75s - loss: 0.0329(469, 160, 320, 3)\n",
      " 1392/20000 [=>............................] - ETA: 71s - loss: 0.0317(458, 160, 320, 3)\n",
      " 1853/20000 [=>............................] - ETA: 69s - loss: 0.0333(464, 160, 320, 3)\n",
      " 2312/20000 [==>...........................] - ETA: 67s - loss: 0.0339(461, 160, 320, 3)\n",
      " 2873/20000 [===>..........................] - ETA: 65s - loss: 0.0359(460, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 3339/20000 [====>.........................] - ETA: 67s - loss: 0.0356(470, 160, 320, 3)\n",
      " 3804/20000 [====>.........................] - ETA: 65s - loss: 0.0352(459, 160, 320, 3)\n",
      " 4252/20000 [=====>........................] - ETA: 62s - loss: 0.0350(462, 160, 320, 3)\n",
      " 4710/20000 [======>.......................] - ETA: 60s - loss: 0.0347(464, 160, 320, 3)\n",
      " 5181/20000 [======>.......................] - ETA: 58s - loss: 0.0345(455, 160, 320, 3)\n",
      " 5650/20000 [=======>......................] - ETA: 56s - loss: 0.0348(453, 160, 320, 3)\n",
      " 6108/20000 [========>.....................] - ETA: 54s - loss: 0.0347(459, 160, 320, 3)\n",
      " 6572/20000 [========>.....................] - ETA: 52s - loss: 0.0347(456, 160, 320, 3)\n",
      " 7033/20000 [=========>....................] - ETA: 50s - loss: 0.0342(464, 160, 320, 3)\n",
      " 7493/20000 [==========>...................] - ETA: 48s - loss: 0.0342(454, 160, 320, 3)\n",
      " 7949/20000 [==========>...................] - ETA: 47s - loss: 0.0340(465, 160, 320, 3)\n",
      " 8419/20000 [===========>..................] - ETA: 45s - loss: 0.0340(462, 160, 320, 3)\n",
      " 8878/20000 [============>.................] - ETA: 43s - loss: 0.0337(452, 160, 320, 3)\n",
      " 9340/20000 [=============>................] - ETA: 41s - loss: 0.0336(467, 160, 320, 3)\n",
      " 9804/20000 [=============>................] - ETA: 40s - loss: 0.0340(465, 160, 320, 3)\n",
      "10259/20000 [==============>...............] - ETA: 38s - loss: 0.0344(453, 160, 320, 3)\n",
      "10712/20000 [===============>..............] - ETA: 36s - loss: 0.0343(461, 160, 320, 3)\n",
      "11171/20000 [===============>..............] - ETA: 34s - loss: 0.0343(465, 160, 320, 3)\n",
      "11627/20000 [================>.............] - ETA: 32s - loss: 0.0346(463, 160, 320, 3)\n",
      "12091/20000 [=================>............] - ETA: 31s - loss: 0.0345(462, 160, 320, 3)\n",
      "12545/20000 [=================>............] - ETA: 29s - loss: 0.0345(457, 160, 320, 3)\n",
      "13010/20000 [==================>...........] - ETA: 27s - loss: 0.0345(460, 160, 320, 3)\n",
      "13472/20000 [===================>..........] - ETA: 25s - loss: 0.0342(459, 160, 320, 3)\n",
      "13924/20000 [===================>..........] - ETA: 23s - loss: 0.0342(466, 160, 320, 3)\n",
      "14391/20000 [====================>.........] - ETA: 22s - loss: 0.0342(460, 160, 320, 3)\n",
      "14856/20000 [=====================>........] - ETA: 20s - loss: 0.0339(450, 160, 320, 3)\n",
      "15309/20000 [=====================>........] - ETA: 18s - loss: 0.0337(458, 160, 320, 3)\n",
      "15770/20000 [======================>.......] - ETA: 16s - loss: 0.0338(457, 160, 320, 3)\n",
      "16235/20000 [=======================>......] - ETA: 14s - loss: 0.0338(459, 160, 320, 3)\n",
      "16698/20000 [========================>.....] - ETA: 13s - loss: 0.0338(456, 160, 320, 3)\n",
      "17160/20000 [========================>.....] - ETA: 11s - loss: 0.0336(470, 160, 320, 3)\n",
      "17617/20000 [=========================>....] - ETA: 9s - loss: 0.0336 (449, 160, 320, 3)\n",
      "18077/20000 [==========================>...] - ETA: 7s - loss: 0.0335(460, 160, 320, 3)\n",
      "18536/20000 [==========================>...] - ETA: 5s - loss: 0.0335(463, 160, 320, 3)\n",
      "19002/20000 [===========================>..] - ETA: 3s - loss: 0.0334(451, 160, 320, 3)\n",
      "19462/20000 [============================>.] - ETA: 2s - loss: 0.0335(468, 160, 320, 3)\n",
      "19912/20000 [============================>.] - ETA: 0s - loss: 0.0334(456, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00001: saving model to check-01-0.0124.hdf5\n",
      "20370/20000 [==============================] - 86s - loss: 0.0337 - val_loss: 0.0124\n",
      "Epoch 3/15\n",
      "(460, 160, 320, 3)\n",
      "  457/20000 [..............................] - ETA: 85s - loss: 0.0316(458, 160, 320, 3)\n",
      "  916/20000 [>.............................] - ETA: 83s - loss: 0.0280(101, 160, 320, 3)\n",
      " 1372/20000 [=>............................] - ETA: 80s - loss: 0.0273(461, 160, 320, 3)\n",
      " 1842/20000 [=>............................] - ETA: 77s - loss: 0.0288(449, 160, 320, 3)\n",
      " 2291/20000 [==>...........................] - ETA: 74s - loss: 0.0309(466, 160, 320, 3)\n",
      " 2751/20000 [===>..........................] - ETA: 71s - loss: 0.0314(459, 160, 320, 3)\n",
      " 3214/20000 [===>..........................] - ETA: 67s - loss: 0.0315(469, 160, 320, 3)\n",
      " 3665/20000 [====>.........................] - ETA: 65s - loss: 0.0311(458, 160, 320, 3)\n",
      " 4133/20000 [=====>........................] - ETA: 62s - loss: 0.0308(457, 160, 320, 3)\n",
      " 4589/20000 [=====>........................] - ETA: 59s - loss: 0.0306(469, 160, 320, 3)\n",
      " 5049/20000 [======>.......................] - ETA: 57s - loss: 0.0307(459, 160, 320, 3)\n",
      " 5608/20000 [=======>......................] - ETA: 55s - loss: 0.0316(460, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      " 6069/20000 [========>.....................] - ETA: 53s - loss: 0.0314(480, 160, 320, 3)\n",
      " 6518/20000 [========>.....................] - ETA: 51s - loss: 0.0318(451, 160, 320, 3)\n",
      " 6984/20000 [=========>....................] - ETA: 49s - loss: 0.0317(464, 160, 320, 3)\n",
      " 7443/20000 [==========>...................] - ETA: 47s - loss: 0.0312(457, 160, 320, 3)\n",
      " 7912/20000 [==========>...................] - ETA: 45s - loss: 0.0310(445, 160, 320, 3)\n",
      " 8370/20000 [===========>..................] - ETA: 43s - loss: 0.0309(468, 160, 320, 3)\n",
      " 8827/20000 [============>.................] - ETA: 41s - loss: 0.0311(454, 160, 320, 3)\n",
      " 9296/20000 [============>.................] - ETA: 39s - loss: 0.0311(466, 160, 320, 3)\n",
      " 9755/20000 [=============>................] - ETA: 37s - loss: 0.0309(463, 160, 320, 3)\n",
      "10215/20000 [==============>...............] - ETA: 36s - loss: 0.0307(459, 160, 320, 3)\n",
      "10677/20000 [===============>..............] - ETA: 34s - loss: 0.0305(462, 160, 320, 3)\n",
      "11157/20000 [===============>..............] - ETA: 32s - loss: 0.0304(464, 160, 320, 3)\n",
      "11608/20000 [================>.............] - ETA: 30s - loss: 0.0303(459, 160, 320, 3)\n",
      "12072/20000 [=================>............] - ETA: 28s - loss: 0.0302(456, 160, 320, 3)\n",
      "12529/20000 [=================>............] - ETA: 27s - loss: 0.0302(461, 160, 320, 3)\n",
      "12974/20000 [==================>...........] - ETA: 25s - loss: 0.0302(455, 160, 320, 3)\n",
      "13442/20000 [===================>..........] - ETA: 23s - loss: 0.0304(467, 160, 320, 3)\n",
      "13896/20000 [===================>..........] - ETA: 22s - loss: 0.0306(460, 160, 320, 3)\n",
      "14362/20000 [====================>.........] - ETA: 20s - loss: 0.0308(468, 160, 320, 3)\n",
      "14825/20000 [=====================>........] - ETA: 18s - loss: 0.0307(461, 160, 320, 3)\n",
      "15284/20000 [=====================>........] - ETA: 17s - loss: 0.0306(461, 160, 320, 3)\n",
      "15746/20000 [======================>.......] - ETA: 15s - loss: 0.0306(458, 160, 320, 3)\n",
      "16210/20000 [=======================>......] - ETA: 13s - loss: 0.0304(453, 160, 320, 3)\n",
      "16669/20000 [========================>.....] - ETA: 12s - loss: 0.0305(465, 160, 320, 3)\n",
      "17125/20000 [========================>.....] - ETA: 10s - loss: 0.0304(470, 160, 320, 3)\n",
      "17586/20000 [=========================>....] - ETA: 8s - loss: 0.0304 (455, 160, 320, 3)\n",
      "18041/20000 [==========================>...] - ETA: 7s - loss: 0.0303(461, 160, 320, 3)\n",
      "18508/20000 [==========================>...] - ETA: 5s - loss: 0.0303(461, 160, 320, 3)\n",
      "18968/20000 [===========================>..] - ETA: 3s - loss: 0.0303(473, 160, 320, 3)\n",
      "19436/20000 [============================>.] - ETA: 2s - loss: 0.0304(455, 160, 320, 3)\n",
      "19897/20000 [============================>.] - ETA: 0s - loss: 0.0303(461, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00002: saving model to check-02-0.0123.hdf5\n",
      "20358/20000 [==============================] - 79s - loss: 0.0303 - val_loss: 0.0123\n",
      "Epoch 4/15\n",
      "(461, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 64s - loss: 0.0280(464, 160, 320, 3)\n",
      "  911/20000 [>.............................] - ETA: 79s - loss: 0.0344(454, 160, 320, 3)\n",
      " 1376/20000 [=>............................] - ETA: 73s - loss: 0.0329(461, 160, 320, 3)\n",
      " 1846/20000 [=>............................] - ETA: 76s - loss: 0.0329(467, 160, 320, 3)\n",
      " 2301/20000 [==>...........................] - ETA: 71s - loss: 0.0319(458, 160, 320, 3)\n",
      " 2762/20000 [===>..........................] - ETA: 70s - loss: 0.0337(458, 160, 320, 3)\n",
      " 3223/20000 [===>..........................] - ETA: 68s - loss: 0.0337(456, 160, 320, 3)\n",
      " 3696/20000 [====>.........................] - ETA: 66s - loss: 0.0322(103, 160, 320, 3)\n",
      " 4151/20000 [=====>........................] - ETA: 63s - loss: 0.0312(455, 160, 320, 3)\n",
      " 4612/20000 [=====>........................] - ETA: 61s - loss: 0.0308(466, 160, 320, 3)\n",
      " 5073/20000 [======>.......................] - ETA: 59s - loss: 0.0311(468, 160, 320, 3)\n",
      " 5537/20000 [=======>......................] - ETA: 56s - loss: 0.0310(452, 160, 320, 3)\n",
      " 5991/20000 [=======>......................] - ETA: 54s - loss: 0.0311(469, 160, 320, 3)\n",
      " 6452/20000 [========>.....................] - ETA: 52s - loss: 0.0311(452, 160, 320, 3)\n",
      " 6919/20000 [=========>....................] - ETA: 50s - loss: 0.0306(464, 160, 320, 3)\n",
      " 7377/20000 [==========>...................] - ETA: 48s - loss: 0.0301(466, 160, 320, 3)\n",
      " 7835/20000 [==========>...................] - ETA: 46s - loss: 0.0302(458, 160, 320, 3)\n",
      " 8394/20000 [===========>..................] - ETA: 44s - loss: 0.0309(450, 160, 320, 3)\n",
      "(466, 160, 320, 3)\n",
      " 8849/20000 [============>.................] - ETA: 42s - loss: 0.0305(462, 160, 320, 3)\n",
      " 9315/20000 [============>.................] - ETA: 40s - loss: 0.0304(454, 160, 320, 3)\n",
      " 9783/20000 [=============>................] - ETA: 38s - loss: 0.0301(466, 160, 320, 3)\n",
      "10235/20000 [==============>...............] - ETA: 36s - loss: 0.0299(456, 160, 320, 3)\n",
      "10704/20000 [===============>..............] - ETA: 34s - loss: 0.0298(474, 160, 320, 3)\n",
      "11156/20000 [===============>..............] - ETA: 33s - loss: 0.0296(450, 160, 320, 3)\n",
      "11620/20000 [================>.............] - ETA: 31s - loss: 0.0296(459, 160, 320, 3)\n",
      "12086/20000 [=================>............] - ETA: 29s - loss: 0.0296(462, 160, 320, 3)\n",
      "12544/20000 [=================>............] - ETA: 27s - loss: 0.0295(448, 160, 320, 3)\n",
      "12994/20000 [==================>...........] - ETA: 25s - loss: 0.0295(459, 160, 320, 3)\n",
      "13460/20000 [===================>..........] - ETA: 24s - loss: 0.0293(457, 160, 320, 3)\n",
      "13922/20000 [===================>..........] - ETA: 22s - loss: 0.0292(459, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 20s - loss: 0.0291(451, 160, 320, 3)\n",
      "14842/20000 [=====================>........] - ETA: 18s - loss: 0.0289(465, 160, 320, 3)\n",
      "15298/20000 [=====================>........] - ETA: 17s - loss: 0.0290(460, 160, 320, 3)\n",
      "15772/20000 [======================>.......] - ETA: 15s - loss: 0.0295(458, 160, 320, 3)\n",
      "16222/20000 [=======================>......] - ETA: 13s - loss: 0.0294(465, 160, 320, 3)\n",
      "16681/20000 [========================>.....] - ETA: 12s - loss: 0.0296(454, 160, 320, 3)\n",
      "17143/20000 [========================>.....] - ETA: 10s - loss: 0.0295(471, 160, 320, 3)\n",
      "17591/20000 [=========================>....] - ETA: 8s - loss: 0.0296 (461, 160, 320, 3)\n",
      "18050/20000 [==========================>...] - ETA: 7s - loss: 0.0294(460, 160, 320, 3)\n",
      "18507/20000 [==========================>...] - ETA: 5s - loss: 0.0295(458, 160, 320, 3)\n",
      "18966/20000 [===========================>..] - ETA: 3s - loss: 0.0292(455, 160, 320, 3)\n",
      "19417/20000 [============================>.] - ETA: 2s - loss: 0.0292(470, 160, 320, 3)\n",
      "19882/20000 [============================>.] - ETA: 0s - loss: 0.0291(448, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00003: saving model to check-03-0.0120.hdf5\n",
      "20342/20000 [==============================] - 79s - loss: 0.0290 - val_loss: 0.0120\n",
      "Epoch 5/15\n",
      "(453, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 70s - loss: 0.0281(451, 160, 320, 3)\n",
      "  923/20000 [>.............................] - ETA: 69s - loss: 0.0291(457, 160, 320, 3)\n",
      " 1377/20000 [=>............................] - ETA: 69s - loss: 0.0278(453, 160, 320, 3)\n",
      " 1848/20000 [=>............................] - ETA: 68s - loss: 0.0276(473, 160, 320, 3)\n",
      " 2309/20000 [==>...........................] - ETA: 66s - loss: 0.0267(459, 160, 320, 3)\n",
      " 2769/20000 [===>..........................] - ETA: 64s - loss: 0.0267(465, 160, 320, 3)\n",
      " 3227/20000 [===>..........................] - ETA: 62s - loss: 0.0263(455, 160, 320, 3)\n",
      " 3682/20000 [====>.........................] - ETA: 60s - loss: 0.0276(468, 160, 320, 3)\n",
      " 4152/20000 [=====>........................] - ETA: 58s - loss: 0.0279(468, 160, 320, 3)\n",
      " 4600/20000 [=====>........................] - ETA: 56s - loss: 0.0283(466, 160, 320, 3)\n",
      " 5053/20000 [======>.......................] - ETA: 55s - loss: 0.0282(473, 160, 320, 3)\n",
      " 5504/20000 [=======>......................] - ETA: 53s - loss: 0.0293(463, 160, 320, 3)\n",
      " 5961/20000 [=======>......................] - ETA: 51s - loss: 0.0293(452, 160, 320, 3)\n",
      " 6414/20000 [========>.....................] - ETA: 49s - loss: 0.0290(104, 160, 320, 3)\n",
      " 6887/20000 [=========>....................] - ETA: 47s - loss: 0.0286(457, 160, 320, 3)\n",
      " 7346/20000 [==========>...................] - ETA: 46s - loss: 0.0284(473, 160, 320, 3)\n",
      " 7811/20000 [==========>...................] - ETA: 44s - loss: 0.0284(469, 160, 320, 3)\n",
      " 8266/20000 [===========>..................] - ETA: 42s - loss: 0.0282(463, 160, 320, 3)\n",
      " 8734/20000 [============>.................] - ETA: 40s - loss: 0.0280(448, 160, 320, 3)\n",
      " 9202/20000 [============>.................] - ETA: 39s - loss: 0.0278(457, 160, 320, 3)\n",
      " 9668/20000 [=============>................] - ETA: 37s - loss: 0.0277(456, 160, 320, 3)\n",
      "10141/20000 [==============>...............] - ETA: 35s - loss: 0.0275(463, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10604/20000 [==============>...............] - ETA: 33s - loss: 0.0274(453, 160, 320, 3)\n",
      "11160/20000 [===============>..............] - ETA: 31s - loss: 0.0278(449, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      "11617/20000 [================>.............] - ETA: 30s - loss: 0.0278(458, 160, 320, 3)\n",
      "12090/20000 [=================>............] - ETA: 28s - loss: 0.0277(453, 160, 320, 3)\n",
      "12559/20000 [=================>............] - ETA: 26s - loss: 0.0277(462, 160, 320, 3)\n",
      "13022/20000 [==================>...........] - ETA: 25s - loss: 0.0276(446, 160, 320, 3)\n",
      "13470/20000 [===================>..........] - ETA: 23s - loss: 0.0275(463, 160, 320, 3)\n",
      "13927/20000 [===================>..........] - ETA: 21s - loss: 0.0274(455, 160, 320, 3)\n",
      "14383/20000 [====================>.........] - ETA: 20s - loss: 0.0274(461, 160, 320, 3)\n",
      "14846/20000 [=====================>........] - ETA: 18s - loss: 0.0274(463, 160, 320, 3)\n",
      "15299/20000 [=====================>........] - ETA: 16s - loss: 0.0273(455, 160, 320, 3)\n",
      "15748/20000 [======================>.......] - ETA: 15s - loss: 0.0272(465, 160, 320, 3)\n",
      "16210/20000 [=======================>......] - ETA: 13s - loss: 0.0271(472, 160, 320, 3)\n",
      "16668/20000 [========================>.....] - ETA: 11s - loss: 0.0270(462, 160, 320, 3)\n",
      "17121/20000 [========================>.....] - ETA: 10s - loss: 0.0269(461, 160, 320, 3)\n",
      "17583/20000 [=========================>....] - ETA: 8s - loss: 0.0269 (455, 160, 320, 3)\n",
      "18029/20000 [==========================>...] - ETA: 7s - loss: 0.0269(459, 160, 320, 3)\n",
      "18492/20000 [==========================>...] - ETA: 5s - loss: 0.0271(458, 160, 320, 3)\n",
      "18947/20000 [===========================>..] - ETA: 3s - loss: 0.0272(467, 160, 320, 3)\n",
      "19408/20000 [============================>.] - ETA: 2s - loss: 0.0272(460, 160, 320, 3)\n",
      "19871/20000 [============================>.] - ETA: 0s - loss: 0.0273(459, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00004: saving model to check-04-0.0113.hdf5\n",
      "20326/20000 [==============================] - 80s - loss: 0.0273 - val_loss: 0.0113\n",
      "Epoch 6/15\n",
      "(466, 160, 320, 3)\n",
      "  465/20000 [..............................] - ETA: 73s - loss: 0.0232(467, 160, 320, 3)\n",
      "  937/20000 [>.............................] - ETA: 69s - loss: 0.0264(466, 160, 320, 3)\n",
      " 1399/20000 [=>............................] - ETA: 67s - loss: 0.0241(460, 160, 320, 3)\n",
      " 1860/20000 [=>............................] - ETA: 66s - loss: 0.0242(460, 160, 320, 3)\n",
      " 2315/20000 [==>...........................] - ETA: 64s - loss: 0.0241(465, 160, 320, 3)\n",
      " 2774/20000 [===>..........................] - ETA: 63s - loss: 0.0245(465, 160, 320, 3)\n",
      " 3232/20000 [===>..........................] - ETA: 62s - loss: 0.0246(446, 160, 320, 3)\n",
      " 3699/20000 [====>.........................] - ETA: 61s - loss: 0.0246(466, 160, 320, 3)\n",
      " 4159/20000 [=====>........................] - ETA: 59s - loss: 0.0244(452, 160, 320, 3)\n",
      " 4618/20000 [=====>........................] - ETA: 58s - loss: 0.0243(456, 160, 320, 3)\n",
      " 5084/20000 [======>.......................] - ETA: 55s - loss: 0.0246(457, 160, 320, 3)\n",
      " 5551/20000 [=======>......................] - ETA: 53s - loss: 0.0246(464, 160, 320, 3)\n",
      " 6017/20000 [========>.....................] - ETA: 53s - loss: 0.0246(455, 160, 320, 3)\n",
      " 6477/20000 [========>.....................] - ETA: 50s - loss: 0.0253(471, 160, 320, 3)\n",
      " 6937/20000 [=========>....................] - ETA: 50s - loss: 0.0255(459, 160, 320, 3)\n",
      " 7402/20000 [==========>...................] - ETA: 48s - loss: 0.0256(463, 160, 320, 3)\n",
      " 7867/20000 [==========>...................] - ETA: 47s - loss: 0.0257(461, 160, 320, 3)\n",
      " 8313/20000 [===========>..................] - ETA: 44s - loss: 0.0258(452, 160, 320, 3)\n",
      " 8779/20000 [============>.................] - ETA: 43s - loss: 0.0259(471, 160, 320, 3)\n",
      " 9231/20000 [============>.................] - ETA: 41s - loss: 0.0257(103, 160, 320, 3)\n",
      " 9687/20000 [=============>................] - ETA: 40s - loss: 0.0254(446, 160, 320, 3)\n",
      "10144/20000 [==============>...............] - ETA: 38s - loss: 0.0255(457, 160, 320, 3)\n",
      "10608/20000 [==============>...............] - ETA: 36s - loss: 0.0256(462, 160, 320, 3)\n",
      "11063/20000 [===============>..............] - ETA: 34s - loss: 0.0256(465, 160, 320, 3)\n",
      "11534/20000 [================>.............] - ETA: 33s - loss: 0.0256(455, 160, 320, 3)\n",
      "11993/20000 [================>.............] - ETA: 31s - loss: 0.0256(463, 160, 320, 3)\n",
      "12456/20000 [=================>............] - ETA: 29s - loss: 0.0254(456, 160, 320, 3)\n",
      "12917/20000 [==================>...........] - ETA: 28s - loss: 0.0254(464, 160, 320, 3)\n",
      "13369/20000 [===================>..........] - ETA: 26s - loss: 0.0253(469, 160, 320, 3)\n",
      "13943/20000 [===================>..........] - ETA: 24s - loss: 0.0257(465, 160, 320, 3)\n",
      "(465, 160, 320, 3)\n",
      "14389/20000 [====================>.........] - ETA: 22s - loss: 0.0255(452, 160, 320, 3)\n",
      "14846/20000 [=====================>........] - ETA: 20s - loss: 0.0255(461, 160, 320, 3)\n",
      "15308/20000 [=====================>........] - ETA: 18s - loss: 0.0255(456, 160, 320, 3)\n",
      "15773/20000 [======================>.......] - ETA: 16s - loss: 0.0253(455, 160, 320, 3)\n",
      "16228/20000 [=======================>......] - ETA: 15s - loss: 0.0252(461, 160, 320, 3)\n",
      "16691/20000 [========================>.....] - ETA: 13s - loss: 0.0252(468, 160, 320, 3)\n",
      "17147/20000 [========================>.....] - ETA: 11s - loss: 0.0252(463, 160, 320, 3)\n",
      "17611/20000 [=========================>....] - ETA: 9s - loss: 0.0252 (465, 160, 320, 3)\n",
      "18080/20000 [==========================>...] - ETA: 7s - loss: 0.0251(477, 160, 320, 3)\n",
      "18545/20000 [==========================>...] - ETA: 5s - loss: 0.0252(458, 160, 320, 3)\n",
      "19010/20000 [===========================>..] - ETA: 3s - loss: 0.0252(467, 160, 320, 3)\n",
      "19462/20000 [============================>.] - ETA: 2s - loss: 0.0252(450, 160, 320, 3)\n",
      "19923/20000 [============================>.] - ETA: 0s - loss: 0.0251(469, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00005: saving model to check-05-0.0108.hdf5\n",
      "20379/20000 [==============================] - 89s - loss: 0.0251 - val_loss: 0.0108\n",
      "Epoch 7/15\n",
      "(464, 160, 320, 3)\n",
      "  455/20000 [..............................] - ETA: 97s - loss: 0.0309(461, 160, 320, 3)\n",
      "  916/20000 [>.............................] - ETA: 81s - loss: 0.0325(459, 160, 320, 3)\n",
      " 1384/20000 [=>............................] - ETA: 83s - loss: 0.0311(475, 160, 320, 3)\n",
      " 1847/20000 [=>............................] - ETA: 76s - loss: 0.0306(459, 160, 320, 3)\n",
      " 2312/20000 [==>...........................] - ETA: 76s - loss: 0.0292(459, 160, 320, 3)\n",
      " 2789/20000 [===>..........................] - ETA: 72s - loss: 0.0286(468, 160, 320, 3)\n",
      " 3247/20000 [===>..........................] - ETA: 72s - loss: 0.0277(469, 160, 320, 3)\n",
      " 3714/20000 [====>.........................] - ETA: 68s - loss: 0.0280(462, 160, 320, 3)\n",
      " 4164/20000 [=====>........................] - ETA: 65s - loss: 0.0269(455, 160, 320, 3)\n",
      " 4633/20000 [=====>........................] - ETA: 62s - loss: 0.0262(473, 160, 320, 3)\n",
      " 5097/20000 [======>.......................] - ETA: 61s - loss: 0.0258(459, 160, 320, 3)\n",
      " 5558/20000 [=======>......................] - ETA: 60s - loss: 0.0253(459, 160, 320, 3)\n",
      " 6017/20000 [========>.....................] - ETA: 58s - loss: 0.0252(460, 160, 320, 3)\n",
      " 6492/20000 [========>.....................] - ETA: 55s - loss: 0.0251(462, 160, 320, 3)\n",
      " 6951/20000 [=========>....................] - ETA: 54s - loss: 0.0250(466, 160, 320, 3)\n",
      " 7410/20000 [==========>...................] - ETA: 51s - loss: 0.0250(460, 160, 320, 3)\n",
      " 7878/20000 [==========>...................] - ETA: 50s - loss: 0.0250(455, 160, 320, 3)\n",
      " 8347/20000 [===========>..................] - ETA: 48s - loss: 0.0252(461, 160, 320, 3)\n",
      " 8809/20000 [============>.................] - ETA: 46s - loss: 0.0249(465, 160, 320, 3)\n",
      " 9264/20000 [============>.................] - ETA: 44s - loss: 0.0250(460, 160, 320, 3)\n",
      " 9737/20000 [=============>................] - ETA: 42s - loss: 0.0250(454, 160, 320, 3)\n",
      "10196/20000 [==============>...............] - ETA: 40s - loss: 0.0253(458, 160, 320, 3)\n",
      "10655/20000 [==============>...............] - ETA: 39s - loss: 0.0253(454, 160, 320, 3)\n",
      "11115/20000 [===============>..............] - ETA: 36s - loss: 0.0257(465, 160, 320, 3)\n",
      "11577/20000 [================>.............] - ETA: 35s - loss: 0.0258(458, 160, 320, 3)\n",
      "12043/20000 [=================>............] - ETA: 33s - loss: 0.0256(100, 160, 320, 3)\n",
      "12503/20000 [=================>............] - ETA: 31s - loss: 0.0253(464, 160, 320, 3)\n",
      "12958/20000 [==================>...........] - ETA: 29s - loss: 0.0252(450, 160, 320, 3)\n",
      "13419/20000 [===================>..........] - ETA: 27s - loss: 0.0256(447, 160, 320, 3)\n",
      "13884/20000 [===================>..........] - ETA: 25s - loss: 0.0255(467, 160, 320, 3)\n",
      "14344/20000 [====================>.........] - ETA: 23s - loss: 0.0254(468, 160, 320, 3)\n",
      "14798/20000 [=====================>........] - ETA: 21s - loss: 0.0253(461, 160, 320, 3)\n",
      "15256/20000 [=====================>........] - ETA: 19s - loss: 0.0252(460, 160, 320, 3)\n",
      "15710/20000 [======================>.......] - ETA: 17s - loss: 0.0253(468, 160, 320, 3)\n",
      "16175/20000 [=======================>......] - ETA: 15s - loss: 0.0254(468, 160, 320, 3)\n",
      "16733/20000 [========================>.....] - ETA: 13s - loss: 0.0256(465, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      "17197/20000 [========================>.....] - ETA: 11s - loss: 0.0254(460, 160, 320, 3)\n",
      "17647/20000 [=========================>....] - ETA: 9s - loss: 0.0254 (464, 160, 320, 3)\n",
      "18094/20000 [==========================>...] - ETA: 7s - loss: 0.0253(459, 160, 320, 3)\n",
      "18561/20000 [==========================>...] - ETA: 5s - loss: 0.0252(460, 160, 320, 3)\n",
      "19029/20000 [===========================>..] - ETA: 4s - loss: 0.0251(457, 160, 320, 3)\n",
      "19490/20000 [============================>.] - ETA: 2s - loss: 0.0251(463, 160, 320, 3)\n",
      "19950/20000 [============================>.] - ETA: 0s - loss: 0.0250(465, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00006: saving model to check-06-0.0110.hdf5\n",
      "20418/20000 [==============================] - 91s - loss: 0.0250 - val_loss: 0.0110\n",
      "Epoch 8/15\n",
      "(467, 160, 320, 3)\n",
      "  468/20000 [..............................] - ETA: 64s - loss: 0.0238(456, 160, 320, 3)\n",
      "  933/20000 [>.............................] - ETA: 78s - loss: 0.0218(453, 160, 320, 3)\n",
      " 1390/20000 [=>............................] - ETA: 71s - loss: 0.0208(454, 160, 320, 3)\n",
      " 1850/20000 [=>............................] - ETA: 74s - loss: 0.0202(451, 160, 320, 3)\n",
      " 2314/20000 [==>...........................] - ETA: 70s - loss: 0.0204(464, 160, 320, 3)\n",
      " 2773/20000 [===>..........................] - ETA: 71s - loss: 0.0207(470, 160, 320, 3)\n",
      " 3233/20000 [===>..........................] - ETA: 67s - loss: 0.0211(465, 160, 320, 3)\n",
      " 3690/20000 [====>.........................] - ETA: 67s - loss: 0.0220(473, 160, 320, 3)\n",
      " 4153/20000 [=====>........................] - ETA: 65s - loss: 0.0222(458, 160, 320, 3)\n",
      " 4618/20000 [=====>........................] - ETA: 62s - loss: 0.0228(468, 160, 320, 3)\n",
      " 5085/20000 [======>.......................] - ETA: 60s - loss: 0.0237(460, 160, 320, 3)\n",
      " 5541/20000 [=======>......................] - ETA: 58s - loss: 0.0238(464, 160, 320, 3)\n",
      " 5994/20000 [=======>......................] - ETA: 56s - loss: 0.0234(460, 160, 320, 3)\n",
      " 6448/20000 [========>.....................] - ETA: 54s - loss: 0.0234(457, 160, 320, 3)\n",
      " 6899/20000 [=========>....................] - ETA: 52s - loss: 0.0230(462, 160, 320, 3)\n",
      " 7363/20000 [==========>...................] - ETA: 50s - loss: 0.0231(457, 160, 320, 3)\n",
      " 7833/20000 [==========>...................] - ETA: 47s - loss: 0.0228(463, 160, 320, 3)\n",
      " 8298/20000 [===========>..................] - ETA: 45s - loss: 0.0227(468, 160, 320, 3)\n",
      " 8771/20000 [============>.................] - ETA: 43s - loss: 0.0227(459, 160, 320, 3)\n",
      " 9229/20000 [============>.................] - ETA: 42s - loss: 0.0229(446, 160, 320, 3)\n",
      " 9697/20000 [=============>................] - ETA: 40s - loss: 0.0230(455, 160, 320, 3)\n",
      "10157/20000 [==============>...............] - ETA: 38s - loss: 0.0233(468, 160, 320, 3)\n",
      "10621/20000 [==============>...............] - ETA: 36s - loss: 0.0232(454, 160, 320, 3)\n",
      "11081/20000 [===============>..............] - ETA: 34s - loss: 0.0232(452, 160, 320, 3)\n",
      "11538/20000 [================>.............] - ETA: 32s - loss: 0.0231(476, 160, 320, 3)\n",
      "12000/20000 [=================>............] - ETA: 30s - loss: 0.0232(451, 160, 320, 3)\n",
      "12457/20000 [=================>............] - ETA: 29s - loss: 0.0235(458, 160, 320, 3)\n",
      "12920/20000 [==================>...........] - ETA: 27s - loss: 0.0235(451, 160, 320, 3)\n",
      "13388/20000 [===================>..........] - ETA: 25s - loss: 0.0234(456, 160, 320, 3)\n",
      "13847/20000 [===================>..........] - ETA: 24s - loss: 0.0237(465, 160, 320, 3)\n",
      "14293/20000 [====================>.........] - ETA: 22s - loss: 0.0236(475, 160, 320, 3)\n",
      "14748/20000 [=====================>........] - ETA: 20s - loss: 0.0235(99, 160, 320, 3)\n",
      "15216/20000 [=====================>........] - ETA: 18s - loss: 0.0233(454, 160, 320, 3)\n",
      "15670/20000 [======================>.......] - ETA: 17s - loss: 0.0233(459, 160, 320, 3)\n",
      "16122/20000 [=======================>......] - ETA: 15s - loss: 0.0234(457, 160, 320, 3)\n",
      "16598/20000 [=======================>......] - ETA: 13s - loss: 0.0235(463, 160, 320, 3)\n",
      "17049/20000 [========================>.....] - ETA: 11s - loss: 0.0235(455, 160, 320, 3)\n",
      "17507/20000 [=========================>....] - ETA: 9s - loss: 0.0236 (457, 160, 320, 3)\n",
      "17958/20000 [=========================>....] - ETA: 8s - loss: 0.0234(465, 160, 320, 3)\n",
      "18414/20000 [==========================>...] - ETA: 6s - loss: 0.0234(454, 160, 320, 3)\n",
      "18879/20000 [===========================>..] - ETA: 4s - loss: 0.0233(459, 160, 320, 3)\n",
      "19453/20000 [============================>.] - ETA: 2s - loss: 0.0235(453, 160, 320, 3)\n",
      "(464, 160, 320, 3)\n",
      "19907/20000 [============================>.] - ETA: 0s - loss: 0.0234(465, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00007: saving model to check-07-0.0110.hdf5\n",
      "20366/20000 [==============================] - 86s - loss: 0.0235 - val_loss: 0.0110\n",
      "Epoch 9/15\n",
      "(465, 160, 320, 3)\n",
      "  457/20000 [..............................] - ETA: 70s - loss: 0.0205(463, 160, 320, 3)\n",
      "  920/20000 [>.............................] - ETA: 69s - loss: 0.0200(466, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1375/20000 [=>............................] - ETA: 69s - loss: 0.0204(457, 160, 320, 3)\n",
      " 1832/20000 [=>............................] - ETA: 68s - loss: 0.0213(453, 160, 320, 3)\n",
      " 2297/20000 [==>...........................] - ETA: 66s - loss: 0.0219(453, 160, 320, 3)\n",
      " 2751/20000 [===>..........................] - ETA: 64s - loss: 0.0215(467, 160, 320, 3)\n",
      " 3210/20000 [===>..........................] - ETA: 62s - loss: 0.0212(464, 160, 320, 3)\n",
      " 3663/20000 [====>.........................] - ETA: 60s - loss: 0.0215(467, 160, 320, 3)\n",
      " 4127/20000 [=====>........................] - ETA: 59s - loss: 0.0214(462, 160, 320, 3)\n",
      " 4592/20000 [=====>........................] - ETA: 57s - loss: 0.0214(459, 160, 320, 3)\n",
      " 5057/20000 [======>.......................] - ETA: 55s - loss: 0.0212(462, 160, 320, 3)\n",
      " 5520/20000 [=======>......................] - ETA: 53s - loss: 0.0210(463, 160, 320, 3)\n",
      " 5986/20000 [=======>......................] - ETA: 51s - loss: 0.0215(455, 160, 320, 3)\n",
      " 6443/20000 [========>.....................] - ETA: 49s - loss: 0.0217(463, 160, 320, 3)\n",
      " 6896/20000 [=========>....................] - ETA: 47s - loss: 0.0219(457, 160, 320, 3)\n",
      " 7349/20000 [==========>...................] - ETA: 46s - loss: 0.0223(465, 160, 320, 3)\n",
      " 7816/20000 [==========>...................] - ETA: 44s - loss: 0.0222(466, 160, 320, 3)\n",
      " 8280/20000 [===========>..................] - ETA: 42s - loss: 0.0223(459, 160, 320, 3)\n",
      " 8747/20000 [============>.................] - ETA: 41s - loss: 0.0221(459, 160, 320, 3)\n",
      " 9209/20000 [============>.................] - ETA: 39s - loss: 0.0224(466, 160, 320, 3)\n",
      " 9668/20000 [=============>................] - ETA: 38s - loss: 0.0223(459, 160, 320, 3)\n",
      "10130/20000 [==============>...............] - ETA: 36s - loss: 0.0224(459, 160, 320, 3)\n",
      "10593/20000 [==============>...............] - ETA: 34s - loss: 0.0224(448, 160, 320, 3)\n",
      "11048/20000 [===============>..............] - ETA: 33s - loss: 0.0224(467, 160, 320, 3)\n",
      "11511/20000 [================>.............] - ETA: 31s - loss: 0.0222(456, 160, 320, 3)\n",
      "11968/20000 [================>.............] - ETA: 29s - loss: 0.0223(458, 160, 320, 3)\n",
      "12433/20000 [=================>............] - ETA: 27s - loss: 0.0223(458, 160, 320, 3)\n",
      "12899/20000 [==================>...........] - ETA: 26s - loss: 0.0224(466, 160, 320, 3)\n",
      "13358/20000 [===================>..........] - ETA: 24s - loss: 0.0225(459, 160, 320, 3)\n",
      "13817/20000 [===================>..........] - ETA: 22s - loss: 0.0224(458, 160, 320, 3)\n",
      "14283/20000 [====================>.........] - ETA: 21s - loss: 0.0224(474, 160, 320, 3)\n",
      "14742/20000 [=====================>........] - ETA: 19s - loss: 0.0225(459, 160, 320, 3)\n",
      "15201/20000 [=====================>........] - ETA: 17s - loss: 0.0225(464, 160, 320, 3)\n",
      "15649/20000 [======================>.......] - ETA: 15s - loss: 0.0226(464, 160, 320, 3)\n",
      "16116/20000 [=======================>......] - ETA: 14s - loss: 0.0225(462, 160, 320, 3)\n",
      "16572/20000 [=======================>......] - ETA: 12s - loss: 0.0227(469, 160, 320, 3)\n",
      "17030/20000 [========================>.....] - ETA: 10s - loss: 0.0228(464, 160, 320, 3)\n",
      "17488/20000 [=========================>....] - ETA: 9s - loss: 0.0227 (101, 160, 320, 3)\n",
      "17954/20000 [=========================>....] - ETA: 7s - loss: 0.0226(468, 160, 320, 3)\n",
      "18413/20000 [==========================>...] - ETA: 5s - loss: 0.0226(462, 160, 320, 3)\n",
      "18871/20000 [===========================>..] - ETA: 4s - loss: 0.0228(469, 160, 320, 3)\n",
      "19345/20000 [============================>.] - ETA: 2s - loss: 0.0227(470, 160, 320, 3)\n",
      "19804/20000 [============================>.] - ETA: 0s - loss: 0.0227(451, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00008: saving model to check-08-0.0109.hdf5\n",
      "20268/20000 [==============================] - 79s - loss: 0.0226 - val_loss: 0.0109\n",
      "Epoch 10/15\n",
      "(461, 160, 320, 3)\n",
      "  464/20000 [..............................] - ETA: 78s - loss: 0.0195(465, 160, 320, 3)\n",
      "  926/20000 [>.............................] - ETA: 73s - loss: 0.0213(463, 160, 320, 3)\n",
      " 1395/20000 [=>............................] - ETA: 70s - loss: 0.0216(463, 160, 320, 3)\n",
      " 1960/20000 [=>............................] - ETA: 67s - loss: 0.0227(457, 160, 320, 3)\n",
      "(470, 160, 320, 3)\n",
      " 2428/20000 [==>...........................] - ETA: 66s - loss: 0.0219(465, 160, 320, 3)\n",
      " 2890/20000 [===>..........................] - ETA: 64s - loss: 0.0228(454, 160, 320, 3)\n",
      " 3359/20000 [====>.........................] - ETA: 62s - loss: 0.0225(469, 160, 320, 3)\n",
      " 3829/20000 [====>.........................] - ETA: 60s - loss: 0.0219(462, 160, 320, 3)\n",
      " 4280/20000 [=====>........................] - ETA: 58s - loss: 0.0218(462, 160, 320, 3)\n",
      " 4741/20000 [======>.......................] - ETA: 56s - loss: 0.0215(453, 160, 320, 3)\n",
      " 5206/20000 [======>.......................] - ETA: 54s - loss: 0.0218(465, 160, 320, 3)\n",
      " 5669/20000 [=======>......................] - ETA: 52s - loss: 0.0221(465, 160, 320, 3)\n",
      " 6132/20000 [========>.....................] - ETA: 50s - loss: 0.0219(462, 160, 320, 3)\n",
      " 6589/20000 [========>.....................] - ETA: 49s - loss: 0.0216(454, 160, 320, 3)\n",
      " 7059/20000 [=========>....................] - ETA: 47s - loss: 0.0214(465, 160, 320, 3)\n",
      " 7524/20000 [==========>...................] - ETA: 45s - loss: 0.0213(460, 160, 320, 3)\n",
      " 7978/20000 [==========>...................] - ETA: 44s - loss: 0.0212(468, 160, 320, 3)\n",
      " 8447/20000 [===========>..................] - ETA: 42s - loss: 0.0215(454, 160, 320, 3)\n",
      " 8909/20000 [============>.................] - ETA: 41s - loss: 0.0217(471, 160, 320, 3)\n",
      " 9371/20000 [=============>................] - ETA: 39s - loss: 0.0220(454, 160, 320, 3)\n",
      " 9824/20000 [=============>................] - ETA: 38s - loss: 0.0219(463, 160, 320, 3)\n",
      "10289/20000 [==============>...............] - ETA: 36s - loss: 0.0224(464, 160, 320, 3)\n",
      "10754/20000 [===============>..............] - ETA: 34s - loss: 0.0224(453, 160, 320, 3)\n",
      "11216/20000 [===============>..............] - ETA: 32s - loss: 0.0226(482, 160, 320, 3)\n",
      "11670/20000 [================>.............] - ETA: 31s - loss: 0.0224(462, 160, 320, 3)\n",
      "12135/20000 [=================>............] - ETA: 29s - loss: 0.0224(464, 160, 320, 3)\n",
      "12595/20000 [=================>............] - ETA: 27s - loss: 0.0222(472, 160, 320, 3)\n",
      "13063/20000 [==================>...........] - ETA: 25s - loss: 0.0221(459, 160, 320, 3)\n",
      "13517/20000 [===================>..........] - ETA: 24s - loss: 0.0221(459, 160, 320, 3)\n",
      "13988/20000 [===================>..........] - ETA: 22s - loss: 0.0219(446, 160, 320, 3)\n",
      "14442/20000 [====================>.........] - ETA: 20s - loss: 0.0219(463, 160, 320, 3)\n",
      "14905/20000 [=====================>........] - ETA: 18s - loss: 0.0220(460, 160, 320, 3)\n",
      "15369/20000 [======================>.......] - ETA: 17s - loss: 0.0218(454, 160, 320, 3)\n",
      "15822/20000 [======================>.......] - ETA: 15s - loss: 0.0218(457, 160, 320, 3)\n",
      "16304/20000 [=======================>......] - ETA: 13s - loss: 0.0218(466, 160, 320, 3)\n",
      "16766/20000 [========================>.....] - ETA: 11s - loss: 0.0219(461, 160, 320, 3)\n",
      "17230/20000 [========================>.....] - ETA: 10s - loss: 0.0219(453, 160, 320, 3)\n",
      "17702/20000 [=========================>....] - ETA: 8s - loss: 0.0221 (455, 160, 320, 3)\n",
      "18161/20000 [==========================>...] - ETA: 6s - loss: 0.0221(462, 160, 320, 3)\n",
      "18620/20000 [==========================>...] - ETA: 5s - loss: 0.0223(450, 160, 320, 3)\n",
      "19066/20000 [===========================>..] - ETA: 3s - loss: 0.0222(452, 160, 320, 3)\n",
      "19529/20000 [============================>.] - ETA: 1s - loss: 0.0225(466, 160, 320, 3)\n",
      "19989/20000 [============================>.] - ETA: 0s - loss: 0.0224(463, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00009: saving model to check-09-0.0114.hdf5\n",
      "20443/20000 [==============================] - 81s - loss: 0.0223 - val_loss: 0.0114\n",
      "Epoch 11/15\n",
      "(101, 160, 320, 3)\n",
      "  457/20000 [..............................] - ETA: 68s - loss: 0.0167(471, 160, 320, 3)\n",
      "  923/20000 [>.............................] - ETA: 71s - loss: 0.0195(462, 160, 320, 3)\n",
      " 1384/20000 [=>............................] - ETA: 70s - loss: 0.0228(472, 160, 320, 3)\n",
      " 1837/20000 [=>............................] - ETA: 69s - loss: 0.0222(468, 160, 320, 3)\n",
      " 2292/20000 [==>...........................] - ETA: 67s - loss: 0.0224(466, 160, 320, 3)\n",
      " 2754/20000 [===>..........................] - ETA: 65s - loss: 0.0224(462, 160, 320, 3)\n",
      " 3204/20000 [===>..........................] - ETA: 63s - loss: 0.0221(452, 160, 320, 3)\n",
      " 3656/20000 [====>.........................] - ETA: 61s - loss: 0.0219(456, 160, 320, 3)\n",
      " 4122/20000 [=====>........................] - ETA: 59s - loss: 0.0221(452, 160, 320, 3)\n",
      " 4686/20000 [======>.......................] - ETA: 56s - loss: 0.0229(460, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 5157/20000 [======>.......................] - ETA: 55s - loss: 0.0226(463, 160, 320, 3)\n",
      " 5619/20000 [=======>......................] - ETA: 53s - loss: 0.0226(458, 160, 320, 3)\n",
      " 6091/20000 [========>.....................] - ETA: 52s - loss: 0.0228(465, 160, 320, 3)\n",
      " 6559/20000 [========>.....................] - ETA: 50s - loss: 0.0224(464, 160, 320, 3)\n",
      " 7025/20000 [=========>....................] - ETA: 48s - loss: 0.0222(470, 160, 320, 3)\n",
      " 7487/20000 [==========>...................] - ETA: 47s - loss: 0.0221(469, 160, 320, 3)\n",
      " 7939/20000 [==========>...................] - ETA: 45s - loss: 0.0220(457, 160, 320, 3)\n",
      " 8395/20000 [===========>..................] - ETA: 43s - loss: 0.0218(457, 160, 320, 3)\n",
      " 8847/20000 [============>.................] - ETA: 41s - loss: 0.0217(469, 160, 320, 3)\n",
      " 9307/20000 [============>.................] - ETA: 40s - loss: 0.0216(466, 160, 320, 3)\n",
      " 9763/20000 [=============>................] - ETA: 38s - loss: 0.0215(458, 160, 320, 3)\n",
      "10226/20000 [==============>...............] - ETA: 36s - loss: 0.0215(455, 160, 320, 3)\n",
      "10684/20000 [===============>..............] - ETA: 34s - loss: 0.0215(465, 160, 320, 3)\n",
      "11149/20000 [===============>..............] - ETA: 32s - loss: 0.0215(470, 160, 320, 3)\n",
      "11613/20000 [================>.............] - ETA: 31s - loss: 0.0216(462, 160, 320, 3)\n",
      "12083/20000 [=================>............] - ETA: 29s - loss: 0.0219(463, 160, 320, 3)\n",
      "12552/20000 [=================>............] - ETA: 27s - loss: 0.0219(461, 160, 320, 3)\n",
      "13009/20000 [==================>...........] - ETA: 25s - loss: 0.0220(447, 160, 320, 3)\n",
      "13466/20000 [===================>..........] - ETA: 24s - loss: 0.0220(471, 160, 320, 3)\n",
      "13935/20000 [===================>..........] - ETA: 22s - loss: 0.0220(456, 160, 320, 3)\n",
      "14401/20000 [====================>.........] - ETA: 20s - loss: 0.0218(459, 160, 320, 3)\n",
      "14859/20000 [=====================>........] - ETA: 19s - loss: 0.0219(464, 160, 320, 3)\n",
      "15314/20000 [=====================>........] - ETA: 17s - loss: 0.0216(470, 160, 320, 3)\n",
      "15779/20000 [======================>.......] - ETA: 15s - loss: 0.0215(460, 160, 320, 3)\n",
      "16249/20000 [=======================>......] - ETA: 13s - loss: 0.0215(457, 160, 320, 3)\n",
      "16711/20000 [========================>.....] - ETA: 12s - loss: 0.0214(455, 160, 320, 3)\n",
      "17174/20000 [========================>.....] - ETA: 10s - loss: 0.0214(454, 160, 320, 3)\n",
      "17635/20000 [=========================>....] - ETA: 8s - loss: 0.0214 (468, 160, 320, 3)\n",
      "18082/20000 [==========================>...] - ETA: 7s - loss: 0.0213(469, 160, 320, 3)\n",
      "18553/20000 [==========================>...] - ETA: 5s - loss: 0.0214(463, 160, 320, 3)\n",
      "19009/20000 [===========================>..] - ETA: 3s - loss: 0.0215(463, 160, 320, 3)\n",
      "19468/20000 [============================>.] - ETA: 1s - loss: 0.0214(473, 160, 320, 3)\n",
      "19932/20000 [============================>.] - ETA: 0s - loss: 0.0213(454, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00010: saving model to check-10-0.0109.hdf5\n",
      "20402/20000 [==============================] - 81s - loss: 0.0214 - val_loss: 0.0109\n",
      "Epoch 12/15\n",
      "(460, 160, 320, 3)\n",
      "  460/20000 [..............................] - ETA: 74s - loss: 0.0253(468, 160, 320, 3)\n",
      "  917/20000 [>.............................] - ETA: 74s - loss: 0.0268(459, 160, 320, 3)\n",
      " 1372/20000 [=>............................] - ETA: 70s - loss: 0.0251(469, 160, 320, 3)\n",
      " 1826/20000 [=>............................] - ETA: 68s - loss: 0.0261(460, 160, 320, 3)\n",
      " 2294/20000 [==>...........................] - ETA: 66s - loss: 0.0253(460, 160, 320, 3)\n",
      " 2763/20000 [===>..........................] - ETA: 63s - loss: 0.0243(101, 160, 320, 3)\n",
      " 3226/20000 [===>..........................] - ETA: 61s - loss: 0.0231(466, 160, 320, 3)\n",
      " 3689/20000 [====>.........................] - ETA: 60s - loss: 0.0229(462, 160, 320, 3)\n",
      " 4162/20000 [=====>........................] - ETA: 59s - loss: 0.0233(467, 160, 320, 3)\n",
      " 4616/20000 [=====>........................] - ETA: 57s - loss: 0.0228(466, 160, 320, 3)\n",
      " 5076/20000 [======>.......................] - ETA: 56s - loss: 0.0224(456, 160, 320, 3)\n",
      " 5544/20000 [=======>......................] - ETA: 54s - loss: 0.0221(457, 160, 320, 3)\n",
      " 6003/20000 [========>.....................] - ETA: 52s - loss: 0.0217(464, 160, 320, 3)\n",
      " 6472/20000 [========>.....................] - ETA: 50s - loss: 0.0215(470, 160, 320, 3)\n",
      " 6932/20000 [=========>....................] - ETA: 48s - loss: 0.0214(458, 160, 320, 3)\n",
      " 7493/20000 [==========>...................] - ETA: 46s - loss: 0.0216(466, 160, 320, 3)\n",
      "(460, 160, 320, 3)\n",
      " 7959/20000 [==========>...................] - ETA: 45s - loss: 0.0213(448, 160, 320, 3)\n",
      " 8421/20000 [===========>..................] - ETA: 43s - loss: 0.0212(461, 160, 320, 3)\n",
      " 8888/20000 [============>.................] - ETA: 41s - loss: 0.0211(469, 160, 320, 3)\n",
      " 9354/20000 [=============>................] - ETA: 39s - loss: 0.0208(466, 160, 320, 3)\n",
      " 9810/20000 [=============>................] - ETA: 37s - loss: 0.0207(456, 160, 320, 3)\n",
      "10267/20000 [==============>...............] - ETA: 36s - loss: 0.0206(463, 160, 320, 3)\n",
      "10731/20000 [===============>..............] - ETA: 34s - loss: 0.0207(461, 160, 320, 3)\n",
      "11201/20000 [===============>..............] - ETA: 32s - loss: 0.0208(451, 160, 320, 3)\n",
      "11659/20000 [================>.............] - ETA: 30s - loss: 0.0207(453, 160, 320, 3)\n",
      "12125/20000 [=================>............] - ETA: 28s - loss: 0.0207(466, 160, 320, 3)\n",
      "12585/20000 [=================>............] - ETA: 27s - loss: 0.0205(464, 160, 320, 3)\n",
      "13033/20000 [==================>...........] - ETA: 25s - loss: 0.0205(450, 160, 320, 3)\n",
      "13494/20000 [===================>..........] - ETA: 23s - loss: 0.0204(463, 160, 320, 3)\n",
      "13963/20000 [===================>..........] - ETA: 22s - loss: 0.0204(473, 160, 320, 3)\n",
      "14429/20000 [====================>.........] - ETA: 20s - loss: 0.0204(457, 160, 320, 3)\n",
      "14885/20000 [=====================>........] - ETA: 18s - loss: 0.0206(450, 160, 320, 3)\n",
      "15348/20000 [======================>.......] - ETA: 17s - loss: 0.0206(468, 160, 320, 3)\n",
      "15809/20000 [======================>.......] - ETA: 15s - loss: 0.0208(466, 160, 320, 3)\n",
      "16260/20000 [=======================>......] - ETA: 13s - loss: 0.0208(448, 160, 320, 3)\n",
      "16713/20000 [========================>.....] - ETA: 12s - loss: 0.0208(457, 160, 320, 3)\n",
      "17179/20000 [========================>.....] - ETA: 10s - loss: 0.0208(455, 160, 320, 3)\n",
      "17643/20000 [=========================>....] - ETA: 8s - loss: 0.0208 (462, 160, 320, 3)\n",
      "18093/20000 [==========================>...] - ETA: 7s - loss: 0.0207(460, 160, 320, 3)\n",
      "18556/20000 [==========================>...] - ETA: 5s - loss: 0.0206(463, 160, 320, 3)\n",
      "19029/20000 [===========================>..] - ETA: 3s - loss: 0.0206(458, 160, 320, 3)\n",
      "19486/20000 [============================>.] - ETA: 1s - loss: 0.0205(456, 160, 320, 3)\n",
      "19936/20000 [============================>.] - ETA: 0s - loss: 0.0204(461, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00011: saving model to check-11-0.0105.hdf5\n",
      "20404/20000 [==============================] - 81s - loss: 0.0205 - val_loss: 0.0105\n",
      "Epoch 13/15\n",
      "(463, 160, 320, 3)\n",
      "  466/20000 [..............................] - ETA: 70s - loss: 0.0194(464, 160, 320, 3)\n",
      "  914/20000 [>.............................] - ETA: 71s - loss: 0.0217(467, 160, 320, 3)\n",
      " 1371/20000 [=>............................] - ETA: 71s - loss: 0.0211(473, 160, 320, 3)\n",
      " 1826/20000 [=>............................] - ETA: 69s - loss: 0.0209(460, 160, 320, 3)\n",
      " 2288/20000 [==>...........................] - ETA: 67s - loss: 0.0205(457, 160, 320, 3)\n",
      " 2748/20000 [===>..........................] - ETA: 65s - loss: 0.0219(468, 160, 320, 3)\n",
      " 3211/20000 [===>..........................] - ETA: 63s - loss: 0.0223(458, 160, 320, 3)\n",
      " 3669/20000 [====>.........................] - ETA: 61s - loss: 0.0226(460, 160, 320, 3)\n",
      " 4125/20000 [=====>........................] - ETA: 60s - loss: 0.0219(465, 160, 320, 3)\n",
      " 4586/20000 [=====>........................] - ETA: 58s - loss: 0.0226(465, 160, 320, 3)\n",
      " 5049/20000 [======>.......................] - ETA: 57s - loss: 0.0227(465, 160, 320, 3)\n",
      " 5513/20000 [=======>......................] - ETA: 55s - loss: 0.0223(101, 160, 320, 3)\n",
      " 5980/20000 [=======>......................] - ETA: 52s - loss: 0.0216(457, 160, 320, 3)\n",
      " 6453/20000 [========>.....................] - ETA: 50s - loss: 0.0216(464, 160, 320, 3)\n",
      " 6913/20000 [=========>....................] - ETA: 48s - loss: 0.0219(467, 160, 320, 3)\n",
      " 7370/20000 [==========>...................] - ETA: 47s - loss: 0.0218(452, 160, 320, 3)\n",
      " 7838/20000 [==========>...................] - ETA: 45s - loss: 0.0217(454, 160, 320, 3)\n",
      " 8296/20000 [===========>..................] - ETA: 43s - loss: 0.0216(447, 160, 320, 3)\n",
      " 8756/20000 [============>.................] - ETA: 42s - loss: 0.0214(468, 160, 320, 3)\n",
      " 9221/20000 [============>.................] - ETA: 40s - loss: 0.0213(457, 160, 320, 3)\n",
      " 9686/20000 [=============>................] - ETA: 38s - loss: 0.0213(458, 160, 320, 3)\n",
      "10252/20000 [==============>...............] - ETA: 36s - loss: 0.0218(461, 160, 320, 3)\n",
      "(482, 160, 320, 3)\n",
      "10709/20000 [===============>..............] - ETA: 34s - loss: 0.0217(458, 160, 320, 3)\n",
      "11173/20000 [===============>..............] - ETA: 33s - loss: 0.0217(451, 160, 320, 3)\n",
      "11640/20000 [================>.............] - ETA: 31s - loss: 0.0216(457, 160, 320, 3)\n",
      "12092/20000 [=================>............] - ETA: 29s - loss: 0.0215(468, 160, 320, 3)\n",
      "12546/20000 [=================>............] - ETA: 28s - loss: 0.0214(458, 160, 320, 3)\n",
      "12993/20000 [==================>...........] - ETA: 26s - loss: 0.0213(465, 160, 320, 3)\n",
      "13461/20000 [===================>..........] - ETA: 24s - loss: 0.0213(460, 160, 320, 3)\n",
      "13918/20000 [===================>..........] - ETA: 22s - loss: 0.0213(456, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 21s - loss: 0.0212(467, 160, 320, 3)\n",
      "14837/20000 [=====================>........] - ETA: 19s - loss: 0.0212(465, 160, 320, 3)\n",
      "15319/20000 [=====================>........] - ETA: 17s - loss: 0.0211(449, 160, 320, 3)\n",
      "15777/20000 [======================>.......] - ETA: 15s - loss: 0.0210(464, 160, 320, 3)\n",
      "16228/20000 [=======================>......] - ETA: 14s - loss: 0.0210(463, 160, 320, 3)\n",
      "16685/20000 [========================>.....] - ETA: 12s - loss: 0.0209(461, 160, 320, 3)\n",
      "17153/20000 [========================>.....] - ETA: 10s - loss: 0.0210(444, 160, 320, 3)\n",
      "17611/20000 [=========================>....] - ETA: 9s - loss: 0.0212 (457, 160, 320, 3)\n",
      "18076/20000 [==========================>...] - ETA: 7s - loss: 0.0212(450, 160, 320, 3)\n",
      "18536/20000 [==========================>...] - ETA: 5s - loss: 0.0213(460, 160, 320, 3)\n",
      "18992/20000 [===========================>..] - ETA: 3s - loss: 0.0214(448, 160, 320, 3)\n",
      "19459/20000 [============================>.] - ETA: 2s - loss: 0.0215(459, 160, 320, 3)\n",
      "19924/20000 [============================>.] - ETA: 0s - loss: 0.0214(459, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "Epoch 00012: saving model to check-12-0.0102.hdf5\n",
      "20373/20000 [==============================] - 82s - loss: 0.0214 - val_loss: 0.0102\n",
      "Epoch 14/15\n",
      "(465, 160, 320, 3)\n",
      "  464/20000 [..............................] - ETA: 72s - loss: 0.0141(463, 160, 320, 3)\n",
      "  927/20000 [>.............................] - ETA: 69s - loss: 0.0166(457, 160, 320, 3)\n",
      " 1388/20000 [=>............................] - ETA: 69s - loss: 0.0176(456, 160, 320, 3)\n",
      " 1832/20000 [=>............................] - ETA: 68s - loss: 0.0174(472, 160, 320, 3)\n",
      " 2289/20000 [==>...........................] - ETA: 67s - loss: 0.0170(468, 160, 320, 3)\n",
      " 2739/20000 [===>..........................] - ETA: 65s - loss: 0.0176(465, 160, 320, 3)\n",
      " 3199/20000 [===>..........................] - ETA: 63s - loss: 0.0177(469, 160, 320, 3)\n",
      " 3647/20000 [====>.........................] - ETA: 61s - loss: 0.0183(468, 160, 320, 3)\n",
      " 4106/20000 [=====>........................] - ETA: 59s - loss: 0.0185(468, 160, 320, 3)\n",
      " 4565/20000 [=====>........................] - ETA: 58s - loss: 0.0189(446, 160, 320, 3)\n",
      " 5030/20000 [======>.......................] - ETA: 56s - loss: 0.0188(453, 160, 320, 3)\n",
      " 5493/20000 [=======>......................] - ETA: 54s - loss: 0.0195(453, 160, 320, 3)\n",
      " 5950/20000 [=======>......................] - ETA: 53s - loss: 0.0196(461, 160, 320, 3)\n",
      " 6406/20000 [========>.....................] - ETA: 51s - loss: 0.0199(468, 160, 320, 3)\n",
      " 6878/20000 [=========>....................] - ETA: 49s - loss: 0.0198(471, 160, 320, 3)\n",
      " 7346/20000 [==========>...................] - ETA: 47s - loss: 0.0205(464, 160, 320, 3)\n",
      " 7811/20000 [==========>...................] - ETA: 45s - loss: 0.0206(465, 160, 320, 3)\n",
      " 8280/20000 [===========>..................] - ETA: 43s - loss: 0.0204(103, 160, 320, 3)\n",
      " 8748/20000 [============>.................] - ETA: 41s - loss: 0.0200(456, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 39s - loss: 0.0202(460, 160, 320, 3)\n",
      " 9662/20000 [=============>................] - ETA: 37s - loss: 0.0206(459, 160, 320, 3)\n",
      "10115/20000 [==============>...............] - ETA: 36s - loss: 0.0206(453, 160, 320, 3)\n",
      "10568/20000 [==============>...............] - ETA: 34s - loss: 0.0205(454, 160, 320, 3)\n",
      "11029/20000 [===============>..............] - ETA: 33s - loss: 0.0204(473, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11497/20000 [================>.............] - ETA: 31s - loss: 0.0204(466, 160, 320, 3)\n",
      "11968/20000 [================>.............] - ETA: 29s - loss: 0.0205(466, 160, 320, 3)\n",
      "12432/20000 [=================>............] - ETA: 28s - loss: 0.0206(454, 160, 320, 3)\n",
      "13000/20000 [==================>...........] - ETA: 26s - loss: 0.0208(448, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      "13456/20000 [===================>..........] - ETA: 24s - loss: 0.0207(465, 160, 320, 3)\n",
      "13916/20000 [===================>..........] - ETA: 22s - loss: 0.0208(473, 160, 320, 3)\n",
      "14375/20000 [====================>.........] - ETA: 20s - loss: 0.0208(459, 160, 320, 3)\n",
      "14828/20000 [=====================>........] - ETA: 19s - loss: 0.0206(464, 160, 320, 3)\n",
      "15282/20000 [=====================>........] - ETA: 17s - loss: 0.0206(455, 160, 320, 3)\n",
      "15755/20000 [======================>.......] - ETA: 15s - loss: 0.0207(459, 160, 320, 3)\n",
      "16221/20000 [=======================>......] - ETA: 14s - loss: 0.0207(465, 160, 320, 3)\n",
      "16687/20000 [========================>.....] - ETA: 12s - loss: 0.0208(457, 160, 320, 3)\n",
      "17141/20000 [========================>.....] - ETA: 10s - loss: 0.0209(463, 160, 320, 3)\n",
      "17589/20000 [=========================>....] - ETA: 9s - loss: 0.0209 (460, 160, 320, 3)\n",
      "18051/20000 [==========================>...] - ETA: 7s - loss: 0.0208(458, 160, 320, 3)\n",
      "18516/20000 [==========================>...] - ETA: 5s - loss: 0.0207(461, 160, 320, 3)\n",
      "18989/20000 [===========================>..] - ETA: 3s - loss: 0.0206(462, 160, 320, 3)\n",
      "19448/20000 [============================>.] - ETA: 2s - loss: 0.0205(454, 160, 320, 3)\n",
      "19912/20000 [============================>.] - ETA: 0s - loss: 0.0206(464, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00013: saving model to check-13-0.0103.hdf5\n",
      "20367/20000 [==============================] - 83s - loss: 0.0208 - val_loss: 0.0103\n",
      "Epoch 15/15\n",
      "(454, 160, 320, 3)\n",
      "  459/20000 [..............................] - ETA: 75s - loss: 0.0211(468, 160, 320, 3)\n",
      "  924/20000 [>.............................] - ETA: 73s - loss: 0.0229(460, 160, 320, 3)\n",
      " 1381/20000 [=>............................] - ETA: 71s - loss: 0.0222(461, 160, 320, 3)\n",
      " 1844/20000 [=>............................] - ETA: 68s - loss: 0.0228(467, 160, 320, 3)\n",
      " 2304/20000 [==>...........................] - ETA: 65s - loss: 0.0218(464, 160, 320, 3)\n",
      " 2762/20000 [===>..........................] - ETA: 63s - loss: 0.0221(467, 160, 320, 3)\n",
      " 3223/20000 [===>..........................] - ETA: 61s - loss: 0.0209(459, 160, 320, 3)\n",
      " 3685/20000 [====>.........................] - ETA: 59s - loss: 0.0206(467, 160, 320, 3)\n",
      " 4139/20000 [=====>........................] - ETA: 58s - loss: 0.0206(467, 160, 320, 3)\n",
      " 4603/20000 [=====>........................] - ETA: 57s - loss: 0.0203(465, 160, 320, 3)\n",
      " 5057/20000 [======>.......................] - ETA: 55s - loss: 0.0202(468, 160, 320, 3)\n",
      " 5525/20000 [=======>......................] - ETA: 53s - loss: 0.0205(469, 160, 320, 3)\n",
      " 5985/20000 [=======>......................] - ETA: 51s - loss: 0.0204(461, 160, 320, 3)\n",
      " 6446/20000 [========>.....................] - ETA: 50s - loss: 0.0205(456, 160, 320, 3)\n",
      " 6913/20000 [=========>....................] - ETA: 48s - loss: 0.0204(457, 160, 320, 3)\n",
      " 7377/20000 [==========>...................] - ETA: 46s - loss: 0.0204(457, 160, 320, 3)\n",
      " 7844/20000 [==========>...................] - ETA: 45s - loss: 0.0203(469, 160, 320, 3)\n",
      " 8303/20000 [===========>..................] - ETA: 43s - loss: 0.0207(452, 160, 320, 3)\n",
      " 8770/20000 [============>.................] - ETA: 41s - loss: 0.0207(466, 160, 320, 3)\n",
      " 9237/20000 [============>.................] - ETA: 40s - loss: 0.0210(458, 160, 320, 3)\n",
      " 9702/20000 [=============>................] - ETA: 38s - loss: 0.0209(457, 160, 320, 3)\n",
      "10170/20000 [==============>...............] - ETA: 36s - loss: 0.0214(445, 160, 320, 3)\n",
      "10639/20000 [==============>...............] - ETA: 35s - loss: 0.0212(459, 160, 320, 3)\n",
      "11100/20000 [===============>..............] - ETA: 33s - loss: 0.0211(106, 160, 320, 3)\n",
      "11556/20000 [================>.............] - ETA: 31s - loss: 0.0208(469, 160, 320, 3)\n",
      "12013/20000 [=================>............] - ETA: 29s - loss: 0.0208(455, 160, 320, 3)\n",
      "12470/20000 [=================>............] - ETA: 28s - loss: 0.0210(445, 160, 320, 3)\n",
      "12939/20000 [==================>...........] - ETA: 26s - loss: 0.0210(451, 160, 320, 3)\n",
      "13391/20000 [===================>..........] - ETA: 24s - loss: 0.0209(465, 160, 320, 3)\n",
      "13857/20000 [===================>..........] - ETA: 23s - loss: 0.0208(465, 160, 320, 3)\n",
      "14315/20000 [====================>.........] - ETA: 21s - loss: 0.0207(449, 160, 320, 3)\n",
      "14772/20000 [=====================>........] - ETA: 19s - loss: 0.0207(473, 160, 320, 3)\n",
      "15217/20000 [=====================>........] - ETA: 18s - loss: 0.0206(456, 160, 320, 3)\n",
      "15782/20000 [======================>.......] - ETA: 15s - loss: 0.0209(467, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      "16251/20000 [=======================>......] - ETA: 14s - loss: 0.0207(465, 160, 320, 3)\n",
      "16706/20000 [========================>.....] - ETA: 12s - loss: 0.0208(462, 160, 320, 3)\n",
      "17151/20000 [========================>.....] - ETA: 10s - loss: 0.0208(465, 160, 320, 3)\n",
      "17602/20000 [=========================>....] - ETA: 9s - loss: 0.0207 (462, 160, 320, 3)\n",
      "18067/20000 [==========================>...] - ETA: 7s - loss: 0.0207(466, 160, 320, 3)\n",
      "18532/20000 [==========================>...] - ETA: 5s - loss: 0.0206(456, 160, 320, 3)\n",
      "18981/20000 [===========================>..] - ETA: 3s - loss: 0.0207(463, 160, 320, 3)\n",
      "19454/20000 [============================>.] - ETA: 2s - loss: 0.0207(462, 160, 320, 3)\n",
      "19910/20000 [============================>.] - ETA: 0s - loss: 0.0207(464, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00014: saving model to check-14-0.0098.hdf5\n",
      "20377/20000 [==============================] - 82s - loss: 0.0206 - val_loss: 0.0098\n"
     ]
    }
   ],
   "source": [
    "#Model fit generator\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch= samples_per_epoch,\n",
    "                                     validation_data=validation_generator,\n",
    "                                     nb_val_samples=nb_val_samples, nb_epoch=nb_epoch, verbose=1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHEoppfyiwaGUUsovGhxKKaX88v8BZThhN0hEfi4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f8d55ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history_object.history.keys())\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saw that the validation score leveled off at around epoch 15, suggesting that the model was overfitting after that point, and testing epochs 10-20 found that epoch 15 performed the best (getting to the turn after the bridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 162, 322, 3)   0           zeropadding2d_input_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)        (None, 82, 322, 3)    0           zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)               (None, 64, 64, 3)     0           cropping2d_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "color_conv (Convolution2D)       (None, 64, 64, 3)     12          lambda_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Convolution2D)            (None, 32, 32, 32)    896         color_conv[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 32, 32, 32)    0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 31, 31, 32)    0           activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Convolution2D)            (None, 16, 16, 64)    18496       pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu2 (Activation)               (None, 16, 16, 64)    0           conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 8, 8, 64)      0           relu2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Convolution2D)            (None, 8, 8, 128)     73856       pool2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 8, 8, 128)     0           conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)             (None, 4, 4, 128)     0           activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 2048)          0           pool3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 2048)          0           flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense1 (Dense)                   (None, 128)           262272      dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 128)           0           dense1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 128)           0           activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense2 (Dense)                   (None, 128)           16512       dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 1)             129         dense2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 372,173\n",
      "Trainable params: 372,173\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Save model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# go one level up to save final model\n",
    "model_json = model.to_json()\n",
    "with open(\"model_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "model.save(\"model_final.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print(model.summary())\n",
    "# go one level up to save final model for simulator driver.py\n",
    "os.path.dirname(os.path.dirname(cwd))\n",
    "model.save(\"model_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above approch causes to the car to drive on to dirt just after crossing the bridge. Two approach i think is may be usefull first to add more data corresponding to non staright driving as  data is skewed towards driving car in staring line. Second better architecture like Nvidia driving is usefull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia architecture just added max pooling layer for faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](nVidia_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(row, col, ch)))\n",
    "# Crop pixels from top and bottom of image\n",
    "model.add(Cropping2D(cropping=((60, 20), (0, 0))))\n",
    "\n",
    "# Resise data within the neural network\n",
    "model.add(Lambda(resize_normalize))\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation('relu',name='relu1'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model.add(Convolution2D(36, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation('relu',name='relu2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model.add(Convolution2D(48, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation('relu',name='relu3'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "model.add(Activation('relu',name='relu4'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "model.add(Activation('relu',name='relu5'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164))\n",
    "model.add(Activation('relu',name='relu6'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu',name='relu7'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu',name='relu8'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu',name='relu9'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(lr= 0.0001), loss=\"mse\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 162, 322, 3)   0           zeropadding2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)        (None, 82, 322, 3)    0           zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 64, 64, 3)     0           cropping2d_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 32, 24)    1824        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "relu1 (Activation)               (None, 32, 32, 24)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 31, 31, 24)    0           relu1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 16, 36)    21636       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu2 (Activation)               (None, 16, 16, 36)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 15, 15, 36)    0           relu2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 8, 8, 48)      43248       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3 (Activation)               (None, 8, 8, 48)      0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 7, 7, 48)      0           relu3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 7, 7, 64)      27712       maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4 (Activation)               (None, 7, 7, 64)      0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 6, 6, 64)      0           relu4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 6, 6, 64)      36928       maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5 (Activation)               (None, 6, 6, 64)      0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 5, 5, 64)      0           relu5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1600)          0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1164)          1863564     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "relu6 (Activation)               (None, 1164)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 100)           116500      relu6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu7 (Activation)               (None, 100)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 50)            5050        relu7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu8 (Activation)               (None, 50)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 10)            510         relu8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu9 (Activation)               (None, 10)            0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             11          relu9[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 2,116,983\n",
      "Trainable params: 2,116,983\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save every model using Keras checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"Nvidias-check-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_epoch = 8\n",
    "samples_per_epoch = 2000\n",
    "nb_val_samples = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = valid_generator(validation_samples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6428\n",
      "Epoch 1/25\n",
      "(470, 160, 320, 3)\n",
      "(461, 160, 320, 3)\n",
      "  470/20000 [..............................] - ETA: 125s - loss: 0.0202(465, 160, 320, 3)\n",
      "  931/20000 [>.............................] - ETA: 113s - loss: 0.0199(469, 160, 320, 3)\n",
      " 1396/20000 [=>............................] - ETA: 97s - loss: 0.0189 (457, 160, 320, 3)\n",
      " 1865/20000 [=>............................] - ETA: 96s - loss: 0.0204(448, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      " 2322/20000 [==>...........................] - ETA: 89s - loss: 0.0194(466, 160, 320, 3)\n",
      " 2770/20000 [===>..........................] - ETA: 88s - loss: 0.0190(468, 160, 320, 3)\n",
      " 3227/20000 [===>..........................] - ETA: 83s - loss: 0.0192(467, 160, 320, 3)\n",
      " 3693/20000 [====>.........................] - ETA: 81s - loss: 0.0194(456, 160, 320, 3)\n",
      " 4161/20000 [=====>........................] - ETA: 80s - loss: 0.0190(460, 160, 320, 3)\n",
      " 4628/20000 [=====>........................] - ETA: 75s - loss: 0.0192(476, 160, 320, 3)\n",
      " 5084/20000 [======>.......................] - ETA: 74s - loss: 0.0195(465, 160, 320, 3)\n",
      " 5544/20000 [=======>......................] - ETA: 70s - loss: 0.0195(454, 160, 320, 3)\n",
      " 6020/20000 [========>.....................] - ETA: 68s - loss: 0.0198(458, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      " 6485/20000 [========>.....................] - ETA: 65s - loss: 0.0198(460, 160, 320, 3)\n",
      "(467, 160, 320, 3)\n",
      " 6939/20000 [=========>....................] - ETA: 64s - loss: 0.0198(460, 160, 320, 3)\n",
      "(452, 160, 320, 3)\n",
      " 7397/20000 [==========>...................] - ETA: 62s - loss: 0.0196(469, 160, 320, 3)\n",
      "(464, 160, 320, 3)\n",
      " 7859/20000 [==========>...................] - ETA: 59s - loss: 0.0196(463, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      " 8319/20000 [===========>..................] - ETA: 57s - loss: 0.0194(458, 160, 320, 3)\n",
      "(462, 160, 320, 3)\n",
      " 8786/20000 [============>.................] - ETA: 55s - loss: 0.0193(472, 160, 320, 3)\n",
      "(463, 160, 320, 3)\n",
      "(454, 160, 320, 3)\n",
      " 9246/20000 [============>.................] - ETA: 53s - loss: 0.0191(470, 160, 320, 3)\n",
      " 9698/20000 [=============>................] - ETA: 50s - loss: 0.0190(458, 160, 320, 3)\n",
      "10167/20000 [==============>...............] - ETA: 48s - loss: 0.0188(455, 160, 320, 3)\n",
      "10631/20000 [==============>...............] - ETA: 45s - loss: 0.0191(454, 160, 320, 3)\n",
      "11094/20000 [===============>..............] - ETA: 43s - loss: 0.0192(466, 160, 320, 3)\n",
      "11551/20000 [================>.............] - ETA: 41s - loss: 0.0195(461, 160, 320, 3)\n",
      "12009/20000 [=================>............] - ETA: 39s - loss: 0.0195(454, 160, 320, 3)\n",
      "12471/20000 [=================>............] - ETA: 37s - loss: 0.0195(474, 160, 320, 3)\n",
      "12943/20000 [==================>...........] - ETA: 34s - loss: 0.0196(465, 160, 320, 3)\n",
      "13406/20000 [===================>..........] - ETA: 32s - loss: 0.0195(461, 160, 320, 3)\n",
      "13860/20000 [===================>..........] - ETA: 30s - loss: 0.0195(463, 160, 320, 3)\n",
      "14330/20000 [====================>.........] - ETA: 27s - loss: 0.0198(461, 160, 320, 3)\n",
      "14788/20000 [=====================>........] - ETA: 25s - loss: 0.0199(457, 160, 320, 3)\n",
      "15243/20000 [=====================>........] - ETA: 23s - loss: 0.0198(459, 160, 320, 3)\n",
      "15697/20000 [======================>.......] - ETA: 20s - loss: 0.0197(462, 160, 320, 3)\n",
      "16163/20000 [=======================>......] - ETA: 18s - loss: 0.0198(455, 160, 320, 3)\n",
      "16624/20000 [=======================>......] - ETA: 16s - loss: 0.0198(459, 160, 320, 3)\n",
      "17078/20000 [========================>.....] - ETA: 14s - loss: 0.0197(463, 160, 320, 3)\n",
      "17552/20000 [=========================>....] - ETA: 11s - loss: 0.0196(475, 160, 320, 3)\n",
      "18017/20000 [==========================>...] - ETA: 9s - loss: 0.0196 (456, 160, 320, 3)\n",
      "18478/20000 [==========================>...] - ETA: 7s - loss: 0.0197(100, 160, 320, 3)\n",
      "18941/20000 [===========================>..] - ETA: 5s - loss: 0.0198(456, 160, 320, 3)\n",
      "19402/20000 [============================>.] - ETA: 2s - loss: 0.0198(460, 160, 320, 3)\n",
      "19859/20000 [============================>.] - ETA: 0s - loss: 0.0198(466, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/udacity/udacitycCarND/Behavioral Cloning/data/data\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00000: saving model to Nvidias-check-00-0.0116.hdf5\n",
      "20318/20000 [==============================] - 102s - loss: 0.0198 - val_loss: 0.0116\n",
      "Epoch 2/25\n",
      "(457, 160, 320, 3)\n",
      "  462/20000 [..............................] - ETA: 84s - loss: 0.0236(464, 160, 320, 3)\n",
      "  917/20000 [>.............................] - ETA: 82s - loss: 0.0228(459, 160, 320, 3)\n",
      " 1376/20000 [=>............................] - ETA: 79s - loss: 0.0216(463, 160, 320, 3)\n",
      " 1839/20000 [=>............................] - ETA: 77s - loss: 0.0220(467, 160, 320, 3)\n",
      " 2314/20000 [==>...........................] - ETA: 80s - loss: 0.0210(448, 160, 320, 3)\n",
      " 2870/20000 [===>..........................] - ETA: 77s - loss: 0.0205(442, 160, 320, 3)\n",
      "(455, 160, 320, 3)\n",
      " 3326/20000 [===>..........................] - ETA: 77s - loss: 0.0208(463, 160, 320, 3)\n",
      " 3786/20000 [====>.........................] - ETA: 77s - loss: 0.0206(461, 160, 320, 3)\n",
      " 4252/20000 [=====>........................] - ETA: 73s - loss: 0.0201(465, 160, 320, 3)\n",
      " 4709/20000 [======>.......................] - ETA: 72s - loss: 0.0202(455, 160, 320, 3)\n",
      " 5173/20000 [======>.......................] - ETA: 69s - loss: 0.0199(473, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 68s - loss: 0.0196(456, 160, 320, 3)\n",
      " 6095/20000 [========>.....................] - ETA: 65s - loss: 0.0196(449, 160, 320, 3)\n",
      " 6562/20000 [========>.....................] - ETA: 63s - loss: 0.0196(457, 160, 320, 3)\n",
      " 7010/20000 [=========>....................] - ETA: 62s - loss: 0.0198(472, 160, 320, 3)\n",
      " 7452/20000 [==========>...................] - ETA: 59s - loss: 0.0198(460, 160, 320, 3)\n",
      " 7907/20000 [==========>...................] - ETA: 58s - loss: 0.0199(459, 160, 320, 3)\n",
      " 8370/20000 [===========>..................] - ETA: 55s - loss: 0.0198(451, 160, 320, 3)\n",
      " 8831/20000 [============>.................] - ETA: 53s - loss: 0.0199(459, 160, 320, 3)\n",
      " 9296/20000 [============>.................] - ETA: 51s - loss: 0.0199(468, 160, 320, 3)\n",
      " 9751/20000 [=============>................] - ETA: 49s - loss: 0.0198(450, 160, 320, 3)\n",
      "10224/20000 [==============>...............] - ETA: 47s - loss: 0.0198(465, 160, 320, 3)\n",
      "10680/20000 [===============>..............] - ETA: 45s - loss: 0.0198(456, 160, 320, 3)\n",
      "11129/20000 [===============>..............] - ETA: 43s - loss: 0.0196(465, 160, 320, 3)\n",
      "11586/20000 [================>.............] - ETA: 40s - loss: 0.0195(467, 160, 320, 3)\n",
      "12058/20000 [=================>............] - ETA: 38s - loss: 0.0194(464, 160, 320, 3)\n",
      "12518/20000 [=================>............] - ETA: 36s - loss: 0.0193(464, 160, 320, 3)\n",
      "12977/20000 [==================>...........] - ETA: 34s - loss: 0.0193(464, 160, 320, 3)\n",
      "13428/20000 [===================>..........] - ETA: 31s - loss: 0.0196(454, 160, 320, 3)\n",
      "13887/20000 [===================>..........] - ETA: 29s - loss: 0.0196(464, 160, 320, 3)\n",
      "14355/20000 [====================>.........] - ETA: 27s - loss: 0.0196(446, 160, 320, 3)\n",
      "14805/20000 [=====================>........] - ETA: 25s - loss: 0.0196(457, 160, 320, 3)\n",
      "15270/20000 [=====================>........] - ETA: 22s - loss: 0.0197(455, 160, 320, 3)\n",
      "15726/20000 [======================>.......] - ETA: 20s - loss: 0.0198(450, 160, 320, 3)\n",
      "16191/20000 [=======================>......] - ETA: 18s - loss: 0.0197(465, 160, 320, 3)\n",
      "16658/20000 [=======================>......] - ETA: 16s - loss: 0.0197(476, 160, 320, 3)\n",
      "17122/20000 [========================>.....] - ETA: 13s - loss: 0.0198(462, 160, 320, 3)\n",
      "17586/20000 [=========================>....] - ETA: 11s - loss: 0.0198(464, 160, 320, 3)\n",
      "18050/20000 [==========================>...] - ETA: 9s - loss: 0.0196 (457, 160, 320, 3)\n",
      "18504/20000 [==========================>...] - ETA: 7s - loss: 0.0196(462, 160, 320, 3)\n",
      "18968/20000 [===========================>..] - ETA: 4s - loss: 0.0197(466, 160, 320, 3)\n",
      "19414/20000 [============================>.] - ETA: 2s - loss: 0.0196(463, 160, 320, 3)\n",
      "19871/20000 [============================>.] - ETA: 0s - loss: 0.0196(456, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00001: saving model to Nvidias-check-01-0.0122.hdf5\n",
      "20326/20000 [==============================] - 104s - loss: 0.0195 - val_loss: 0.0122\n",
      "Epoch 3/25\n",
      "(467, 160, 320, 3)\n",
      "  450/20000 [..............................] - ETA: 76s - loss: 0.0200(454, 160, 320, 3)\n",
      "  915/20000 [>.............................] - ETA: 85s - loss: 0.0212(100, 160, 320, 3)\n",
      " 1391/20000 [=>............................] - ETA: 84s - loss: 0.0209(456, 160, 320, 3)\n",
      " 1853/20000 [=>............................] - ETA: 81s - loss: 0.0213(463, 160, 320, 3)\n",
      " 2317/20000 [==>...........................] - ETA: 77s - loss: 0.0206(459, 160, 320, 3)\n",
      " 2774/20000 [===>..........................] - ETA: 75s - loss: 0.0213(471, 160, 320, 3)\n",
      " 3236/20000 [===>..........................] - ETA: 73s - loss: 0.0218(459, 160, 320, 3)\n",
      " 3702/20000 [====>.........................] - ETA: 71s - loss: 0.0222(455, 160, 320, 3)\n",
      " 4165/20000 [=====>........................] - ETA: 69s - loss: 0.0217(454, 160, 320, 3)\n",
      " 4621/20000 [=====>........................] - ETA: 67s - loss: 0.0222(459, 160, 320, 3)\n",
      " 5088/20000 [======>.......................] - ETA: 66s - loss: 0.0216(459, 160, 320, 3)\n",
      " 5642/20000 [=======>......................] - ETA: 64s - loss: 0.0215(463, 160, 320, 3)\n",
      "(474, 160, 320, 3)\n",
      " 6098/20000 [========>.....................] - ETA: 62s - loss: 0.0214(456, 160, 320, 3)\n",
      " 6561/20000 [========>.....................] - ETA: 60s - loss: 0.0211(461, 160, 320, 3)\n",
      " 7020/20000 [=========>....................] - ETA: 58s - loss: 0.0207(462, 160, 320, 3)\n",
      " 7491/20000 [==========>...................] - ETA: 55s - loss: 0.0207(463, 160, 320, 3)\n",
      " 7950/20000 [==========>...................] - ETA: 53s - loss: 0.0205(459, 160, 320, 3)\n",
      " 8405/20000 [===========>..................] - ETA: 51s - loss: 0.0202(459, 160, 320, 3)\n",
      " 8859/20000 [============>.................] - ETA: 49s - loss: 0.0201(463, 160, 320, 3)\n",
      " 9318/20000 [============>.................] - ETA: 46s - loss: 0.0204(456, 160, 320, 3)\n",
      " 9777/20000 [=============>................] - ETA: 44s - loss: 0.0204(459, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 42s - loss: 0.0203(461, 160, 320, 3)\n",
      "10714/20000 [===============>..............] - ETA: 40s - loss: 0.0202(469, 160, 320, 3)\n",
      "11170/20000 [===============>..............] - ETA: 38s - loss: 0.0204(460, 160, 320, 3)\n",
      "11631/20000 [================>.............] - ETA: 36s - loss: 0.0203(455, 160, 320, 3)\n",
      "12093/20000 [=================>............] - ETA: 34s - loss: 0.0202(453, 160, 320, 3)\n",
      "12556/20000 [=================>............] - ETA: 32s - loss: 0.0201(454, 160, 320, 3)\n",
      "13015/20000 [==================>...........] - ETA: 30s - loss: 0.0200(464, 160, 320, 3)\n",
      "13474/20000 [===================>..........] - ETA: 28s - loss: 0.0200(464, 160, 320, 3)\n",
      "13937/20000 [===================>..........] - ETA: 26s - loss: 0.0199(457, 160, 320, 3)\n",
      "14393/20000 [====================>.........] - ETA: 24s - loss: 0.0199(466, 160, 320, 3)\n",
      "14852/20000 [=====================>........] - ETA: 22s - loss: 0.0198(467, 160, 320, 3)\n",
      "15313/20000 [=====================>........] - ETA: 20s - loss: 0.0197(470, 160, 320, 3)\n",
      "15782/20000 [======================>.......] - ETA: 18s - loss: 0.0196(460, 160, 320, 3)\n",
      "16242/20000 [=======================>......] - ETA: 16s - loss: 0.0198(457, 160, 320, 3)\n",
      "16697/20000 [========================>.....] - ETA: 14s - loss: 0.0198(461, 160, 320, 3)\n",
      "17150/20000 [========================>.....] - ETA: 12s - loss: 0.0198(458, 160, 320, 3)\n",
      "17604/20000 [=========================>....] - ETA: 10s - loss: 0.0197(453, 160, 320, 3)\n",
      "18068/20000 [==========================>...] - ETA: 8s - loss: 0.0198 (463, 160, 320, 3)\n",
      "18532/20000 [==========================>...] - ETA: 6s - loss: 0.0198(456, 160, 320, 3)\n",
      "18989/20000 [===========================>..] - ETA: 4s - loss: 0.0197(460, 160, 320, 3)\n",
      "19455/20000 [============================>.] - ETA: 2s - loss: 0.0197(461, 160, 320, 3)\n",
      "19922/20000 [============================>.] - ETA: 0s - loss: 0.0199(453, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00002: saving model to Nvidias-check-02-0.0114.hdf5\n",
      "20392/20000 [==============================] - 94s - loss: 0.0198 - val_loss: 0.0114\n",
      "Epoch 4/25\n",
      "(463, 160, 320, 3)\n",
      "  460/20000 [..............................] - ETA: 83s - loss: 0.0153(459, 160, 320, 3)\n",
      "  917/20000 [>.............................] - ETA: 81s - loss: 0.0158(473, 160, 320, 3)\n",
      " 1378/20000 [=>............................] - ETA: 79s - loss: 0.0195(451, 160, 320, 3)\n",
      " 1836/20000 [=>............................] - ETA: 77s - loss: 0.0189(466, 160, 320, 3)\n",
      " 2289/20000 [==>...........................] - ETA: 75s - loss: 0.0189(448, 160, 320, 3)\n",
      " 2752/20000 [===>..........................] - ETA: 73s - loss: 0.0185(460, 160, 320, 3)\n",
      " 3208/20000 [===>..........................] - ETA: 71s - loss: 0.0189(462, 160, 320, 3)\n",
      " 3668/20000 [====>.........................] - ETA: 69s - loss: 0.0192(98, 160, 320, 3)\n",
      " 4129/20000 [=====>........................] - ETA: 66s - loss: 0.0195(460, 160, 320, 3)\n",
      " 4582/20000 [=====>........................] - ETA: 64s - loss: 0.0195(462, 160, 320, 3)\n",
      " 5045/20000 [======>.......................] - ETA: 61s - loss: 0.0196(464, 160, 320, 3)\n",
      " 5504/20000 [=======>......................] - ETA: 61s - loss: 0.0199(463, 160, 320, 3)\n",
      " 5977/20000 [=======>......................] - ETA: 58s - loss: 0.0206(457, 160, 320, 3)\n",
      " 6428/20000 [========>.....................] - ETA: 56s - loss: 0.0209(452, 160, 320, 3)\n",
      " 6894/20000 [=========>....................] - ETA: 55s - loss: 0.0209(469, 160, 320, 3)\n",
      " 7342/20000 [==========>...................] - ETA: 54s - loss: 0.0212(464, 160, 320, 3)\n",
      " 7802/20000 [==========>...................] - ETA: 52s - loss: 0.0209(475, 160, 320, 3)\n",
      " 8362/20000 [===========>..................] - ETA: 50s - loss: 0.0207(464, 160, 320, 3)\n",
      "(461, 160, 320, 3)\n",
      " 8822/20000 [============>.................] - ETA: 48s - loss: 0.0208(458, 160, 320, 3)\n",
      " 9284/20000 [============>.................] - ETA: 47s - loss: 0.0207(469, 160, 320, 3)\n",
      " 9748/20000 [=============>................] - ETA: 44s - loss: 0.0205(460, 160, 320, 3)\n",
      "10211/20000 [==============>...............] - ETA: 43s - loss: 0.0204(466, 160, 320, 3)\n",
      "10668/20000 [===============>..............] - ETA: 41s - loss: 0.0202(460, 160, 320, 3)\n",
      "11120/20000 [===============>..............] - ETA: 39s - loss: 0.0201(459, 160, 320, 3)\n",
      "11589/20000 [================>.............] - ETA: 37s - loss: 0.0201(451, 160, 320, 3)\n",
      "12053/20000 [=================>............] - ETA: 35s - loss: 0.0203(463, 160, 320, 3)\n",
      "12528/20000 [=================>............] - ETA: 33s - loss: 0.0203(464, 160, 320, 3)\n",
      "12992/20000 [==================>...........] - ETA: 31s - loss: 0.0201(456, 160, 320, 3)\n",
      "13453/20000 [===================>..........] - ETA: 29s - loss: 0.0201(454, 160, 320, 3)\n",
      "13911/20000 [===================>..........] - ETA: 27s - loss: 0.0200(463, 160, 320, 3)\n",
      "14380/20000 [====================>.........] - ETA: 25s - loss: 0.0200(461, 160, 320, 3)\n",
      "14840/20000 [=====================>........] - ETA: 22s - loss: 0.0200(477, 160, 320, 3)\n",
      "15306/20000 [=====================>........] - ETA: 21s - loss: 0.0199(461, 160, 320, 3)\n",
      "15766/20000 [======================>.......] - ETA: 18s - loss: 0.0199(458, 160, 320, 3)\n",
      "16225/20000 [=======================>......] - ETA: 16s - loss: 0.0199(465, 160, 320, 3)\n",
      "16676/20000 [========================>.....] - ETA: 14s - loss: 0.0198(464, 160, 320, 3)\n",
      "17139/20000 [========================>.....] - ETA: 12s - loss: 0.0197(463, 160, 320, 3)\n",
      "17603/20000 [=========================>....] - ETA: 10s - loss: 0.0196(467, 160, 320, 3)\n",
      "18059/20000 [==========================>...] - ETA: 8s - loss: 0.0196 (471, 160, 320, 3)\n",
      "18513/20000 [==========================>...] - ETA: 6s - loss: 0.0195(468, 160, 320, 3)\n",
      "18976/20000 [===========================>..] - ETA: 4s - loss: 0.0197(455, 160, 320, 3)\n",
      "19437/20000 [============================>.] - ETA: 2s - loss: 0.0197(460, 160, 320, 3)\n",
      "19914/20000 [============================>.] - ETA: 0s - loss: 0.0197(457, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00003: saving model to Nvidias-check-03-0.0111.hdf5\n",
      "20375/20000 [==============================] - 98s - loss: 0.0197 - val_loss: 0.0111\n",
      "Epoch 5/25\n",
      "(462, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 97s - loss: 0.0209(455, 160, 320, 3)\n",
      "  923/20000 [>.............................] - ETA: 95s - loss: 0.0212(461, 160, 320, 3)\n",
      " 1387/20000 [=>............................] - ETA: 87s - loss: 0.0196(457, 160, 320, 3)\n",
      " 1850/20000 [=>............................] - ETA: 82s - loss: 0.0192(451, 160, 320, 3)\n",
      " 2317/20000 [==>...........................] - ETA: 78s - loss: 0.0202(469, 160, 320, 3)\n",
      " 2788/20000 [===>..........................] - ETA: 75s - loss: 0.0201(466, 160, 320, 3)\n",
      " 3256/20000 [===>..........................] - ETA: 72s - loss: 0.0196(470, 160, 320, 3)\n",
      " 3711/20000 [====>.........................] - ETA: 70s - loss: 0.0192(462, 160, 320, 3)\n",
      " 4171/20000 [=====>........................] - ETA: 68s - loss: 0.0193(455, 160, 320, 3)\n",
      " 4628/20000 [=====>........................] - ETA: 66s - loss: 0.0192(463, 160, 320, 3)\n",
      " 5090/20000 [======>.......................] - ETA: 64s - loss: 0.0187(461, 160, 320, 3)\n",
      " 5545/20000 [=======>......................] - ETA: 62s - loss: 0.0187(472, 160, 320, 3)\n",
      " 6006/20000 [========>.....................] - ETA: 60s - loss: 0.0185(458, 160, 320, 3)\n",
      " 6463/20000 [========>.....................] - ETA: 58s - loss: 0.0189(105, 160, 320, 3)\n",
      " 6914/20000 [=========>....................] - ETA: 56s - loss: 0.0191(460, 160, 320, 3)\n",
      " 7383/20000 [==========>...................] - ETA: 54s - loss: 0.0192(461, 160, 320, 3)\n",
      " 7849/20000 [==========>...................] - ETA: 52s - loss: 0.0190(459, 160, 320, 3)\n",
      " 8319/20000 [===========>..................] - ETA: 50s - loss: 0.0193(460, 160, 320, 3)\n",
      " 8781/20000 [============>.................] - ETA: 47s - loss: 0.0200(457, 160, 320, 3)\n",
      " 9236/20000 [============>.................] - ETA: 46s - loss: 0.0201(451, 160, 320, 3)\n",
      " 9699/20000 [=============>................] - ETA: 44s - loss: 0.0199(460, 160, 320, 3)\n",
      "10160/20000 [==============>...............] - ETA: 42s - loss: 0.0201(463, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10632/20000 [==============>...............] - ETA: 39s - loss: 0.0199(449, 160, 320, 3)\n",
      "11195/20000 [===============>..............] - ETA: 37s - loss: 0.0198(467, 160, 320, 3)\n",
      "(470, 160, 320, 3)\n",
      "11655/20000 [================>.............] - ETA: 35s - loss: 0.0199(450, 160, 320, 3)\n",
      "12116/20000 [=================>............] - ETA: 33s - loss: 0.0198(460, 160, 320, 3)\n",
      "12575/20000 [=================>............] - ETA: 31s - loss: 0.0197(461, 160, 320, 3)\n",
      "13035/20000 [==================>...........] - ETA: 29s - loss: 0.0197(466, 160, 320, 3)\n",
      "13492/20000 [===================>..........] - ETA: 27s - loss: 0.0196(454, 160, 320, 3)\n",
      "13943/20000 [===================>..........] - ETA: 25s - loss: 0.0195(464, 160, 320, 3)\n",
      "14403/20000 [====================>.........] - ETA: 23s - loss: 0.0195(468, 160, 320, 3)\n",
      "14866/20000 [=====================>........] - ETA: 21s - loss: 0.0195(463, 160, 320, 3)\n",
      "15315/20000 [=====================>........] - ETA: 19s - loss: 0.0195(463, 160, 320, 3)\n",
      "15782/20000 [======================>.......] - ETA: 17s - loss: 0.0195(458, 160, 320, 3)\n",
      "16252/20000 [=======================>......] - ETA: 15s - loss: 0.0195(458, 160, 320, 3)\n",
      "16702/20000 [========================>.....] - ETA: 13s - loss: 0.0196(458, 160, 320, 3)\n",
      "17162/20000 [========================>.....] - ETA: 12s - loss: 0.0196(458, 160, 320, 3)\n",
      "17623/20000 [=========================>....] - ETA: 10s - loss: 0.0196(458, 160, 320, 3)\n",
      "18089/20000 [==========================>...] - ETA: 8s - loss: 0.0196 (475, 160, 320, 3)\n",
      "18543/20000 [==========================>...] - ETA: 6s - loss: 0.0195(467, 160, 320, 3)\n",
      "19007/20000 [===========================>..] - ETA: 4s - loss: 0.0195(451, 160, 320, 3)\n",
      "19475/20000 [============================>.] - ETA: 2s - loss: 0.0194(461, 160, 320, 3)\n",
      "19938/20000 [============================>.] - ETA: 0s - loss: 0.0193(459, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00004: saving model to Nvidias-check-04-0.0120.hdf5\n",
      "20401/20000 [==============================] - 91s - loss: 0.0193 - val_loss: 0.0120\n",
      "Epoch 6/25\n",
      "(462, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 81s - loss: 0.0187(473, 160, 320, 3)\n",
      "  916/20000 [>.............................] - ETA: 78s - loss: 0.0181(453, 160, 320, 3)\n",
      " 1374/20000 [=>............................] - ETA: 76s - loss: 0.0195(457, 160, 320, 3)\n",
      " 1832/20000 [=>............................] - ETA: 74s - loss: 0.0196(463, 160, 320, 3)\n",
      " 2290/20000 [==>...........................] - ETA: 72s - loss: 0.0194(469, 160, 320, 3)\n",
      " 2765/20000 [===>..........................] - ETA: 70s - loss: 0.0189(465, 160, 320, 3)\n",
      " 3232/20000 [===>..........................] - ETA: 68s - loss: 0.0191(470, 160, 320, 3)\n",
      " 3683/20000 [====>.........................] - ETA: 66s - loss: 0.0192(454, 160, 320, 3)\n",
      " 4144/20000 [=====>........................] - ETA: 66s - loss: 0.0187(460, 160, 320, 3)\n",
      " 4603/20000 [=====>........................] - ETA: 64s - loss: 0.0184(460, 160, 320, 3)\n",
      " 5065/20000 [======>.......................] - ETA: 62s - loss: 0.0189(464, 160, 320, 3)\n",
      " 5538/20000 [=======>......................] - ETA: 61s - loss: 0.0187(474, 160, 320, 3)\n",
      " 5991/20000 [=======>......................] - ETA: 58s - loss: 0.0187(460, 160, 320, 3)\n",
      " 6448/20000 [========>.....................] - ETA: 58s - loss: 0.0183(464, 160, 320, 3)\n",
      " 6911/20000 [=========>....................] - ETA: 55s - loss: 0.0188(451, 160, 320, 3)\n",
      " 7380/20000 [==========>...................] - ETA: 54s - loss: 0.0187(462, 160, 320, 3)\n",
      " 7845/20000 [==========>...................] - ETA: 52s - loss: 0.0187(464, 160, 320, 3)\n",
      " 8315/20000 [===========>..................] - ETA: 51s - loss: 0.0185(462, 160, 320, 3)\n",
      " 8769/20000 [============>.................] - ETA: 48s - loss: 0.0185(465, 160, 320, 3)\n",
      " 9229/20000 [============>.................] - ETA: 46s - loss: 0.0187(95, 160, 320, 3)\n",
      " 9689/20000 [=============>................] - ETA: 45s - loss: 0.0188(467, 160, 320, 3)\n",
      "10153/20000 [==============>...............] - ETA: 42s - loss: 0.0187(472, 160, 320, 3)\n",
      "10627/20000 [==============>...............] - ETA: 41s - loss: 0.0186(457, 160, 320, 3)\n",
      "11087/20000 [===============>..............] - ETA: 39s - loss: 0.0188(464, 160, 320, 3)\n",
      "11551/20000 [================>.............] - ETA: 37s - loss: 0.0193(444, 160, 320, 3)\n",
      "12002/20000 [=================>............] - ETA: 35s - loss: 0.0194(459, 160, 320, 3)\n",
      "12464/20000 [=================>............] - ETA: 33s - loss: 0.0194(462, 160, 320, 3)\n",
      "12928/20000 [==================>...........] - ETA: 31s - loss: 0.0195(468, 160, 320, 3)\n",
      "13390/20000 [===================>..........] - ETA: 29s - loss: 0.0194(467, 160, 320, 3)\n",
      "13950/20000 [===================>..........] - ETA: 27s - loss: 0.0195(470, 160, 320, 3)\n",
      "(461, 160, 320, 3)\n",
      "14417/20000 [====================>.........] - ETA: 24s - loss: 0.0196(456, 160, 320, 3)\n",
      "14889/20000 [=====================>........] - ETA: 22s - loss: 0.0195(458, 160, 320, 3)\n",
      "15346/20000 [======================>.......] - ETA: 20s - loss: 0.0194(475, 160, 320, 3)\n",
      "15810/20000 [======================>.......] - ETA: 18s - loss: 0.0194(463, 160, 320, 3)\n",
      "16254/20000 [=======================>......] - ETA: 16s - loss: 0.0194(463, 160, 320, 3)\n",
      "16713/20000 [========================>.....] - ETA: 14s - loss: 0.0193(470, 160, 320, 3)\n",
      "17175/20000 [========================>.....] - ETA: 12s - loss: 0.0193(450, 160, 320, 3)\n",
      "17643/20000 [=========================>....] - ETA: 10s - loss: 0.0193(450, 160, 320, 3)\n",
      "18110/20000 [==========================>...] - ETA: 8s - loss: 0.0194 (457, 160, 320, 3)\n",
      "18580/20000 [==========================>...] - ETA: 6s - loss: 0.0193(450, 160, 320, 3)\n",
      "19041/20000 [===========================>..] - ETA: 4s - loss: 0.0193(450, 160, 320, 3)\n",
      "19497/20000 [============================>.] - ETA: 2s - loss: 0.0194(471, 160, 320, 3)\n",
      "19955/20000 [============================>.] - ETA: 0s - loss: 0.0193(463, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00005: saving model to Nvidias-check-05-0.0117.hdf5\n",
      "20430/20000 [==============================] - 97s - loss: 0.0193 - val_loss: 0.0117\n",
      "Epoch 7/25\n",
      "(460, 160, 320, 3)\n",
      "  463/20000 [..............................] - ETA: 98s - loss: 0.0183(456, 160, 320, 3)\n",
      "  926/20000 [>.............................] - ETA: 84s - loss: 0.0182(460, 160, 320, 3)\n",
      " 1396/20000 [=>............................] - ETA: 88s - loss: 0.0189(463, 160, 320, 3)\n",
      " 1846/20000 [=>............................] - ETA: 82s - loss: 0.0176(458, 160, 320, 3)\n",
      " 2296/20000 [==>...........................] - ETA: 78s - loss: 0.0170(466, 160, 320, 3)\n",
      " 2753/20000 [===>..........................] - ETA: 75s - loss: 0.0168(456, 160, 320, 3)\n",
      " 3203/20000 [===>..........................] - ETA: 72s - loss: 0.0171(462, 160, 320, 3)\n",
      " 3653/20000 [====>.........................] - ETA: 69s - loss: 0.0169(460, 160, 320, 3)\n",
      " 4124/20000 [=====>........................] - ETA: 66s - loss: 0.0176(465, 160, 320, 3)\n",
      " 4587/20000 [=====>........................] - ETA: 64s - loss: 0.0179(462, 160, 320, 3)\n",
      " 5047/20000 [======>.......................] - ETA: 62s - loss: 0.0181(461, 160, 320, 3)\n",
      " 5503/20000 [=======>......................] - ETA: 60s - loss: 0.0179(460, 160, 320, 3)\n",
      " 5963/20000 [=======>......................] - ETA: 58s - loss: 0.0182(455, 160, 320, 3)\n",
      " 6426/20000 [========>.....................] - ETA: 56s - loss: 0.0183(458, 160, 320, 3)\n",
      " 6884/20000 [=========>....................] - ETA: 54s - loss: 0.0181(466, 160, 320, 3)\n",
      " 7350/20000 [==========>...................] - ETA: 52s - loss: 0.0181(452, 160, 320, 3)\n",
      " 7806/20000 [==========>...................] - ETA: 50s - loss: 0.0185(466, 160, 320, 3)\n",
      " 8268/20000 [===========>..................] - ETA: 48s - loss: 0.0185(450, 160, 320, 3)\n",
      " 8728/20000 [============>.................] - ETA: 46s - loss: 0.0184(472, 160, 320, 3)\n",
      " 9193/20000 [============>.................] - ETA: 44s - loss: 0.0183(467, 160, 320, 3)\n",
      " 9655/20000 [=============>................] - ETA: 43s - loss: 0.0185(448, 160, 320, 3)\n",
      "10116/20000 [==============>...............] - ETA: 41s - loss: 0.0184(455, 160, 320, 3)\n",
      "10576/20000 [==============>...............] - ETA: 39s - loss: 0.0183(470, 160, 320, 3)\n",
      "11031/20000 [===============>..............] - ETA: 37s - loss: 0.0182(450, 160, 320, 3)\n",
      "11489/20000 [================>.............] - ETA: 35s - loss: 0.0182(471, 160, 320, 3)\n",
      "11955/20000 [================>.............] - ETA: 33s - loss: 0.0184(104, 160, 320, 3)\n",
      "12407/20000 [=================>............] - ETA: 31s - loss: 0.0184(461, 160, 320, 3)\n",
      "12873/20000 [==================>...........] - ETA: 29s - loss: 0.0183(463, 160, 320, 3)\n",
      "13323/20000 [==================>...........] - ETA: 27s - loss: 0.0183(449, 160, 320, 3)\n",
      "13795/20000 [===================>..........] - ETA: 25s - loss: 0.0184(467, 160, 320, 3)\n",
      "14262/20000 [====================>.........] - ETA: 24s - loss: 0.0188(453, 160, 320, 3)\n",
      "14710/20000 [=====================>........] - ETA: 22s - loss: 0.0188(451, 160, 320, 3)\n",
      "15165/20000 [=====================>........] - ETA: 20s - loss: 0.0188(471, 160, 320, 3)\n",
      "15635/20000 [======================>.......] - ETA: 18s - loss: 0.0190(457, 160, 320, 3)\n",
      "16085/20000 [=======================>......] - ETA: 16s - loss: 0.0189(461, 160, 320, 3)\n",
      "16660/20000 [=======================>......] - ETA: 14s - loss: 0.0189(469, 160, 320, 3)\n",
      "(455, 160, 320, 3)\n",
      "17121/20000 [========================>.....] - ETA: 12s - loss: 0.0189(458, 160, 320, 3)\n",
      "17584/20000 [=========================>....] - ETA: 10s - loss: 0.0188(458, 160, 320, 3)\n",
      "18033/20000 [==========================>...] - ETA: 8s - loss: 0.0187 (462, 160, 320, 3)\n",
      "18500/20000 [==========================>...] - ETA: 6s - loss: 0.0187(461, 160, 320, 3)\n",
      "18953/20000 [===========================>..] - ETA: 4s - loss: 0.0186(465, 160, 320, 3)\n",
      "19404/20000 [============================>.] - ETA: 2s - loss: 0.0185(450, 160, 320, 3)\n",
      "19875/20000 [============================>.] - ETA: 0s - loss: 0.0186(462, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00006: saving model to Nvidias-check-06-0.0115.hdf5\n",
      "20332/20000 [==============================] - 93s - loss: 0.0187 - val_loss: 0.0115\n",
      "Epoch 8/25\n",
      "(464, 160, 320, 3)\n",
      "  461/20000 [..............................] - ETA: 104s - loss: 0.0197(461, 160, 320, 3)\n",
      "  930/20000 [>.............................] - ETA: 87s - loss: 0.0172 (449, 160, 320, 3)\n",
      " 1385/20000 [=>............................] - ETA: 80s - loss: 0.0171(457, 160, 320, 3)\n",
      " 1843/20000 [=>............................] - ETA: 83s - loss: 0.0178(472, 160, 320, 3)\n",
      " 2301/20000 [==>...........................] - ETA: 79s - loss: 0.0179(467, 160, 320, 3)\n",
      " 2763/20000 [===>..........................] - ETA: 75s - loss: 0.0181(455, 160, 320, 3)\n",
      " 3224/20000 [===>..........................] - ETA: 75s - loss: 0.0182(457, 160, 320, 3)\n",
      " 3689/20000 [====>.........................] - ETA: 72s - loss: 0.0180(468, 160, 320, 3)\n",
      " 4139/20000 [=====>........................] - ETA: 69s - loss: 0.0184(461, 160, 320, 3)\n",
      " 4601/20000 [=====>........................] - ETA: 68s - loss: 0.0180(453, 160, 320, 3)\n",
      " 5065/20000 [======>.......................] - ETA: 66s - loss: 0.0178(464, 160, 320, 3)\n",
      " 5526/20000 [=======>......................] - ETA: 63s - loss: 0.0179(452, 160, 320, 3)\n",
      " 5975/20000 [=======>......................] - ETA: 61s - loss: 0.0180(462, 160, 320, 3)\n",
      " 6432/20000 [========>.....................] - ETA: 58s - loss: 0.0180(457, 160, 320, 3)\n",
      " 6904/20000 [=========>....................] - ETA: 56s - loss: 0.0183(463, 160, 320, 3)\n",
      " 7371/20000 [==========>...................] - ETA: 54s - loss: 0.0183(456, 160, 320, 3)\n",
      " 7826/20000 [==========>...................] - ETA: 52s - loss: 0.0185(465, 160, 320, 3)\n",
      " 8283/20000 [===========>..................] - ETA: 49s - loss: 0.0184(455, 160, 320, 3)\n",
      " 8751/20000 [============>.................] - ETA: 48s - loss: 0.0185(454, 160, 320, 3)\n",
      " 9212/20000 [============>.................] - ETA: 46s - loss: 0.0187(466, 160, 320, 3)\n",
      " 9665/20000 [=============>................] - ETA: 44s - loss: 0.0185(449, 160, 320, 3)\n",
      "10129/20000 [==============>...............] - ETA: 42s - loss: 0.0186(451, 160, 320, 3)\n",
      "10581/20000 [==============>...............] - ETA: 40s - loss: 0.0189(480, 160, 320, 3)\n",
      "11043/20000 [===============>..............] - ETA: 38s - loss: 0.0190(474, 160, 320, 3)\n",
      "11500/20000 [================>.............] - ETA: 37s - loss: 0.0189(459, 160, 320, 3)\n",
      "11963/20000 [================>.............] - ETA: 35s - loss: 0.0187(453, 160, 320, 3)\n",
      "12419/20000 [=================>............] - ETA: 33s - loss: 0.0188(459, 160, 320, 3)\n",
      "12884/20000 [==================>...........] - ETA: 31s - loss: 0.0187(457, 160, 320, 3)\n",
      "13339/20000 [===================>..........] - ETA: 29s - loss: 0.0185(468, 160, 320, 3)\n",
      "13793/20000 [===================>..........] - ETA: 27s - loss: 0.0184(468, 160, 320, 3)\n",
      "14259/20000 [====================>.........] - ETA: 25s - loss: 0.0183(457, 160, 320, 3)\n",
      "14708/20000 [=====================>........] - ETA: 23s - loss: 0.0184(101, 160, 320, 3)\n",
      "15159/20000 [=====================>........] - ETA: 21s - loss: 0.0184(470, 160, 320, 3)\n",
      "15639/20000 [======================>.......] - ETA: 19s - loss: 0.0184(460, 160, 320, 3)\n",
      "16113/20000 [=======================>......] - ETA: 17s - loss: 0.0183(457, 160, 320, 3)\n",
      "16572/20000 [=======================>......] - ETA: 15s - loss: 0.0184(456, 160, 320, 3)\n",
      "17025/20000 [========================>.....] - ETA: 13s - loss: 0.0187(462, 160, 320, 3)\n",
      "17484/20000 [=========================>....] - ETA: 11s - loss: 0.0187(466, 160, 320, 3)\n",
      "17941/20000 [=========================>....] - ETA: 9s - loss: 0.0187 (449, 160, 320, 3)\n",
      "18409/20000 [==========================>...] - ETA: 7s - loss: 0.0189(470, 160, 320, 3)\n",
      "18877/20000 [===========================>..] - ETA: 4s - loss: 0.0188(453, 160, 320, 3)\n",
      "19435/20000 [============================>.] - ETA: 2s - loss: 0.0188(458, 160, 320, 3)\n",
      "(458, 160, 320, 3)\n",
      "19905/20000 [============================>.] - ETA: 0s - loss: 0.0189(458, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00007: saving model to Nvidias-check-07-0.0116.hdf5\n",
      "20365/20000 [==============================] - 95s - loss: 0.0188 - val_loss: 0.0116\n",
      "Epoch 9/25\n",
      "(466, 160, 320, 3)\n",
      "  457/20000 [..............................] - ETA: 92s - loss: 0.0148(465, 160, 320, 3)\n",
      "  913/20000 [>.............................] - ETA: 93s - loss: 0.0166(463, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1375/20000 [=>............................] - ETA: 89s - loss: 0.0159(462, 160, 320, 3)\n",
      " 1841/20000 [=>............................] - ETA: 85s - loss: 0.0157(467, 160, 320, 3)\n",
      " 2290/20000 [==>...........................] - ETA: 81s - loss: 0.0162(460, 160, 320, 3)\n",
      " 2760/20000 [===>..........................] - ETA: 77s - loss: 0.0172(469, 160, 320, 3)\n",
      " 3213/20000 [===>..........................] - ETA: 74s - loss: 0.0173(464, 160, 320, 3)\n",
      " 3671/20000 [====>.........................] - ETA: 72s - loss: 0.0172(458, 160, 320, 3)\n",
      " 4129/20000 [=====>........................] - ETA: 69s - loss: 0.0177(466, 160, 320, 3)\n",
      " 4587/20000 [=====>........................] - ETA: 68s - loss: 0.0178(456, 160, 320, 3)\n",
      " 5053/20000 [======>.......................] - ETA: 66s - loss: 0.0178(467, 160, 320, 3)\n",
      " 5518/20000 [=======>......................] - ETA: 64s - loss: 0.0181(456, 160, 320, 3)\n",
      " 5981/20000 [=======>......................] - ETA: 62s - loss: 0.0182(456, 160, 320, 3)\n",
      " 6443/20000 [========>.....................] - ETA: 60s - loss: 0.0181(462, 160, 320, 3)\n",
      " 6910/20000 [=========>....................] - ETA: 58s - loss: 0.0182(453, 160, 320, 3)\n",
      " 7370/20000 [==========>...................] - ETA: 55s - loss: 0.0181(467, 160, 320, 3)\n",
      " 7839/20000 [==========>...................] - ETA: 53s - loss: 0.0179(459, 160, 320, 3)\n",
      " 8303/20000 [===========>..................] - ETA: 51s - loss: 0.0177(453, 160, 320, 3)\n",
      " 8761/20000 [============>.................] - ETA: 49s - loss: 0.0178(457, 160, 320, 3)\n",
      " 9227/20000 [============>.................] - ETA: 47s - loss: 0.0178(455, 160, 320, 3)\n",
      " 9683/20000 [=============>................] - ETA: 45s - loss: 0.0180(465, 160, 320, 3)\n",
      "10150/20000 [==============>...............] - ETA: 43s - loss: 0.0181(463, 160, 320, 3)\n",
      "10606/20000 [==============>...............] - ETA: 41s - loss: 0.0183(465, 160, 320, 3)\n",
      "11062/20000 [===============>..............] - ETA: 38s - loss: 0.0182(469, 160, 320, 3)\n",
      "11524/20000 [================>.............] - ETA: 37s - loss: 0.0183(452, 160, 320, 3)\n",
      "11977/20000 [================>.............] - ETA: 35s - loss: 0.0183(468, 160, 320, 3)\n",
      "12444/20000 [=================>............] - ETA: 33s - loss: 0.0182(472, 160, 320, 3)\n",
      "12903/20000 [==================>...........] - ETA: 31s - loss: 0.0182(458, 160, 320, 3)\n",
      "13356/20000 [===================>..........] - ETA: 29s - loss: 0.0183(465, 160, 320, 3)\n",
      "13813/20000 [===================>..........] - ETA: 27s - loss: 0.0183(452, 160, 320, 3)\n",
      "14268/20000 [====================>.........] - ETA: 25s - loss: 0.0183(461, 160, 320, 3)\n",
      "14733/20000 [=====================>........] - ETA: 23s - loss: 0.0182(456, 160, 320, 3)\n",
      "15196/20000 [=====================>........] - ETA: 21s - loss: 0.0183(472, 160, 320, 3)\n",
      "15661/20000 [======================>.......] - ETA: 19s - loss: 0.0183(463, 160, 320, 3)\n",
      "16130/20000 [=======================>......] - ETA: 17s - loss: 0.0182(464, 160, 320, 3)\n",
      "16582/20000 [=======================>......] - ETA: 15s - loss: 0.0180(453, 160, 320, 3)\n",
      "17050/20000 [========================>.....] - ETA: 13s - loss: 0.0181(463, 160, 320, 3)\n",
      "17522/20000 [=========================>....] - ETA: 11s - loss: 0.0182(103, 160, 320, 3)\n",
      "17980/20000 [=========================>....] - ETA: 9s - loss: 0.0182 (454, 160, 320, 3)\n",
      "18445/20000 [==========================>...] - ETA: 6s - loss: 0.0182(467, 160, 320, 3)\n",
      "18897/20000 [===========================>..] - ETA: 4s - loss: 0.0181(460, 160, 320, 3)\n",
      "19358/20000 [============================>.] - ETA: 2s - loss: 0.0183(455, 160, 320, 3)\n",
      "19814/20000 [============================>.] - ETA: 0s - loss: 0.0185(473, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00008: saving model to Nvidias-check-08-0.0113.hdf5\n",
      "20286/20000 [==============================] - 97s - loss: 0.0186 - val_loss: 0.0113\n",
      "Epoch 10/25\n",
      "(468, 160, 320, 3)\n",
      "  463/20000 [..............................] - ETA: 107s - loss: 0.0168(463, 160, 320, 3)\n",
      "  927/20000 [>.............................] - ETA: 89s - loss: 0.0203 (461, 160, 320, 3)\n",
      " 1380/20000 [=>............................] - ETA: 91s - loss: 0.0191(456, 160, 320, 3)\n",
      " 1946/20000 [=>............................] - ETA: 83s - loss: 0.0184(453, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 2400/20000 [==>...........................] - ETA: 85s - loss: 0.0193(463, 160, 320, 3)\n",
      " 2867/20000 [===>..........................] - ETA: 80s - loss: 0.0190(456, 160, 320, 3)\n",
      " 3327/20000 [===>..........................] - ETA: 78s - loss: 0.0182(459, 160, 320, 3)\n",
      " 3782/20000 [====>.........................] - ETA: 75s - loss: 0.0183(454, 160, 320, 3)\n",
      " 4255/20000 [=====>........................] - ETA: 72s - loss: 0.0181(454, 160, 320, 3)\n",
      " 4723/20000 [======>.......................] - ETA: 69s - loss: 0.0179(473, 160, 320, 3)\n",
      " 5186/20000 [======>.......................] - ETA: 67s - loss: 0.0181(453, 160, 320, 3)\n",
      " 5647/20000 [=======>......................] - ETA: 64s - loss: 0.0183(472, 160, 320, 3)\n",
      " 6103/20000 [========>.....................] - ETA: 61s - loss: 0.0183(452, 160, 320, 3)\n",
      " 6556/20000 [========>.....................] - ETA: 60s - loss: 0.0180(460, 160, 320, 3)\n",
      " 7012/20000 [=========>....................] - ETA: 58s - loss: 0.0179(465, 160, 320, 3)\n",
      " 7475/20000 [==========>...................] - ETA: 56s - loss: 0.0179(463, 160, 320, 3)\n",
      " 7931/20000 [==========>...................] - ETA: 54s - loss: 0.0179(459, 160, 320, 3)\n",
      " 8390/20000 [===========>..................] - ETA: 52s - loss: 0.0179(455, 160, 320, 3)\n",
      " 8844/20000 [============>.................] - ETA: 50s - loss: 0.0179(467, 160, 320, 3)\n",
      " 9298/20000 [============>.................] - ETA: 48s - loss: 0.0179(457, 160, 320, 3)\n",
      " 9771/20000 [=============>................] - ETA: 46s - loss: 0.0179(479, 160, 320, 3)\n",
      "10224/20000 [==============>...............] - ETA: 44s - loss: 0.0177(459, 160, 320, 3)\n",
      "10696/20000 [===============>..............] - ETA: 42s - loss: 0.0176(459, 160, 320, 3)\n",
      "11148/20000 [===============>..............] - ETA: 40s - loss: 0.0175(452, 160, 320, 3)\n",
      "11608/20000 [================>.............] - ETA: 37s - loss: 0.0176(462, 160, 320, 3)\n",
      "12073/20000 [=================>............] - ETA: 35s - loss: 0.0176(467, 160, 320, 3)\n",
      "12536/20000 [=================>............] - ETA: 33s - loss: 0.0178(472, 160, 320, 3)\n",
      "12995/20000 [==================>...........] - ETA: 31s - loss: 0.0178(464, 160, 320, 3)\n",
      "13450/20000 [===================>..........] - ETA: 29s - loss: 0.0178(452, 160, 320, 3)\n",
      "13917/20000 [===================>..........] - ETA: 27s - loss: 0.0179(447, 160, 320, 3)\n",
      "14374/20000 [====================>.........] - ETA: 25s - loss: 0.0179(462, 160, 320, 3)\n",
      "14853/20000 [=====================>........] - ETA: 23s - loss: 0.0180(461, 160, 320, 3)\n",
      "15312/20000 [=====================>........] - ETA: 21s - loss: 0.0179(456, 160, 320, 3)\n",
      "15771/20000 [======================>.......] - ETA: 19s - loss: 0.0180(456, 160, 320, 3)\n",
      "16223/20000 [=======================>......] - ETA: 17s - loss: 0.0181(466, 160, 320, 3)\n",
      "16685/20000 [========================>.....] - ETA: 14s - loss: 0.0181(470, 160, 320, 3)\n",
      "17152/20000 [========================>.....] - ETA: 12s - loss: 0.0180(453, 160, 320, 3)\n",
      "17624/20000 [=========================>....] - ETA: 10s - loss: 0.0180(457, 160, 320, 3)\n",
      "18088/20000 [==========================>...] - ETA: 8s - loss: 0.0181 (465, 160, 320, 3)\n",
      "18540/20000 [==========================>...] - ETA: 6s - loss: 0.0180(461, 160, 320, 3)\n",
      "18987/20000 [===========================>..] - ETA: 4s - loss: 0.0180(455, 160, 320, 3)\n",
      "19449/20000 [============================>.] - ETA: 2s - loss: 0.0180(463, 160, 320, 3)\n",
      "19910/20000 [============================>.] - ETA: 0s - loss: 0.0180(463, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00009: saving model to Nvidias-check-09-0.0104.hdf5\n",
      "20366/20000 [==============================] - 96s - loss: 0.0181 - val_loss: 0.0104\n",
      "Epoch 11/25\n",
      "(100, 160, 320, 3)\n",
      "  456/20000 [..............................] - ETA: 75s - loss: 0.0181(462, 160, 320, 3)\n",
      "  922/20000 [>.............................] - ETA: 74s - loss: 0.0194(461, 160, 320, 3)\n",
      " 1392/20000 [=>............................] - ETA: 74s - loss: 0.0183(465, 160, 320, 3)\n",
      " 1845/20000 [=>............................] - ETA: 73s - loss: 0.0187(465, 160, 320, 3)\n",
      " 2302/20000 [==>...........................] - ETA: 72s - loss: 0.0208(468, 160, 320, 3)\n",
      " 2767/20000 [===>..........................] - ETA: 71s - loss: 0.0206(457, 160, 320, 3)\n",
      " 3228/20000 [===>..........................] - ETA: 70s - loss: 0.0201(466, 160, 320, 3)\n",
      " 3683/20000 [====>.........................] - ETA: 68s - loss: 0.0205(462, 160, 320, 3)\n",
      " 4146/20000 [=====>........................] - ETA: 67s - loss: 0.0200(459, 160, 320, 3)\n",
      " 4709/20000 [======>.......................] - ETA: 64s - loss: 0.0196(453, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 5171/20000 [======>.......................] - ETA: 62s - loss: 0.0198(453, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 60s - loss: 0.0198(464, 160, 320, 3)\n",
      " 6097/20000 [========>.....................] - ETA: 58s - loss: 0.0194(452, 160, 320, 3)\n",
      " 6562/20000 [========>.....................] - ETA: 56s - loss: 0.0196(445, 160, 320, 3)\n",
      " 7030/20000 [=========>....................] - ETA: 54s - loss: 0.0193(460, 160, 320, 3)\n",
      " 7487/20000 [==========>...................] - ETA: 52s - loss: 0.0190(466, 160, 320, 3)\n",
      " 7953/20000 [==========>...................] - ETA: 50s - loss: 0.0190(478, 160, 320, 3)\n",
      " 8415/20000 [===========>..................] - ETA: 48s - loss: 0.0190(456, 160, 320, 3)\n",
      " 8874/20000 [============>.................] - ETA: 46s - loss: 0.0189(463, 160, 320, 3)\n",
      " 9327/20000 [============>.................] - ETA: 44s - loss: 0.0188(462, 160, 320, 3)\n",
      " 9783/20000 [=============>................] - ETA: 42s - loss: 0.0189(466, 160, 320, 3)\n",
      "10236/20000 [==============>...............] - ETA: 40s - loss: 0.0188(457, 160, 320, 3)\n",
      "10700/20000 [===============>..............] - ETA: 39s - loss: 0.0188(451, 160, 320, 3)\n",
      "11152/20000 [===============>..............] - ETA: 37s - loss: 0.0187(460, 160, 320, 3)\n",
      "11597/20000 [================>.............] - ETA: 35s - loss: 0.0186(453, 160, 320, 3)\n",
      "12057/20000 [=================>............] - ETA: 33s - loss: 0.0185(459, 160, 320, 3)\n",
      "12523/20000 [=================>............] - ETA: 31s - loss: 0.0185(466, 160, 320, 3)\n",
      "13001/20000 [==================>...........] - ETA: 29s - loss: 0.0184(470, 160, 320, 3)\n",
      "13457/20000 [===================>..........] - ETA: 28s - loss: 0.0183(468, 160, 320, 3)\n",
      "13920/20000 [===================>..........] - ETA: 26s - loss: 0.0182(459, 160, 320, 3)\n",
      "14382/20000 [====================>.........] - ETA: 24s - loss: 0.0182(466, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 22s - loss: 0.0181(467, 160, 320, 3)\n",
      "15305/20000 [=====================>........] - ETA: 20s - loss: 0.0182(468, 160, 320, 3)\n",
      "15756/20000 [======================>.......] - ETA: 18s - loss: 0.0182(458, 160, 320, 3)\n",
      "16216/20000 [=======================>......] - ETA: 16s - loss: 0.0182(464, 160, 320, 3)\n",
      "16669/20000 [========================>.....] - ETA: 14s - loss: 0.0182(453, 160, 320, 3)\n",
      "17128/20000 [========================>.....] - ETA: 12s - loss: 0.0182(459, 160, 320, 3)\n",
      "17594/20000 [=========================>....] - ETA: 10s - loss: 0.0182(456, 160, 320, 3)\n",
      "18064/20000 [==========================>...] - ETA: 8s - loss: 0.0181 (469, 160, 320, 3)\n",
      "18532/20000 [==========================>...] - ETA: 6s - loss: 0.0181(448, 160, 320, 3)\n",
      "18991/20000 [===========================>..] - ETA: 4s - loss: 0.0182(448, 160, 320, 3)\n",
      "19457/20000 [============================>.] - ETA: 2s - loss: 0.0181(462, 160, 320, 3)\n",
      "19924/20000 [============================>.] - ETA: 0s - loss: 0.0181(464, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00010: saving model to Nvidias-check-10-0.0099.hdf5\n",
      "20392/20000 [==============================] - 95s - loss: 0.0180 - val_loss: 0.0099\n",
      "Epoch 12/25\n",
      "(460, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 106s - loss: 0.0236(461, 160, 320, 3)\n",
      "  922/20000 [>.............................] - ETA: 88s - loss: 0.0202 (461, 160, 320, 3)\n",
      " 1375/20000 [=>............................] - ETA: 83s - loss: 0.0191(453, 160, 320, 3)\n",
      " 1834/20000 [=>............................] - ETA: 84s - loss: 0.0182(471, 160, 320, 3)\n",
      " 2290/20000 [==>...........................] - ETA: 79s - loss: 0.0182(459, 160, 320, 3)\n",
      " 2759/20000 [===>..........................] - ETA: 78s - loss: 0.0191(101, 160, 320, 3)\n",
      " 3207/20000 [===>..........................] - ETA: 75s - loss: 0.0193(448, 160, 320, 3)\n",
      " 3655/20000 [====>.........................] - ETA: 72s - loss: 0.0189(453, 160, 320, 3)\n",
      " 4117/20000 [=====>........................] - ETA: 72s - loss: 0.0185(450, 160, 320, 3)\n",
      " 4581/20000 [=====>........................] - ETA: 69s - loss: 0.0188(465, 160, 320, 3)\n",
      " 5041/20000 [======>.......................] - ETA: 66s - loss: 0.0196(464, 160, 320, 3)\n",
      " 5502/20000 [=======>......................] - ETA: 64s - loss: 0.0195(458, 160, 320, 3)\n",
      " 5963/20000 [=======>......................] - ETA: 62s - loss: 0.0193(461, 160, 320, 3)\n",
      " 6416/20000 [========>.....................] - ETA: 61s - loss: 0.0197(467, 160, 320, 3)\n",
      " 6887/20000 [=========>....................] - ETA: 58s - loss: 0.0193(450, 160, 320, 3)\n",
      " 7447/20000 [==========>...................] - ETA: 56s - loss: 0.0195(465, 160, 320, 3)\n",
      "(468, 160, 320, 3)\n",
      " 7895/20000 [==========>...................] - ETA: 54s - loss: 0.0197(461, 160, 320, 3)\n",
      " 8348/20000 [===========>..................] - ETA: 53s - loss: 0.0195(457, 160, 320, 3)\n",
      " 8798/20000 [============>.................] - ETA: 50s - loss: 0.0193(467, 160, 320, 3)\n",
      " 9263/20000 [============>.................] - ETA: 49s - loss: 0.0193(465, 160, 320, 3)\n",
      " 9727/20000 [=============>................] - ETA: 46s - loss: 0.0191(460, 160, 320, 3)\n",
      "10185/20000 [==============>...............] - ETA: 44s - loss: 0.0190(464, 160, 320, 3)\n",
      "10646/20000 [==============>...............] - ETA: 42s - loss: 0.0190(465, 160, 320, 3)\n",
      "11113/20000 [===============>..............] - ETA: 40s - loss: 0.0190(459, 160, 320, 3)\n",
      "11563/20000 [================>.............] - ETA: 38s - loss: 0.0190(459, 160, 320, 3)\n",
      "12028/20000 [=================>............] - ETA: 36s - loss: 0.0190(460, 160, 320, 3)\n",
      "12496/20000 [=================>............] - ETA: 34s - loss: 0.0190(461, 160, 320, 3)\n",
      "12957/20000 [==================>...........] - ETA: 31s - loss: 0.0190(458, 160, 320, 3)\n",
      "13414/20000 [===================>..........] - ETA: 30s - loss: 0.0189(449, 160, 320, 3)\n",
      "13881/20000 [===================>..........] - ETA: 27s - loss: 0.0189(453, 160, 320, 3)\n",
      "14346/20000 [====================>.........] - ETA: 25s - loss: 0.0189(452, 160, 320, 3)\n",
      "14806/20000 [=====================>........] - ETA: 23s - loss: 0.0188(459, 160, 320, 3)\n",
      "15270/20000 [=====================>........] - ETA: 21s - loss: 0.0189(459, 160, 320, 3)\n",
      "15735/20000 [======================>.......] - ETA: 19s - loss: 0.0187(456, 160, 320, 3)\n",
      "16194/20000 [=======================>......] - ETA: 17s - loss: 0.0186(459, 160, 320, 3)\n",
      "16653/20000 [=======================>......] - ETA: 15s - loss: 0.0185(465, 160, 320, 3)\n",
      "17113/20000 [========================>.....] - ETA: 13s - loss: 0.0185(459, 160, 320, 3)\n",
      "17574/20000 [=========================>....] - ETA: 11s - loss: 0.0184(464, 160, 320, 3)\n",
      "18032/20000 [==========================>...] - ETA: 8s - loss: 0.0185 (460, 160, 320, 3)\n",
      "18481/20000 [==========================>...] - ETA: 6s - loss: 0.0185(461, 160, 320, 3)\n",
      "18934/20000 [===========================>..] - ETA: 4s - loss: 0.0185(456, 160, 320, 3)\n",
      "19386/20000 [============================>.] - ETA: 2s - loss: 0.0185(448, 160, 320, 3)\n",
      "19845/20000 [============================>.] - ETA: 0s - loss: 0.0186(452, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00011: saving model to Nvidias-check-11-0.0109.hdf5\n",
      "20304/20000 [==============================] - 97s - loss: 0.0186 - val_loss: 0.0109\n",
      "Epoch 13/25\n",
      "(446, 160, 320, 3)\n",
      "  456/20000 [..............................] - ETA: 80s - loss: 0.0135(462, 160, 320, 3)\n",
      "  915/20000 [>.............................] - ETA: 76s - loss: 0.0160(454, 160, 320, 3)\n",
      " 1380/20000 [=>............................] - ETA: 75s - loss: 0.0183(474, 160, 320, 3)\n",
      " 1839/20000 [=>............................] - ETA: 73s - loss: 0.0180(468, 160, 320, 3)\n",
      " 2303/20000 [==>...........................] - ETA: 72s - loss: 0.0177(450, 160, 320, 3)\n",
      " 2763/20000 [===>..........................] - ETA: 71s - loss: 0.0170(458, 160, 320, 3)\n",
      " 3224/20000 [===>..........................] - ETA: 69s - loss: 0.0179(459, 160, 320, 3)\n",
      " 3680/20000 [====>.........................] - ETA: 67s - loss: 0.0179(466, 160, 320, 3)\n",
      " 4128/20000 [=====>........................] - ETA: 65s - loss: 0.0177(462, 160, 320, 3)\n",
      " 4580/20000 [=====>........................] - ETA: 63s - loss: 0.0175(465, 160, 320, 3)\n",
      " 5026/20000 [======>.......................] - ETA: 61s - loss: 0.0178(456, 160, 320, 3)\n",
      " 5488/20000 [=======>......................] - ETA: 59s - loss: 0.0182(98, 160, 320, 3)\n",
      " 5942/20000 [=======>......................] - ETA: 57s - loss: 0.0183(478, 160, 320, 3)\n",
      " 6416/20000 [========>.....................] - ETA: 55s - loss: 0.0183(442, 160, 320, 3)\n",
      " 6884/20000 [=========>....................] - ETA: 53s - loss: 0.0181(467, 160, 320, 3)\n",
      " 7334/20000 [==========>...................] - ETA: 51s - loss: 0.0183(462, 160, 320, 3)\n",
      " 7792/20000 [==========>...................] - ETA: 49s - loss: 0.0188(473, 160, 320, 3)\n",
      " 8251/20000 [===========>..................] - ETA: 47s - loss: 0.0188(461, 160, 320, 3)\n",
      " 8717/20000 [============>.................] - ETA: 45s - loss: 0.0187(465, 160, 320, 3)\n",
      " 9179/20000 [============>.................] - ETA: 43s - loss: 0.0189(466, 160, 320, 3)\n",
      " 9644/20000 [=============>................] - ETA: 42s - loss: 0.0187(459, 160, 320, 3)\n",
      "10198/20000 [==============>...............] - ETA: 40s - loss: 0.0187(476, 160, 320, 3)\n",
      "(446, 160, 320, 3)\n",
      "10676/20000 [===============>..............] - ETA: 38s - loss: 0.0188(459, 160, 320, 3)\n",
      "11118/20000 [===============>..............] - ETA: 36s - loss: 0.0188(477, 160, 320, 3)\n",
      "11585/20000 [================>.............] - ETA: 34s - loss: 0.0187(456, 160, 320, 3)\n",
      "12047/20000 [=================>............] - ETA: 32s - loss: 0.0186(465, 160, 320, 3)\n",
      "12520/20000 [=================>............] - ETA: 30s - loss: 0.0185(456, 160, 320, 3)\n",
      "12981/20000 [==================>...........] - ETA: 29s - loss: 0.0184(464, 160, 320, 3)\n",
      "13446/20000 [===================>..........] - ETA: 27s - loss: 0.0183(468, 160, 320, 3)\n",
      "13912/20000 [===================>..........] - ETA: 25s - loss: 0.0184(460, 160, 320, 3)\n",
      "14371/20000 [====================>.........] - ETA: 23s - loss: 0.0185(452, 160, 320, 3)\n",
      "14847/20000 [=====================>........] - ETA: 21s - loss: 0.0184(458, 160, 320, 3)\n",
      "15293/20000 [=====================>........] - ETA: 19s - loss: 0.0184(455, 160, 320, 3)\n",
      "15752/20000 [======================>.......] - ETA: 17s - loss: 0.0185(462, 160, 320, 3)\n",
      "16229/20000 [=======================>......] - ETA: 15s - loss: 0.0184(466, 160, 320, 3)\n",
      "16685/20000 [========================>.....] - ETA: 14s - loss: 0.0184(451, 160, 320, 3)\n",
      "17150/20000 [========================>.....] - ETA: 12s - loss: 0.0183(460, 160, 320, 3)\n",
      "17606/20000 [=========================>....] - ETA: 10s - loss: 0.0182(456, 160, 320, 3)\n",
      "18070/20000 [==========================>...] - ETA: 8s - loss: 0.0182 (462, 160, 320, 3)\n",
      "18538/20000 [==========================>...] - ETA: 6s - loss: 0.0181(456, 160, 320, 3)\n",
      "18998/20000 [===========================>..] - ETA: 4s - loss: 0.0180(463, 160, 320, 3)\n",
      "19450/20000 [============================>.] - ETA: 2s - loss: 0.0179(462, 160, 320, 3)\n",
      "19908/20000 [============================>.] - ETA: 0s - loss: 0.0180(465, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "Epoch 00012: saving model to Nvidias-check-12-0.0107.hdf5\n",
      "20363/20000 [==============================] - 93s - loss: 0.0180 - val_loss: 0.0107\n",
      "Epoch 14/25\n",
      "(461, 160, 320, 3)\n",
      "  462/20000 [..............................] - ETA: 80s - loss: 0.0213(469, 160, 320, 3)\n",
      "  928/20000 [>.............................] - ETA: 84s - loss: 0.0208(465, 160, 320, 3)\n",
      " 1379/20000 [=>............................] - ETA: 86s - loss: 0.0197(471, 160, 320, 3)\n",
      " 1839/20000 [=>............................] - ETA: 81s - loss: 0.0190(461, 160, 320, 3)\n",
      " 2295/20000 [==>...........................] - ETA: 76s - loss: 0.0193(468, 160, 320, 3)\n",
      " 2757/20000 [===>..........................] - ETA: 78s - loss: 0.0196(459, 160, 320, 3)\n",
      " 3213/20000 [===>..........................] - ETA: 74s - loss: 0.0188(465, 160, 320, 3)\n",
      " 3676/20000 [====>.........................] - ETA: 74s - loss: 0.0188(463, 160, 320, 3)\n",
      " 4138/20000 [=====>........................] - ETA: 70s - loss: 0.0191(464, 160, 320, 3)\n",
      " 4603/20000 [=====>........................] - ETA: 70s - loss: 0.0185(455, 160, 320, 3)\n",
      " 5064/20000 [======>.......................] - ETA: 66s - loss: 0.0183(464, 160, 320, 3)\n",
      " 5533/20000 [=======>......................] - ETA: 64s - loss: 0.0183(459, 160, 320, 3)\n",
      " 5998/20000 [=======>......................] - ETA: 63s - loss: 0.0184(460, 160, 320, 3)\n",
      " 6469/20000 [========>.....................] - ETA: 60s - loss: 0.0185(453, 160, 320, 3)\n",
      " 6930/20000 [=========>....................] - ETA: 58s - loss: 0.0184(452, 160, 320, 3)\n",
      " 7398/20000 [==========>...................] - ETA: 55s - loss: 0.0182(462, 160, 320, 3)\n",
      " 7857/20000 [==========>...................] - ETA: 53s - loss: 0.0183(460, 160, 320, 3)\n",
      " 8322/20000 [===========>..................] - ETA: 51s - loss: 0.0185(101, 160, 320, 3)\n",
      " 8785/20000 [============>.................] - ETA: 48s - loss: 0.0185(457, 160, 320, 3)\n",
      " 9249/20000 [============>.................] - ETA: 46s - loss: 0.0186(459, 160, 320, 3)\n",
      " 9704/20000 [=============>................] - ETA: 44s - loss: 0.0184(458, 160, 320, 3)\n",
      "10168/20000 [==============>...............] - ETA: 42s - loss: 0.0186(451, 160, 320, 3)\n",
      "10627/20000 [==============>...............] - ETA: 40s - loss: 0.0190(461, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11087/20000 [===============>..............] - ETA: 38s - loss: 0.0190(467, 160, 320, 3)\n",
      "11540/20000 [================>.............] - ETA: 36s - loss: 0.0188(457, 160, 320, 3)\n",
      "11992/20000 [================>.............] - ETA: 34s - loss: 0.0189(461, 160, 320, 3)\n",
      "12454/20000 [=================>............] - ETA: 32s - loss: 0.0188(459, 160, 320, 3)\n",
      "13015/20000 [==================>...........] - ETA: 30s - loss: 0.0188(455, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      "13472/20000 [===================>..........] - ETA: 28s - loss: 0.0189(463, 160, 320, 3)\n",
      "13931/20000 [===================>..........] - ETA: 26s - loss: 0.0188(461, 160, 320, 3)\n",
      "14389/20000 [====================>.........] - ETA: 24s - loss: 0.0186(459, 160, 320, 3)\n",
      "14840/20000 [=====================>........] - ETA: 22s - loss: 0.0186(465, 160, 320, 3)\n",
      "15301/20000 [=====================>........] - ETA: 20s - loss: 0.0185(461, 160, 320, 3)\n",
      "15768/20000 [======================>.......] - ETA: 18s - loss: 0.0184(451, 160, 320, 3)\n",
      "16225/20000 [=======================>......] - ETA: 16s - loss: 0.0184(455, 160, 320, 3)\n",
      "16686/20000 [========================>.....] - ETA: 14s - loss: 0.0184(468, 160, 320, 3)\n",
      "17145/20000 [========================>.....] - ETA: 12s - loss: 0.0184(460, 160, 320, 3)\n",
      "17600/20000 [=========================>....] - ETA: 10s - loss: 0.0183(460, 160, 320, 3)\n",
      "18057/20000 [==========================>...] - ETA: 8s - loss: 0.0183 (467, 160, 320, 3)\n",
      "18520/20000 [==========================>...] - ETA: 6s - loss: 0.0184(458, 160, 320, 3)\n",
      "18981/20000 [===========================>..] - ETA: 4s - loss: 0.0183(462, 160, 320, 3)\n",
      "19440/20000 [============================>.] - ETA: 2s - loss: 0.0183(467, 160, 320, 3)\n",
      "19905/20000 [============================>.] - ETA: 0s - loss: 0.0183(467, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00013: saving model to Nvidias-check-13-0.0110.hdf5\n",
      "20366/20000 [==============================] - 92s - loss: 0.0182 - val_loss: 0.0110\n",
      "Epoch 15/25\n",
      "(454, 160, 320, 3)\n",
      "  451/20000 [..............................] - ETA: 80s - loss: 0.0184(465, 160, 320, 3)\n",
      "  906/20000 [>.............................] - ETA: 78s - loss: 0.0165(465, 160, 320, 3)\n",
      " 1374/20000 [=>............................] - ETA: 75s - loss: 0.0162(460, 160, 320, 3)\n",
      " 1834/20000 [=>............................] - ETA: 73s - loss: 0.0162(463, 160, 320, 3)\n",
      " 2294/20000 [==>...........................] - ETA: 70s - loss: 0.0166(441, 160, 320, 3)\n",
      " 2761/20000 [===>..........................] - ETA: 72s - loss: 0.0166(459, 160, 320, 3)\n",
      " 3219/20000 [===>..........................] - ETA: 69s - loss: 0.0172(467, 160, 320, 3)\n",
      " 3681/20000 [====>.........................] - ETA: 69s - loss: 0.0175(466, 160, 320, 3)\n",
      " 4148/20000 [=====>........................] - ETA: 67s - loss: 0.0179(469, 160, 320, 3)\n",
      " 4615/20000 [=====>........................] - ETA: 65s - loss: 0.0179(457, 160, 320, 3)\n",
      " 5069/20000 [======>.......................] - ETA: 64s - loss: 0.0180(460, 160, 320, 3)\n",
      " 5534/20000 [=======>......................] - ETA: 62s - loss: 0.0184(460, 160, 320, 3)\n",
      " 5999/20000 [=======>......................] - ETA: 61s - loss: 0.0180(446, 160, 320, 3)\n",
      " 6459/20000 [========>.....................] - ETA: 58s - loss: 0.0180(465, 160, 320, 3)\n",
      " 6922/20000 [=========>....................] - ETA: 56s - loss: 0.0183(462, 160, 320, 3)\n",
      " 7363/20000 [==========>...................] - ETA: 54s - loss: 0.0182(451, 160, 320, 3)\n",
      " 7822/20000 [==========>...................] - ETA: 52s - loss: 0.0181(466, 160, 320, 3)\n",
      " 8289/20000 [===========>..................] - ETA: 50s - loss: 0.0180(462, 160, 320, 3)\n",
      " 8755/20000 [============>.................] - ETA: 47s - loss: 0.0182(475, 160, 320, 3)\n",
      " 9224/20000 [============>.................] - ETA: 45s - loss: 0.0182(454, 160, 320, 3)\n",
      " 9681/20000 [=============>................] - ETA: 43s - loss: 0.0180(461, 160, 320, 3)\n",
      "10141/20000 [==============>...............] - ETA: 41s - loss: 0.0179(455, 160, 320, 3)\n",
      "10601/20000 [==============>...............] - ETA: 39s - loss: 0.0178(460, 160, 320, 3)\n",
      "11047/20000 [===============>..............] - ETA: 37s - loss: 0.0179(100, 160, 320, 3)\n",
      "11512/20000 [================>.............] - ETA: 35s - loss: 0.0179(467, 160, 320, 3)\n",
      "11974/20000 [================>.............] - ETA: 33s - loss: 0.0178(458, 160, 320, 3)\n",
      "12425/20000 [=================>............] - ETA: 31s - loss: 0.0177(462, 160, 320, 3)\n",
      "12891/20000 [==================>...........] - ETA: 29s - loss: 0.0179(462, 160, 320, 3)\n",
      "13353/20000 [===================>..........] - ETA: 27s - loss: 0.0182(455, 160, 320, 3)\n",
      "13828/20000 [===================>..........] - ETA: 25s - loss: 0.0183(467, 160, 320, 3)\n",
      "14282/20000 [====================>.........] - ETA: 23s - loss: 0.0182(457, 160, 320, 3)\n",
      "14743/20000 [=====================>........] - ETA: 22s - loss: 0.0184(467, 160, 320, 3)\n",
      "15198/20000 [=====================>........] - ETA: 20s - loss: 0.0183(453, 160, 320, 3)\n",
      "15758/20000 [======================>.......] - ETA: 17s - loss: 0.0183(469, 160, 320, 3)\n",
      "(466, 160, 320, 3)\n",
      "16225/20000 [=======================>......] - ETA: 15s - loss: 0.0183(455, 160, 320, 3)\n",
      "16683/20000 [========================>.....] - ETA: 13s - loss: 0.0183(460, 160, 320, 3)\n",
      "17145/20000 [========================>.....] - ETA: 12s - loss: 0.0182(454, 160, 320, 3)\n",
      "17607/20000 [=========================>....] - ETA: 10s - loss: 0.0182(460, 160, 320, 3)\n",
      "18062/20000 [==========================>...] - ETA: 8s - loss: 0.0182 (472, 160, 320, 3)\n",
      "18529/20000 [==========================>...] - ETA: 6s - loss: 0.0181(461, 160, 320, 3)\n",
      "18986/20000 [===========================>..] - ETA: 4s - loss: 0.0181(460, 160, 320, 3)\n",
      "19453/20000 [============================>.] - ETA: 2s - loss: 0.0182(462, 160, 320, 3)\n",
      "19906/20000 [============================>.] - ETA: 0s - loss: 0.0182(465, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00014: saving model to Nvidias-check-14-0.0103.hdf5\n",
      "20375/20000 [==============================] - 92s - loss: 0.0181 - val_loss: 0.0103\n",
      "Epoch 16/25\n",
      "(451, 160, 320, 3)\n",
      "  466/20000 [..............................] - ETA: 80s - loss: 0.0165(468, 160, 320, 3)\n",
      "  921/20000 [>.............................] - ETA: 77s - loss: 0.0174(456, 160, 320, 3)\n",
      " 1381/20000 [=>............................] - ETA: 75s - loss: 0.0179(452, 160, 320, 3)\n",
      " 1835/20000 [=>............................] - ETA: 73s - loss: 0.0178(468, 160, 320, 3)\n",
      " 2295/20000 [==>...........................] - ETA: 72s - loss: 0.0183(462, 160, 320, 3)\n",
      " 2767/20000 [===>..........................] - ETA: 70s - loss: 0.0175(462, 160, 320, 3)\n",
      " 3228/20000 [===>..........................] - ETA: 68s - loss: 0.0178(467, 160, 320, 3)\n",
      " 3688/20000 [====>.........................] - ETA: 66s - loss: 0.0173(460, 160, 320, 3)\n",
      " 4150/20000 [=====>........................] - ETA: 64s - loss: 0.0168(454, 160, 320, 3)\n",
      " 4615/20000 [=====>........................] - ETA: 62s - loss: 0.0168(469, 160, 320, 3)\n",
      " 5066/20000 [======>.......................] - ETA: 60s - loss: 0.0169(460, 160, 320, 3)\n",
      " 5534/20000 [=======>......................] - ETA: 58s - loss: 0.0167(470, 160, 320, 3)\n",
      " 5990/20000 [=======>......................] - ETA: 56s - loss: 0.0172(467, 160, 320, 3)\n",
      " 6442/20000 [========>.....................] - ETA: 54s - loss: 0.0174(461, 160, 320, 3)\n",
      " 6910/20000 [=========>....................] - ETA: 52s - loss: 0.0175(459, 160, 320, 3)\n",
      " 7372/20000 [==========>...................] - ETA: 50s - loss: 0.0174(458, 160, 320, 3)\n",
      " 7834/20000 [==========>...................] - ETA: 49s - loss: 0.0175(464, 160, 320, 3)\n",
      " 8301/20000 [===========>..................] - ETA: 47s - loss: 0.0177(462, 160, 320, 3)\n",
      " 8761/20000 [============>.................] - ETA: 45s - loss: 0.0174(454, 160, 320, 3)\n",
      " 9215/20000 [============>.................] - ETA: 43s - loss: 0.0174(459, 160, 320, 3)\n",
      " 9684/20000 [=============>................] - ETA: 41s - loss: 0.0177(458, 160, 320, 3)\n",
      "10144/20000 [==============>...............] - ETA: 39s - loss: 0.0176(459, 160, 320, 3)\n",
      "10614/20000 [==============>...............] - ETA: 38s - loss: 0.0175(454, 160, 320, 3)\n",
      "11081/20000 [===============>..............] - ETA: 36s - loss: 0.0174(448, 160, 320, 3)\n",
      "11542/20000 [================>.............] - ETA: 34s - loss: 0.0175(455, 160, 320, 3)\n",
      "12001/20000 [=================>............] - ETA: 32s - loss: 0.0175(466, 160, 320, 3)\n",
      "12459/20000 [=================>............] - ETA: 30s - loss: 0.0174(464, 160, 320, 3)\n",
      "12923/20000 [==================>...........] - ETA: 28s - loss: 0.0173(458, 160, 320, 3)\n",
      "13385/20000 [===================>..........] - ETA: 26s - loss: 0.0173(461, 160, 320, 3)\n",
      "13839/20000 [===================>..........] - ETA: 24s - loss: 0.0174(105, 160, 320, 3)\n",
      "14298/20000 [====================>.........] - ETA: 22s - loss: 0.0174(452, 160, 320, 3)\n",
      "14756/20000 [=====================>........] - ETA: 21s - loss: 0.0174(472, 160, 320, 3)\n",
      "15215/20000 [=====================>........] - ETA: 19s - loss: 0.0173(463, 160, 320, 3)\n",
      "15669/20000 [======================>.......] - ETA: 17s - loss: 0.0173(455, 160, 320, 3)\n",
      "16117/20000 [=======================>......] - ETA: 15s - loss: 0.0176(455, 160, 320, 3)\n",
      "16572/20000 [=======================>......] - ETA: 13s - loss: 0.0176(468, 160, 320, 3)\n",
      "17038/20000 [========================>.....] - ETA: 12s - loss: 0.0177(458, 160, 320, 3)\n",
      "17502/20000 [=========================>....] - ETA: 10s - loss: 0.0178(465, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0177 (461, 160, 320, 3)\n",
      "18526/20000 [==========================>...] - ETA: 6s - loss: 0.0177(455, 160, 320, 3)\n",
      "(460, 160, 320, 3)\n",
      "18978/20000 [===========================>..] - ETA: 4s - loss: 0.0177(470, 160, 320, 3)\n",
      "19450/20000 [============================>.] - ETA: 2s - loss: 0.0178(464, 160, 320, 3)\n",
      "19913/20000 [============================>.] - ETA: 0s - loss: 0.0177(461, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00015: saving model to Nvidias-check-15-0.0102.hdf5\n",
      "20368/20000 [==============================] - 89s - loss: 0.0177 - val_loss: 0.0102\n",
      "Epoch 17/25\n",
      "(467, 160, 320, 3)\n",
      "  455/20000 [..............................] - ETA: 80s - loss: 0.0161(462, 160, 320, 3)\n",
      "  923/20000 [>.............................] - ETA: 78s - loss: 0.0148(460, 160, 320, 3)\n",
      " 1381/20000 [=>............................] - ETA: 76s - loss: 0.0158(465, 160, 320, 3)\n",
      " 1846/20000 [=>............................] - ETA: 74s - loss: 0.0170(461, 160, 320, 3)\n",
      " 2307/20000 [==>...........................] - ETA: 71s - loss: 0.0176(452, 160, 320, 3)\n",
      " 2762/20000 [===>..........................] - ETA: 70s - loss: 0.0173(472, 160, 320, 3)\n",
      " 3222/20000 [===>..........................] - ETA: 68s - loss: 0.0178(453, 160, 320, 3)\n",
      " 3692/20000 [====>.........................] - ETA: 66s - loss: 0.0179(460, 160, 320, 3)\n",
      " 4156/20000 [=====>........................] - ETA: 64s - loss: 0.0179(458, 160, 320, 3)\n",
      " 4617/20000 [=====>........................] - ETA: 62s - loss: 0.0179(451, 160, 320, 3)\n",
      " 5084/20000 [======>.......................] - ETA: 60s - loss: 0.0180(462, 160, 320, 3)\n",
      " 5546/20000 [=======>......................] - ETA: 58s - loss: 0.0177(458, 160, 320, 3)\n",
      " 6006/20000 [========>.....................] - ETA: 56s - loss: 0.0181(467, 160, 320, 3)\n",
      " 6471/20000 [========>.....................] - ETA: 54s - loss: 0.0180(454, 160, 320, 3)\n",
      " 6932/20000 [=========>....................] - ETA: 52s - loss: 0.0177(460, 160, 320, 3)\n",
      " 7384/20000 [==========>...................] - ETA: 51s - loss: 0.0176(455, 160, 320, 3)\n",
      " 7856/20000 [==========>...................] - ETA: 49s - loss: 0.0176(471, 160, 320, 3)\n",
      " 8309/20000 [===========>..................] - ETA: 47s - loss: 0.0176(458, 160, 320, 3)\n",
      " 8769/20000 [============>.................] - ETA: 45s - loss: 0.0178(464, 160, 320, 3)\n",
      " 9227/20000 [============>.................] - ETA: 43s - loss: 0.0179(457, 160, 320, 3)\n",
      " 9678/20000 [=============>................] - ETA: 41s - loss: 0.0179(460, 160, 320, 3)\n",
      "10140/20000 [==============>...............] - ETA: 40s - loss: 0.0178(458, 160, 320, 3)\n",
      "10598/20000 [==============>...............] - ETA: 38s - loss: 0.0180(464, 160, 320, 3)\n",
      "11065/20000 [===============>..............] - ETA: 36s - loss: 0.0181(460, 160, 320, 3)\n",
      "11519/20000 [================>.............] - ETA: 34s - loss: 0.0180(464, 160, 320, 3)\n",
      "11979/20000 [================>.............] - ETA: 33s - loss: 0.0180(470, 160, 320, 3)\n",
      "12434/20000 [=================>............] - ETA: 31s - loss: 0.0182(464, 160, 320, 3)\n",
      "12905/20000 [==================>...........] - ETA: 29s - loss: 0.0181(461, 160, 320, 3)\n",
      "13363/20000 [===================>..........] - ETA: 27s - loss: 0.0181(455, 160, 320, 3)\n",
      "13827/20000 [===================>..........] - ETA: 25s - loss: 0.0180(456, 160, 320, 3)\n",
      "14284/20000 [====================>.........] - ETA: 23s - loss: 0.0182(471, 160, 320, 3)\n",
      "14744/20000 [=====================>........] - ETA: 21s - loss: 0.0181(465, 160, 320, 3)\n",
      "15202/20000 [=====================>........] - ETA: 20s - loss: 0.0180(455, 160, 320, 3)\n",
      "15666/20000 [======================>.......] - ETA: 18s - loss: 0.0179(458, 160, 320, 3)\n",
      "16126/20000 [=======================>......] - ETA: 16s - loss: 0.0180(462, 160, 320, 3)\n",
      "16590/20000 [=======================>......] - ETA: 14s - loss: 0.0181(101, 160, 320, 3)\n",
      "17060/20000 [========================>.....] - ETA: 12s - loss: 0.0181(469, 160, 320, 3)\n",
      "17524/20000 [=========================>....] - ETA: 10s - loss: 0.0181(469, 160, 320, 3)\n",
      "17985/20000 [=========================>....] - ETA: 8s - loss: 0.0180 (477, 160, 320, 3)\n",
      "18440/20000 [==========================>...] - ETA: 6s - loss: 0.0180(464, 160, 320, 3)\n",
      "18896/20000 [===========================>..] - ETA: 4s - loss: 0.0183(463, 160, 320, 3)\n",
      "19367/20000 [============================>.] - ETA: 2s - loss: 0.0183(470, 160, 320, 3)\n",
      "19832/20000 [============================>.] - ETA: 0s - loss: 0.0182(452, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00016: saving model to Nvidias-check-16-0.0108.hdf5\n",
      "20287/20000 [==============================] - 90s - loss: 0.0183 - val_loss: 0.0108\n",
      "Epoch 18/25\n",
      "(462, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 87s - loss: 0.0151(464, 160, 320, 3)\n",
      " 1021/20000 [>.............................] - ETA: 83s - loss: 0.0170(468, 160, 320, 3)\n",
      "(457, 160, 320, 3)\n",
      " 1490/20000 [=>............................] - ETA: 81s - loss: 0.0182(460, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1959/20000 [=>............................] - ETA: 78s - loss: 0.0179(468, 160, 320, 3)\n",
      " 2436/20000 [==>...........................] - ETA: 75s - loss: 0.0174(462, 160, 320, 3)\n",
      " 2900/20000 [===>..........................] - ETA: 72s - loss: 0.0175(461, 160, 320, 3)\n",
      " 3363/20000 [====>.........................] - ETA: 70s - loss: 0.0174(458, 160, 320, 3)\n",
      " 3833/20000 [====>.........................] - ETA: 67s - loss: 0.0169(464, 160, 320, 3)\n",
      " 4285/20000 [=====>........................] - ETA: 65s - loss: 0.0170(464, 160, 320, 3)\n",
      " 4747/20000 [======>.......................] - ETA: 62s - loss: 0.0176(458, 160, 320, 3)\n",
      " 5211/20000 [======>.......................] - ETA: 60s - loss: 0.0178(465, 160, 320, 3)\n",
      " 5679/20000 [=======>......................] - ETA: 59s - loss: 0.0176(467, 160, 320, 3)\n",
      " 6136/20000 [========>.....................] - ETA: 57s - loss: 0.0177(459, 160, 320, 3)\n",
      " 6596/20000 [========>.....................] - ETA: 55s - loss: 0.0178(460, 160, 320, 3)\n",
      " 7064/20000 [=========>....................] - ETA: 53s - loss: 0.0177(465, 160, 320, 3)\n",
      " 7526/20000 [==========>...................] - ETA: 51s - loss: 0.0177(455, 160, 320, 3)\n",
      " 7987/20000 [==========>...................] - ETA: 49s - loss: 0.0178(459, 160, 320, 3)\n",
      " 8445/20000 [===========>..................] - ETA: 47s - loss: 0.0178(460, 160, 320, 3)\n",
      " 8909/20000 [============>.................] - ETA: 45s - loss: 0.0179(458, 160, 320, 3)\n",
      " 9373/20000 [=============>................] - ETA: 43s - loss: 0.0179(453, 160, 320, 3)\n",
      " 9831/20000 [=============>................] - ETA: 41s - loss: 0.0177(453, 160, 320, 3)\n",
      "10296/20000 [==============>...............] - ETA: 39s - loss: 0.0176(469, 160, 320, 3)\n",
      "10763/20000 [===============>..............] - ETA: 38s - loss: 0.0176(459, 160, 320, 3)\n",
      "11222/20000 [===============>..............] - ETA: 36s - loss: 0.0175(463, 160, 320, 3)\n",
      "11682/20000 [================>.............] - ETA: 34s - loss: 0.0177(457, 160, 320, 3)\n",
      "12147/20000 [=================>............] - ETA: 32s - loss: 0.0177(452, 160, 320, 3)\n",
      "12602/20000 [=================>............] - ETA: 30s - loss: 0.0179(467, 160, 320, 3)\n",
      "13061/20000 [==================>...........] - ETA: 28s - loss: 0.0178(462, 160, 320, 3)\n",
      "13521/20000 [===================>..........] - ETA: 26s - loss: 0.0178(462, 160, 320, 3)\n",
      "13979/20000 [===================>..........] - ETA: 24s - loss: 0.0180(466, 160, 320, 3)\n",
      "14432/20000 [====================>.........] - ETA: 22s - loss: 0.0178(471, 160, 320, 3)\n",
      "14885/20000 [=====================>........] - ETA: 21s - loss: 0.0178(460, 160, 320, 3)\n",
      "15354/20000 [======================>.......] - ETA: 19s - loss: 0.0180(459, 160, 320, 3)\n",
      "15813/20000 [======================>.......] - ETA: 17s - loss: 0.0180(463, 160, 320, 3)\n",
      "16276/20000 [=======================>......] - ETA: 15s - loss: 0.0179(456, 160, 320, 3)\n",
      "16733/20000 [========================>.....] - ETA: 13s - loss: 0.0179(469, 160, 320, 3)\n",
      "17185/20000 [========================>.....] - ETA: 11s - loss: 0.0179(462, 160, 320, 3)\n",
      "17652/20000 [=========================>....] - ETA: 9s - loss: 0.0179 (467, 160, 320, 3)\n",
      "18114/20000 [==========================>...] - ETA: 7s - loss: 0.0178(465, 160, 320, 3)\n",
      "18576/20000 [==========================>...] - ETA: 5s - loss: 0.0177(454, 160, 320, 3)\n",
      "19042/20000 [===========================>..] - ETA: 3s - loss: 0.0178(459, 160, 320, 3)\n",
      "19513/20000 [============================>.] - ETA: 2s - loss: 0.0179(99, 160, 320, 3)\n",
      "19973/20000 [============================>.] - ETA: 0s - loss: 0.0178(469, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00017: saving model to Nvidias-check-17-0.0109.hdf5\n",
      "20432/20000 [==============================] - 91s - loss: 0.0178 - val_loss: 0.0109\n",
      "Epoch 19/25\n",
      "(457, 160, 320, 3)\n",
      "  463/20000 [..............................] - ETA: 106s - loss: 0.0156(466, 160, 320, 3)\n",
      "  919/20000 [>.............................] - ETA: 88s - loss: 0.0172 (460, 160, 320, 3)\n",
      " 1388/20000 [=>............................] - ETA: 81s - loss: 0.0208(469, 160, 320, 3)\n",
      " 1850/20000 [=>............................] - ETA: 84s - loss: 0.0204(462, 160, 320, 3)\n",
      " 2317/20000 [==>...........................] - ETA: 79s - loss: 0.0204(470, 160, 320, 3)\n",
      " 2782/20000 [===>..........................] - ETA: 79s - loss: 0.0210(457, 160, 320, 3)\n",
      " 3236/20000 [===>..........................] - ETA: 75s - loss: 0.0199(452, 160, 320, 3)\n",
      " 3794/20000 [====>.........................] - ETA: 74s - loss: 0.0196(463, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      " 4263/20000 [=====>........................] - ETA: 71s - loss: 0.0199(464, 160, 320, 3)\n",
      " 4720/20000 [======>.......................] - ETA: 68s - loss: 0.0195(461, 160, 320, 3)\n",
      " 5186/20000 [======>.......................] - ETA: 67s - loss: 0.0190(464, 160, 320, 3)\n",
      " 5646/20000 [=======>......................] - ETA: 64s - loss: 0.0190(465, 160, 320, 3)\n",
      " 6115/20000 [========>.....................] - ETA: 62s - loss: 0.0185(461, 160, 320, 3)\n",
      " 6577/20000 [========>.....................] - ETA: 60s - loss: 0.0181(469, 160, 320, 3)\n",
      " 7047/20000 [=========>....................] - ETA: 59s - loss: 0.0180(464, 160, 320, 3)\n",
      " 7504/20000 [==========>...................] - ETA: 56s - loss: 0.0180(461, 160, 320, 3)\n",
      " 7956/20000 [==========>...................] - ETA: 54s - loss: 0.0181(456, 160, 320, 3)\n",
      " 8419/20000 [===========>..................] - ETA: 51s - loss: 0.0180(457, 160, 320, 3)\n",
      " 8875/20000 [============>.................] - ETA: 49s - loss: 0.0180(466, 160, 320, 3)\n",
      " 9339/20000 [=============>................] - ETA: 47s - loss: 0.0180(446, 160, 320, 3)\n",
      " 9800/20000 [=============>................] - ETA: 45s - loss: 0.0181(466, 160, 320, 3)\n",
      "10264/20000 [==============>...............] - ETA: 42s - loss: 0.0180(464, 160, 320, 3)\n",
      "10729/20000 [===============>..............] - ETA: 40s - loss: 0.0179(457, 160, 320, 3)\n",
      "11190/20000 [===============>..............] - ETA: 38s - loss: 0.0178(446, 160, 320, 3)\n",
      "11659/20000 [================>.............] - ETA: 36s - loss: 0.0178(454, 160, 320, 3)\n",
      "12123/20000 [=================>............] - ETA: 34s - loss: 0.0178(468, 160, 320, 3)\n",
      "12584/20000 [=================>............] - ETA: 32s - loss: 0.0176(452, 160, 320, 3)\n",
      "13040/20000 [==================>...........] - ETA: 30s - loss: 0.0175(467, 160, 320, 3)\n",
      "13497/20000 [===================>..........] - ETA: 28s - loss: 0.0176(443, 160, 320, 3)\n",
      "13963/20000 [===================>..........] - ETA: 26s - loss: 0.0175(456, 160, 320, 3)\n",
      "14409/20000 [====================>.........] - ETA: 24s - loss: 0.0176(467, 160, 320, 3)\n",
      "14875/20000 [=====================>........] - ETA: 22s - loss: 0.0176(458, 160, 320, 3)\n",
      "15339/20000 [======================>.......] - ETA: 20s - loss: 0.0176(466, 160, 320, 3)\n",
      "15796/20000 [======================>.......] - ETA: 18s - loss: 0.0176(462, 160, 320, 3)\n",
      "16242/20000 [=======================>......] - ETA: 16s - loss: 0.0176(451, 160, 320, 3)\n",
      "16696/20000 [========================>.....] - ETA: 14s - loss: 0.0176(462, 160, 320, 3)\n",
      "17164/20000 [========================>.....] - ETA: 12s - loss: 0.0175(458, 160, 320, 3)\n",
      "17616/20000 [=========================>....] - ETA: 10s - loss: 0.0175(467, 160, 320, 3)\n",
      "18083/20000 [==========================>...] - ETA: 8s - loss: 0.0177 (463, 160, 320, 3)\n",
      "18526/20000 [==========================>...] - ETA: 6s - loss: 0.0176(454, 160, 320, 3)\n",
      "18982/20000 [===========================>..] - ETA: 4s - loss: 0.0175(455, 160, 320, 3)\n",
      "19449/20000 [============================>.] - ETA: 2s - loss: 0.0175(458, 160, 320, 3)\n",
      "19907/20000 [============================>.] - ETA: 0s - loss: 0.0177(465, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00018: saving model to Nvidias-check-18-0.0113.hdf5\n",
      "20373/20000 [==============================] - 96s - loss: 0.0177 - val_loss: 0.0113\n",
      "Epoch 20/25\n",
      "(460, 160, 320, 3)\n",
      "  462/20000 [..............................] - ETA: 105s - loss: 0.0157(465, 160, 320, 3)\n",
      "  913/20000 [>.............................] - ETA: 88s - loss: 0.0153 (462, 160, 320, 3)\n",
      " 1375/20000 [=>............................] - ETA: 82s - loss: 0.0162(455, 160, 320, 3)\n",
      " 1833/20000 [=>............................] - ETA: 83s - loss: 0.0180(101, 160, 320, 3)\n",
      " 2300/20000 [==>...........................] - ETA: 78s - loss: 0.0179(468, 160, 320, 3)\n",
      " 2763/20000 [===>..........................] - ETA: 78s - loss: 0.0182(457, 160, 320, 3)\n",
      " 3217/20000 [===>..........................] - ETA: 75s - loss: 0.0179(459, 160, 320, 3)\n",
      " 3672/20000 [====>.........................] - ETA: 74s - loss: 0.0179(464, 160, 320, 3)\n",
      " 4130/20000 [=====>........................] - ETA: 71s - loss: 0.0189(461, 160, 320, 3)\n",
      " 4595/20000 [=====>........................] - ETA: 68s - loss: 0.0189(468, 160, 320, 3)\n",
      " 5055/20000 [======>.......................] - ETA: 66s - loss: 0.0188(467, 160, 320, 3)\n",
      " 5520/20000 [=======>......................] - ETA: 63s - loss: 0.0192(459, 160, 320, 3)\n",
      " 5982/20000 [=======>......................] - ETA: 61s - loss: 0.0187(459, 160, 320, 3)\n",
      " 6538/20000 [========>.....................] - ETA: 58s - loss: 0.0186(458, 160, 320, 3)\n",
      "(454, 160, 320, 3)\n",
      " 7006/20000 [=========>....................] - ETA: 56s - loss: 0.0186(466, 160, 320, 3)\n",
      " 7463/20000 [==========>...................] - ETA: 54s - loss: 0.0185(473, 160, 320, 3)\n",
      " 7922/20000 [==========>...................] - ETA: 52s - loss: 0.0183(473, 160, 320, 3)\n",
      " 8386/20000 [===========>..................] - ETA: 50s - loss: 0.0182(460, 160, 320, 3)\n",
      " 8847/20000 [============>.................] - ETA: 48s - loss: 0.0181(463, 160, 320, 3)\n",
      " 9315/20000 [============>.................] - ETA: 46s - loss: 0.0179(461, 160, 320, 3)\n",
      " 9782/20000 [=============>................] - ETA: 44s - loss: 0.0178(467, 160, 320, 3)\n",
      "10241/20000 [==============>...............] - ETA: 42s - loss: 0.0179(458, 160, 320, 3)\n",
      "10700/20000 [===============>..............] - ETA: 39s - loss: 0.0178(465, 160, 320, 3)\n",
      "11158/20000 [===============>..............] - ETA: 37s - loss: 0.0177(463, 160, 320, 3)\n",
      "11612/20000 [================>.............] - ETA: 35s - loss: 0.0177(464, 160, 320, 3)\n",
      "12078/20000 [=================>............] - ETA: 34s - loss: 0.0178(462, 160, 320, 3)\n",
      "12551/20000 [=================>............] - ETA: 32s - loss: 0.0178(443, 160, 320, 3)\n",
      "13024/20000 [==================>...........] - ETA: 29s - loss: 0.0177(473, 160, 320, 3)\n",
      "13484/20000 [===================>..........] - ETA: 28s - loss: 0.0177(465, 160, 320, 3)\n",
      "13947/20000 [===================>..........] - ETA: 26s - loss: 0.0177(449, 160, 320, 3)\n",
      "14408/20000 [====================>.........] - ETA: 24s - loss: 0.0178(462, 160, 320, 3)\n",
      "14875/20000 [=====================>........] - ETA: 22s - loss: 0.0177(451, 160, 320, 3)\n",
      "15333/20000 [=====================>........] - ETA: 20s - loss: 0.0175(457, 160, 320, 3)\n",
      "15798/20000 [======================>.......] - ETA: 18s - loss: 0.0175(463, 160, 320, 3)\n",
      "16261/20000 [=======================>......] - ETA: 16s - loss: 0.0176(449, 160, 320, 3)\n",
      "16725/20000 [========================>.....] - ETA: 14s - loss: 0.0175(468, 160, 320, 3)\n",
      "17187/20000 [========================>.....] - ETA: 12s - loss: 0.0176(459, 160, 320, 3)\n",
      "17630/20000 [=========================>....] - ETA: 10s - loss: 0.0177(474, 160, 320, 3)\n",
      "18103/20000 [==========================>...] - ETA: 8s - loss: 0.0178 (467, 160, 320, 3)\n",
      "18568/20000 [==========================>...] - ETA: 6s - loss: 0.0177(449, 160, 320, 3)\n",
      "19017/20000 [===========================>..] - ETA: 4s - loss: 0.0177(460, 160, 320, 3)\n",
      "19479/20000 [============================>.] - ETA: 2s - loss: 0.0178(471, 160, 320, 3)\n",
      "19930/20000 [============================>.] - ETA: 0s - loss: 0.0176(467, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00019: saving model to Nvidias-check-19-0.0113.hdf5\n",
      "20387/20000 [==============================] - 94s - loss: 0.0176 - val_loss: 0.0113\n",
      "Epoch 21/25\n",
      "(458, 160, 320, 3)\n",
      "  463/20000 [..............................] - ETA: 102s - loss: 0.0232(461, 160, 320, 3)\n",
      "  912/20000 [>.............................] - ETA: 87s - loss: 0.0198 (457, 160, 320, 3)\n",
      " 1380/20000 [=>............................] - ETA: 89s - loss: 0.0193(453, 160, 320, 3)\n",
      " 1839/20000 [=>............................] - ETA: 83s - loss: 0.0181(462, 160, 320, 3)\n",
      " 2313/20000 [==>...........................] - ETA: 84s - loss: 0.0188(460, 160, 320, 3)\n",
      " 2780/20000 [===>..........................] - ETA: 79s - loss: 0.0186(461, 160, 320, 3)\n",
      " 3229/20000 [===>..........................] - ETA: 79s - loss: 0.0179(470, 160, 320, 3)\n",
      " 3689/20000 [====>.........................] - ETA: 75s - loss: 0.0173(476, 160, 320, 3)\n",
      " 4160/20000 [=====>........................] - ETA: 71s - loss: 0.0175(443, 160, 320, 3)\n",
      " 4627/20000 [=====>........................] - ETA: 71s - loss: 0.0180(97, 160, 320, 3)\n",
      " 5085/20000 [======>.......................] - ETA: 67s - loss: 0.0181(456, 160, 320, 3)\n",
      " 5546/20000 [=======>......................] - ETA: 66s - loss: 0.0181(471, 160, 320, 3)\n",
      " 6003/20000 [========>.....................] - ETA: 63s - loss: 0.0178(463, 160, 320, 3)\n",
      " 6456/20000 [========>.....................] - ETA: 61s - loss: 0.0179(464, 160, 320, 3)\n",
      " 6918/20000 [=========>....................] - ETA: 59s - loss: 0.0187(459, 160, 320, 3)\n",
      " 7378/20000 [==========>...................] - ETA: 56s - loss: 0.0188(465, 160, 320, 3)\n",
      " 7839/20000 [==========>...................] - ETA: 55s - loss: 0.0187(457, 160, 320, 3)\n",
      " 8309/20000 [===========>..................] - ETA: 52s - loss: 0.0189(445, 160, 320, 3)\n",
      " 8785/20000 [============>.................] - ETA: 51s - loss: 0.0186(460, 160, 320, 3)\n",
      " 9325/20000 [============>.................] - ETA: 48s - loss: 0.0186(465, 160, 320, 3)\n",
      "(464, 160, 320, 3)\n",
      " 9781/20000 [=============>................] - ETA: 46s - loss: 0.0187(455, 160, 320, 3)\n",
      "10252/20000 [==============>...............] - ETA: 44s - loss: 0.0185(462, 160, 320, 3)\n",
      "10715/20000 [===============>..............] - ETA: 42s - loss: 0.0183(458, 160, 320, 3)\n",
      "11179/20000 [===============>..............] - ETA: 40s - loss: 0.0183(456, 160, 320, 3)\n",
      "11638/20000 [================>.............] - ETA: 38s - loss: 0.0182(461, 160, 320, 3)\n",
      "12103/20000 [=================>............] - ETA: 35s - loss: 0.0181(474, 160, 320, 3)\n",
      "12560/20000 [=================>............] - ETA: 33s - loss: 0.0181(457, 160, 320, 3)\n",
      "13005/20000 [==================>...........] - ETA: 31s - loss: 0.0181(459, 160, 320, 3)\n",
      "13465/20000 [===================>..........] - ETA: 29s - loss: 0.0181(464, 160, 320, 3)\n",
      "13930/20000 [===================>..........] - ETA: 27s - loss: 0.0179(454, 160, 320, 3)\n",
      "14394/20000 [====================>.........] - ETA: 25s - loss: 0.0179(451, 160, 320, 3)\n",
      "14849/20000 [=====================>........] - ETA: 23s - loss: 0.0179(453, 160, 320, 3)\n",
      "15311/20000 [=====================>........] - ETA: 21s - loss: 0.0179(458, 160, 320, 3)\n",
      "15769/20000 [======================>.......] - ETA: 19s - loss: 0.0179(458, 160, 320, 3)\n",
      "16225/20000 [=======================>......] - ETA: 17s - loss: 0.0179(462, 160, 320, 3)\n",
      "16686/20000 [========================>.....] - ETA: 15s - loss: 0.0179(454, 160, 320, 3)\n",
      "17160/20000 [========================>.....] - ETA: 13s - loss: 0.0179(459, 160, 320, 3)\n",
      "17617/20000 [=========================>....] - ETA: 10s - loss: 0.0178(462, 160, 320, 3)\n",
      "18076/20000 [==========================>...] - ETA: 8s - loss: 0.0177 (470, 160, 320, 3)\n",
      "18540/20000 [==========================>...] - ETA: 6s - loss: 0.0177(457, 160, 320, 3)\n",
      "18994/20000 [===========================>..] - ETA: 4s - loss: 0.0177(462, 160, 320, 3)\n",
      "19445/20000 [============================>.] - ETA: 2s - loss: 0.0176(459, 160, 320, 3)\n",
      "19898/20000 [============================>.] - ETA: 0s - loss: 0.0177(464, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00020: saving model to Nvidias-check-20-0.0111.hdf5\n",
      "20356/20000 [==============================] - 99s - loss: 0.0177 - val_loss: 0.0111\n",
      "Epoch 22/25\n",
      "(461, 160, 320, 3)\n",
      "  458/20000 [..............................] - ETA: 74s - loss: 0.0206(452, 160, 320, 3)\n",
      "  920/20000 [>.............................] - ETA: 73s - loss: 0.0187(466, 160, 320, 3)\n",
      " 1374/20000 [=>............................] - ETA: 81s - loss: 0.0187(450, 160, 320, 3)\n",
      " 1833/20000 [=>............................] - ETA: 77s - loss: 0.0189(465, 160, 320, 3)\n",
      " 2295/20000 [==>...........................] - ETA: 79s - loss: 0.0177(458, 160, 320, 3)\n",
      " 2765/20000 [===>..........................] - ETA: 75s - loss: 0.0175(452, 160, 320, 3)\n",
      " 3222/20000 [===>..........................] - ETA: 75s - loss: 0.0185(472, 160, 320, 3)\n",
      " 3684/20000 [====>.........................] - ETA: 72s - loss: 0.0181(454, 160, 320, 3)\n",
      " 4143/20000 [=====>........................] - ETA: 69s - loss: 0.0178(449, 160, 320, 3)\n",
      " 4607/20000 [=====>........................] - ETA: 68s - loss: 0.0177(456, 160, 320, 3)\n",
      " 5068/20000 [======>.......................] - ETA: 65s - loss: 0.0180(458, 160, 320, 3)\n",
      " 5520/20000 [=======>......................] - ETA: 65s - loss: 0.0181(460, 160, 320, 3)\n",
      " 5986/20000 [=======>......................] - ETA: 62s - loss: 0.0177(461, 160, 320, 3)\n",
      " 6436/20000 [========>.....................] - ETA: 61s - loss: 0.0175(454, 160, 320, 3)\n",
      " 6901/20000 [=========>....................] - ETA: 58s - loss: 0.0177(453, 160, 320, 3)\n",
      " 7359/20000 [==========>...................] - ETA: 56s - loss: 0.0178(102, 160, 320, 3)\n",
      " 7811/20000 [==========>...................] - ETA: 54s - loss: 0.0180(464, 160, 320, 3)\n",
      " 8283/20000 [===========>..................] - ETA: 52s - loss: 0.0179(456, 160, 320, 3)\n",
      " 8737/20000 [============>.................] - ETA: 50s - loss: 0.0178(462, 160, 320, 3)\n",
      " 9186/20000 [============>.................] - ETA: 48s - loss: 0.0179(456, 160, 320, 3)\n",
      " 9642/20000 [=============>................] - ETA: 46s - loss: 0.0182(474, 160, 320, 3)\n",
      "10100/20000 [==============>...............] - ETA: 44s - loss: 0.0182(464, 160, 320, 3)\n",
      "10560/20000 [==============>...............] - ETA: 42s - loss: 0.0181(464, 160, 320, 3)\n",
      "11021/20000 [===============>..............] - ETA: 40s - loss: 0.0183(458, 160, 320, 3)\n",
      "11475/20000 [================>.............] - ETA: 38s - loss: 0.0181(460, 160, 320, 3)\n",
      "12030/20000 [=================>............] - ETA: 36s - loss: 0.0182(461, 160, 320, 3)\n",
      "(456, 160, 320, 3)\n",
      "12494/20000 [=================>............] - ETA: 34s - loss: 0.0182(451, 160, 320, 3)\n",
      "12950/20000 [==================>...........] - ETA: 31s - loss: 0.0181(456, 160, 320, 3)\n",
      "13412/20000 [===================>..........] - ETA: 29s - loss: 0.0181(463, 160, 320, 3)\n",
      "13868/20000 [===================>..........] - ETA: 27s - loss: 0.0181(462, 160, 320, 3)\n",
      "14342/20000 [====================>.........] - ETA: 25s - loss: 0.0180(453, 160, 320, 3)\n",
      "14806/20000 [=====================>........] - ETA: 23s - loss: 0.0179(456, 160, 320, 3)\n",
      "15270/20000 [=====================>........] - ETA: 21s - loss: 0.0178(452, 160, 320, 3)\n",
      "15728/20000 [======================>.......] - ETA: 19s - loss: 0.0178(456, 160, 320, 3)\n",
      "16188/20000 [=======================>......] - ETA: 17s - loss: 0.0178(462, 160, 320, 3)\n",
      "16649/20000 [=======================>......] - ETA: 15s - loss: 0.0178(461, 160, 320, 3)\n",
      "17105/20000 [========================>.....] - ETA: 13s - loss: 0.0179(465, 160, 320, 3)\n",
      "17556/20000 [=========================>....] - ETA: 10s - loss: 0.0179(459, 160, 320, 3)\n",
      "18012/20000 [==========================>...] - ETA: 8s - loss: 0.0178 (452, 160, 320, 3)\n",
      "18475/20000 [==========================>...] - ETA: 6s - loss: 0.0178(470, 160, 320, 3)\n",
      "18937/20000 [===========================>..] - ETA: 4s - loss: 0.0178(454, 160, 320, 3)\n",
      "19390/20000 [============================>.] - ETA: 2s - loss: 0.0178(449, 160, 320, 3)\n",
      "19846/20000 [============================>.] - ETA: 0s - loss: 0.0178(464, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00021: saving model to Nvidias-check-21-0.0110.hdf5\n",
      "20298/20000 [==============================] - 95s - loss: 0.0177 - val_loss: 0.0110\n",
      "Epoch 23/25\n",
      "(457, 160, 320, 3)\n",
      "  456/20000 [..............................] - ETA: 74s - loss: 0.0126(460, 160, 320, 3)\n",
      "  918/20000 [>.............................] - ETA: 75s - loss: 0.0133(467, 160, 320, 3)\n",
      " 1379/20000 [=>............................] - ETA: 81s - loss: 0.0148(456, 160, 320, 3)\n",
      " 1844/20000 [=>............................] - ETA: 76s - loss: 0.0148(459, 160, 320, 3)\n",
      " 2303/20000 [==>...........................] - ETA: 78s - loss: 0.0172(458, 160, 320, 3)\n",
      " 2755/20000 [===>..........................] - ETA: 75s - loss: 0.0173(460, 160, 320, 3)\n",
      " 3225/20000 [===>..........................] - ETA: 71s - loss: 0.0176(473, 160, 320, 3)\n",
      " 3679/20000 [====>.........................] - ETA: 70s - loss: 0.0174(455, 160, 320, 3)\n",
      " 4128/20000 [=====>........................] - ETA: 69s - loss: 0.0177(453, 160, 320, 3)\n",
      " 4592/20000 [=====>........................] - ETA: 66s - loss: 0.0177(474, 160, 320, 3)\n",
      " 5049/20000 [======>.......................] - ETA: 65s - loss: 0.0173(464, 160, 320, 3)\n",
      " 5509/20000 [=======>......................] - ETA: 63s - loss: 0.0173(455, 160, 320, 3)\n",
      " 5976/20000 [=======>......................] - ETA: 62s - loss: 0.0176(462, 160, 320, 3)\n",
      " 6432/20000 [========>.....................] - ETA: 59s - loss: 0.0177(453, 160, 320, 3)\n",
      " 6891/20000 [=========>....................] - ETA: 57s - loss: 0.0175(466, 160, 320, 3)\n",
      " 7349/20000 [==========>...................] - ETA: 56s - loss: 0.0173(456, 160, 320, 3)\n",
      " 7809/20000 [==========>...................] - ETA: 53s - loss: 0.0176(473, 160, 320, 3)\n",
      " 8282/20000 [===========>..................] - ETA: 51s - loss: 0.0175(460, 160, 320, 3)\n",
      " 8737/20000 [============>.................] - ETA: 49s - loss: 0.0173(460, 160, 320, 3)\n",
      " 9190/20000 [============>.................] - ETA: 47s - loss: 0.0172(461, 160, 320, 3)\n",
      " 9664/20000 [=============>................] - ETA: 44s - loss: 0.0173(454, 160, 320, 3)\n",
      "10128/20000 [==============>...............] - ETA: 42s - loss: 0.0174(102, 160, 320, 3)\n",
      "10583/20000 [==============>...............] - ETA: 40s - loss: 0.0174(451, 160, 320, 3)\n",
      "11045/20000 [===============>..............] - ETA: 38s - loss: 0.0174(475, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11498/20000 [================>.............] - ETA: 36s - loss: 0.0173(466, 160, 320, 3)\n",
      "11964/20000 [================>.............] - ETA: 34s - loss: 0.0175(465, 160, 320, 3)\n",
      "12420/20000 [=================>............] - ETA: 32s - loss: 0.0180(463, 160, 320, 3)\n",
      "12893/20000 [==================>...........] - ETA: 30s - loss: 0.0181(456, 160, 320, 3)\n",
      "13353/20000 [===================>..........] - ETA: 28s - loss: 0.0180(474, 160, 320, 3)\n",
      "13813/20000 [===================>..........] - ETA: 26s - loss: 0.0182(458, 160, 320, 3)\n",
      "14274/20000 [====================>.........] - ETA: 24s - loss: 0.0181(458, 160, 320, 3)\n",
      "14830/20000 [=====================>........] - ETA: 21s - loss: 0.0181(461, 160, 320, 3)\n",
      "(455, 160, 320, 3)\n",
      "15281/20000 [=====================>........] - ETA: 20s - loss: 0.0181(462, 160, 320, 3)\n",
      "15756/20000 [======================>.......] - ETA: 18s - loss: 0.0181(463, 160, 320, 3)\n",
      "16222/20000 [=======================>......] - ETA: 16s - loss: 0.0180(462, 160, 320, 3)\n",
      "16687/20000 [========================>.....] - ETA: 14s - loss: 0.0180(460, 160, 320, 3)\n",
      "17150/20000 [========================>.....] - ETA: 12s - loss: 0.0179(461, 160, 320, 3)\n",
      "17606/20000 [=========================>....] - ETA: 10s - loss: 0.0178(451, 160, 320, 3)\n",
      "18080/20000 [==========================>...] - ETA: 8s - loss: 0.0178 (466, 160, 320, 3)\n",
      "18538/20000 [==========================>...] - ETA: 6s - loss: 0.0178(465, 160, 320, 3)\n",
      "18996/20000 [===========================>..] - ETA: 4s - loss: 0.0178(465, 160, 320, 3)\n",
      "19457/20000 [============================>.] - ETA: 2s - loss: 0.0178(465, 160, 320, 3)\n",
      "19912/20000 [============================>.] - ETA: 0s - loss: 0.0178(456, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00022: saving model to Nvidias-check-22-0.0102.hdf5\n",
      "20374/20000 [==============================] - 92s - loss: 0.0178 - val_loss: 0.0102\n",
      "Epoch 24/25\n",
      "(446, 160, 320, 3)\n",
      "  463/20000 [..............................] - ETA: 75s - loss: 0.0178(456, 160, 320, 3)\n",
      "  925/20000 [>.............................] - ETA: 88s - loss: 0.0176(456, 160, 320, 3)\n",
      " 1385/20000 [=>............................] - ETA: 81s - loss: 0.0177(459, 160, 320, 3)\n",
      " 1846/20000 [=>............................] - ETA: 77s - loss: 0.0167(454, 160, 320, 3)\n",
      " 2297/20000 [==>...........................] - ETA: 79s - loss: 0.0168(461, 160, 320, 3)\n",
      " 2763/20000 [===>..........................] - ETA: 78s - loss: 0.0166(472, 160, 320, 3)\n",
      " 3228/20000 [===>..........................] - ETA: 75s - loss: 0.0165(468, 160, 320, 3)\n",
      " 3693/20000 [====>.........................] - ETA: 75s - loss: 0.0164(458, 160, 320, 3)\n",
      " 4158/20000 [=====>........................] - ETA: 71s - loss: 0.0168(464, 160, 320, 3)\n",
      " 4614/20000 [=====>........................] - ETA: 70s - loss: 0.0164(472, 160, 320, 3)\n",
      " 5060/20000 [======>.......................] - ETA: 67s - loss: 0.0169(454, 160, 320, 3)\n",
      " 5516/20000 [=======>......................] - ETA: 66s - loss: 0.0170(451, 160, 320, 3)\n",
      " 5972/20000 [=======>......................] - ETA: 63s - loss: 0.0172(454, 160, 320, 3)\n",
      " 6431/20000 [========>.....................] - ETA: 61s - loss: 0.0171(467, 160, 320, 3)\n",
      " 6885/20000 [=========>....................] - ETA: 59s - loss: 0.0173(461, 160, 320, 3)\n",
      " 7346/20000 [==========>...................] - ETA: 56s - loss: 0.0174(458, 160, 320, 3)\n",
      " 7818/20000 [==========>...................] - ETA: 55s - loss: 0.0171(464, 160, 320, 3)\n",
      " 8286/20000 [===========>..................] - ETA: 52s - loss: 0.0171(460, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 51s - loss: 0.0173(466, 160, 320, 3)\n",
      " 9208/20000 [============>.................] - ETA: 48s - loss: 0.0172(463, 160, 320, 3)\n",
      " 9680/20000 [=============>................] - ETA: 46s - loss: 0.0172(474, 160, 320, 3)\n",
      "10134/20000 [==============>...............] - ETA: 44s - loss: 0.0171(461, 160, 320, 3)\n",
      "10585/20000 [==============>...............] - ETA: 42s - loss: 0.0174(455, 160, 320, 3)\n",
      "11039/20000 [===============>..............] - ETA: 40s - loss: 0.0173(466, 160, 320, 3)\n",
      "11506/20000 [================>.............] - ETA: 38s - loss: 0.0173(464, 160, 320, 3)\n",
      "11967/20000 [================>.............] - ETA: 35s - loss: 0.0171(460, 160, 320, 3)\n",
      "12425/20000 [=================>............] - ETA: 33s - loss: 0.0172(453, 160, 320, 3)\n",
      "12889/20000 [==================>...........] - ETA: 31s - loss: 0.0173(99, 160, 320, 3)\n",
      "13349/20000 [===================>..........] - ETA: 29s - loss: 0.0174(459, 160, 320, 3)\n",
      "13815/20000 [===================>..........] - ETA: 27s - loss: 0.0174(460, 160, 320, 3)\n",
      "14278/20000 [====================>.........] - ETA: 25s - loss: 0.0174(462, 160, 320, 3)\n",
      "14752/20000 [=====================>........] - ETA: 23s - loss: 0.0175(466, 160, 320, 3)\n",
      "15213/20000 [=====================>........] - ETA: 21s - loss: 0.0178(453, 160, 320, 3)\n",
      "15668/20000 [======================>.......] - ETA: 19s - loss: 0.0178(454, 160, 320, 3)\n",
      "16134/20000 [=======================>......] - ETA: 17s - loss: 0.0177(460, 160, 320, 3)\n",
      "16598/20000 [=======================>......] - ETA: 14s - loss: 0.0179(459, 160, 320, 3)\n",
      "17058/20000 [========================>.....] - ETA: 12s - loss: 0.0178(460, 160, 320, 3)\n",
      "17610/20000 [=========================>....] - ETA: 10s - loss: 0.0178(466, 160, 320, 3)\n",
      "(460, 160, 320, 3)\n",
      "18069/20000 [==========================>...] - ETA: 8s - loss: 0.0179 (460, 160, 320, 3)\n",
      "18529/20000 [==========================>...] - ETA: 6s - loss: 0.0178(456, 160, 320, 3)\n",
      "18991/20000 [===========================>..] - ETA: 4s - loss: 0.0178(454, 160, 320, 3)\n",
      "19457/20000 [============================>.] - ETA: 2s - loss: 0.0178(468, 160, 320, 3)\n",
      "19910/20000 [============================>.] - ETA: 0s - loss: 0.0178(461, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00023: saving model to Nvidias-check-23-0.0101.hdf5\n",
      "20364/20000 [==============================] - 95s - loss: 0.0177 - val_loss: 0.0101\n",
      "Epoch 25/25\n",
      "(467, 160, 320, 3)\n",
      "  460/20000 [..............................] - ETA: 80s - loss: 0.0181(455, 160, 320, 3)\n",
      "  919/20000 [>.............................] - ETA: 79s - loss: 0.0198(460, 160, 320, 3)\n",
      " 1379/20000 [=>............................] - ETA: 77s - loss: 0.0194(457, 160, 320, 3)\n",
      " 1845/20000 [=>............................] - ETA: 74s - loss: 0.0188(445, 160, 320, 3)\n",
      " 2305/20000 [==>...........................] - ETA: 72s - loss: 0.0191(461, 160, 320, 3)\n",
      " 2765/20000 [===>..........................] - ETA: 69s - loss: 0.0193(468, 160, 320, 3)\n",
      " 3221/20000 [===>..........................] - ETA: 67s - loss: 0.0188(449, 160, 320, 3)\n",
      " 3675/20000 [====>.........................] - ETA: 65s - loss: 0.0187(457, 160, 320, 3)\n",
      " 4143/20000 [=====>........................] - ETA: 65s - loss: 0.0187(470, 160, 320, 3)\n",
      " 4604/20000 [=====>........................] - ETA: 63s - loss: 0.0183(463, 160, 320, 3)\n",
      " 5071/20000 [======>.......................] - ETA: 61s - loss: 0.0185(464, 160, 320, 3)\n",
      " 5526/20000 [=======>......................] - ETA: 59s - loss: 0.0182(459, 160, 320, 3)\n",
      " 5986/20000 [=======>......................] - ETA: 58s - loss: 0.0179(460, 160, 320, 3)\n",
      " 6443/20000 [========>.....................] - ETA: 56s - loss: 0.0177(465, 160, 320, 3)\n",
      " 6888/20000 [=========>....................] - ETA: 55s - loss: 0.0176(452, 160, 320, 3)\n",
      " 7349/20000 [==========>...................] - ETA: 53s - loss: 0.0174(460, 160, 320, 3)\n",
      " 7817/20000 [==========>...................] - ETA: 52s - loss: 0.0177(456, 160, 320, 3)\n",
      " 8266/20000 [===========>..................] - ETA: 50s - loss: 0.0178(465, 160, 320, 3)\n",
      " 8723/20000 [============>.................] - ETA: 48s - loss: 0.0178(459, 160, 320, 3)\n",
      " 9193/20000 [============>.................] - ETA: 46s - loss: 0.0177(464, 160, 320, 3)\n",
      " 9656/20000 [=============>................] - ETA: 44s - loss: 0.0176(466, 160, 320, 3)\n",
      "10120/20000 [==============>...............] - ETA: 41s - loss: 0.0178(462, 160, 320, 3)\n",
      "10579/20000 [==============>...............] - ETA: 39s - loss: 0.0176(467, 160, 320, 3)\n",
      "11039/20000 [===============>..............] - ETA: 37s - loss: 0.0176(457, 160, 320, 3)\n",
      "11504/20000 [================>.............] - ETA: 35s - loss: 0.0178(467, 160, 320, 3)\n",
      "11956/20000 [================>.............] - ETA: 33s - loss: 0.0178(466, 160, 320, 3)\n",
      "12416/20000 [=================>............] - ETA: 31s - loss: 0.0177(457, 160, 320, 3)\n",
      "12872/20000 [==================>...........] - ETA: 30s - loss: 0.0176(454, 160, 320, 3)\n",
      "13337/20000 [===================>..........] - ETA: 28s - loss: 0.0179(461, 160, 320, 3)\n",
      "13796/20000 [===================>..........] - ETA: 26s - loss: 0.0178(467, 160, 320, 3)\n",
      "14260/20000 [====================>.........] - ETA: 24s - loss: 0.0177(462, 160, 320, 3)\n",
      "14726/20000 [=====================>........] - ETA: 22s - loss: 0.0176(467, 160, 320, 3)\n",
      "15188/20000 [=====================>........] - ETA: 20s - loss: 0.0176(461, 160, 320, 3)\n",
      "15655/20000 [======================>.......] - ETA: 18s - loss: 0.0178(99, 160, 320, 3)\n",
      "16112/20000 [=======================>......] - ETA: 16s - loss: 0.0177(468, 160, 320, 3)\n",
      "16579/20000 [=======================>......] - ETA: 14s - loss: 0.0177(462, 160, 320, 3)\n",
      "17045/20000 [========================>.....] - ETA: 12s - loss: 0.0176(460, 160, 320, 3)\n",
      "17502/20000 [=========================>....] - ETA: 10s - loss: 0.0177(465, 160, 320, 3)\n",
      "17956/20000 [=========================>....] - ETA: 8s - loss: 0.0179 (466, 160, 320, 3)\n",
      "18417/20000 [==========================>...] - ETA: 6s - loss: 0.0180(470, 160, 320, 3)\n",
      "18884/20000 [===========================>..] - ETA: 4s - loss: 0.0179(469, 160, 320, 3)\n",
      "19346/20000 [============================>.] - ETA: 2s - loss: 0.0180(457, 160, 320, 3)\n",
      "19813/20000 [============================>.] - ETA: 0s - loss: 0.0179(457, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(72, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00024: saving model to Nvidias-check-24-0.0108.hdf5\n",
      "20274/20000 [==============================] - 89s - loss: 0.0180 - val_loss: 0.0108\n"
     ]
    }
   ],
   "source": [
    "history_object = model.fit_generator(train_generator, samples_per_epoch= samples_per_epoch,\n",
    "                                     validation_data=validation_generator,\n",
    "                                     nb_val_samples=nb_val_samples, nb_epoch=nb_epoch, verbose=1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-5e5236275ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_object' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history_object.history.keys())\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# go one level up to save final model\n",
    "model_json = model.to_json()\n",
    "with open(\"model_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "model.save(\"model_final.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added more data to training using simulator as car goes outside track after crossing bridge. Also added logic to add equal data of images having steering angle less than .2 and images having steering angle more than .2 so that car is able to run properly  in sharp turning roads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File(\"model_final.h5\", mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x7f5212d6d780>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model(\"model_final.h5\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f51cc0e1080>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51cc175fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import as a dataframe and plot steering\n",
    "df2 = pd.read_csv('/media/ashutosh/unix-extra1/udacity/udacitycCarND/Behavioral Cloning/linux_sim/IMG/driving_log.csv', header=0)\n",
    "df2.columns = [\"center_image\", \"left_image\", \"right_image\", \"steering\", \"throttle\", \"break\", \"speed\"]\n",
    "df2.drop(['throttle', 'break', 'speed'], axis = 1, inplace = True)\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.distplot(df2['steering'], kde = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with train generator shared in the class and add image augmentations\n",
    "def train_generator2(samples, batch_size=batch_size):\n",
    "    num_samples = len(samples)\n",
    "    print(num_samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        from sklearn.utils import shuffle\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            straight_count=0\n",
    "            images = []\n",
    "            angles = []\n",
    "            #print(batch_samples[0])\n",
    "            # Read center, left and right images from a folder containing Udacity data and my data\n",
    "            for sample_index,batch_sample in enumerate(batch_samples):\n",
    "                cwd = os.getcwd()\n",
    "                #print(cwd)\n",
    "                #print(batch_sample)\n",
    "                \n",
    "                #print(batch_sample)\n",
    "                center_angle = float(batch_sample[3])                \n",
    "                # Limit angles of less than absolute value of .1 to no more than 1/2 of data\n",
    "                # to reduce bias of car driving straight\n",
    "                if abs(center_angle) < .4:\n",
    "                    straight_count += 1\n",
    "                if straight_count > (batch_size * .3):\n",
    "                    while abs(samples[sample_index][3]) < .4:\n",
    "                        sample_index = random.randrange(len(samples))\n",
    "                    \n",
    "                    batch_sample=samples[sample_index]\n",
    "                    \n",
    "                center_name = batch_sample[0].strip()#.split('/')[-1]\n",
    "                if Path(center_name).exists():\n",
    "                    center_image = cv2.imread(center_name)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(center_name))\n",
    "                    continue\n",
    "                #print(center_image.shape)\n",
    "                center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                left_name = batch_sample[1].strip()#.split('/')[-1]\n",
    "                if Path(left_name).exists():\n",
    "                    left_image = cv2.imread(left_name)\n",
    "                    left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(left_name))\n",
    "                    continue\n",
    "                #left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                right_name = batch_sample[2].strip()#.split('/')[-1]\n",
    "                if Path(right_name).exists():\n",
    "                    right_image = cv2.imread(right_name)\n",
    "                    right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB)\n",
    "                else:\n",
    "                    print(\"not Found:-\"+str(batch_sample))\n",
    "                    continue\n",
    "                #right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "                \n",
    "                # Apply correction for left and right steering\n",
    "                correction = 0.20\n",
    "                left_angle = center_angle + correction\n",
    "                right_angle = center_angle - correction\n",
    "\n",
    "                # Randomly include either center, left or right image\n",
    "                num = random.random()\n",
    "                #print(num)\n",
    "                if num <= 0.33:\n",
    "                    select_image = center_image\n",
    "                    select_angle = center_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                elif num>0.33 and num<=0.66:\n",
    "                    select_image = left_image\n",
    "                    select_angle = left_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                    #print(select_image)\n",
    "                else:\n",
    "                    select_image = right_image\n",
    "                    select_angle = right_angle\n",
    "                    images.append(select_image)\n",
    "                    angles.append(select_angle)\n",
    "                    #print(select_image)\n",
    "\n",
    "                # Randomly horizontally flip selected images with 80% probability\n",
    "                flip_image = np.fliplr(select_image)\n",
    "                flip_angle = -1*select_angle\n",
    "                images.append(flip_image)\n",
    "                angles.append(flip_angle)\n",
    "\n",
    "                # Augment with images of different brightness\n",
    "                # Randomly select a percent change\n",
    "                change_pct = random.uniform(0.4, 1.2)\n",
    "\n",
    "                # Change to HSV to change the brightness V\n",
    "                hsv = cv2.cvtColor(select_image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "                hsv[:, :, 2] = hsv[:, :, 2] * change_pct\n",
    "                # Convert back to RGB and append\n",
    "                bright_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "                images.append(bright_img)\n",
    "                angles.append(select_angle)\n",
    "\n",
    "                ## Randomly shear image with 80% probability\n",
    "                shear_range = 40\n",
    "                rows, cols, ch = select_image.shape\n",
    "                dx = np.random.randint(-shear_range, shear_range + 1)\n",
    "                #    print('dx',dx)\n",
    "                random_point = [cols / 2 + dx, rows / 2]\n",
    "                pts1 = np.float32([[0, rows], [cols, rows], [cols / 2, rows / 2]])\n",
    "                pts2 = np.float32([[0, rows], [cols, rows], random_point])\n",
    "                dsteering = dx / (rows / 2) * 360 / (2 * np.pi * 25.0) / 10.0\n",
    "                M = cv2.getAffineTransform(pts1, pts2)\n",
    "                shear_image = cv2.warpAffine(center_image, M, (cols, rows), borderMode=1)\n",
    "                shear_angle = select_angle + dsteering\n",
    "                images.append(shear_image)\n",
    "                angles.append(shear_angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            print(X_train.shape)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "def valid_generator2(samples, batch_size=batch_size):\n",
    "        num_samples = len(samples)\n",
    "        cwd = os.getcwd()\n",
    "        print(cwd)\n",
    "        while 1:  # Loop forever so the generator never terminates\n",
    "            from sklearn.utils import shuffle\n",
    "            shuffle(samples)\n",
    "            for offset in range(0, num_samples, batch_size):\n",
    "                batch_samples = samples[offset:offset + batch_size]\n",
    "\n",
    "                images = []\n",
    "                angles = []\n",
    "                straight_count=0\n",
    "                #Validation generator only has center images and no augmentations\n",
    "                for sample_index,batch_sample in enumerate(batch_samples):\n",
    "                    \n",
    "                    center_angle = float(batch_sample[3])\n",
    "                    # Limit angles of less than absolute value of .1 to no more than 1/2 of data\n",
    "                    # to reduce bias of car driving straight\n",
    "                    if abs(center_angle) < .4:\n",
    "                        straight_count += 1\n",
    "                    if straight_count > (batch_size * .5):\n",
    "                        while abs(batch_samples[sample_index][3]) < .4:\n",
    "                            sample_index = random.randrange(len(batch_samples))\n",
    "                        batch_sample=batch_samples[sample_index]  \n",
    "                        \n",
    "                    #print(batch_sample)\n",
    "                    center_name =  batch_sample[0].strip()#.split('/')[-1]\n",
    "                    if Path(center_name).exists():\n",
    "                        center_image = cv2.imread(center_name)\n",
    "                    else:\n",
    "                        print(\"not Found:-\"+str(center_name))\n",
    "                        continue\n",
    "                    \n",
    "                    center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                      \n",
    "                            \n",
    "                    images.append(center_image)\n",
    "                    angles.append(center_angle)\n",
    "\n",
    "                X_train = np.array(images)\n",
    "                y_train = np.array(angles)\n",
    "                print(X_train.shape)\n",
    "                yield shuffle(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_epoch = 20\n",
    "samples_per_epoch = 20000\n",
    "nb_val_samples = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftoList2=df2.values.tolist()\n",
    "train_samples2, validation_samples2 = train_test_split(dftoList2, test_size=0.20)\n",
    "training_generator2 = train_generator2(train_samples2, batch_size=batch_size)\n",
    "validation_generator2 = valid_generator2(validation_samples2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 94s - loss: 0.0940(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 80s - loss: 0.0997(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 75s - loss: 0.1029(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 72s - loss: 0.0984(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 69s - loss: 0.0984(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 66s - loss: 0.0989(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 64s - loss: 0.0984(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 61s - loss: 0.0996(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 59s - loss: 0.0994(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 56s - loss: 0.0981(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 54s - loss: 0.0984(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 52s - loss: 0.0952(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 50s - loss: 0.0966(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 47s - loss: 0.0969(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 45s - loss: 0.0976(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 43s - loss: 0.0973(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 41s - loss: 0.0971(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 39s - loss: 0.0965(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 37s - loss: 0.0957(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 35s - loss: 0.0947(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 33s - loss: 0.0943(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 32s - loss: 0.0935(40, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 30s - loss: 0.0940(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 28s - loss: 0.0941(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 26s - loss: 0.0924(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 24s - loss: 0.0918(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 22s - loss: 0.0943(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 20s - loss: 0.0944(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 18s - loss: 0.0937(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 16s - loss: 0.0943(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 15s - loss: 0.0941(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 12s - loss: 0.0959(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 11s - loss: 0.0954(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 9s - loss: 0.0958 (512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 7s - loss: 0.0964(512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 5s - loss: 0.0969(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 3s - loss: 0.0969(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 1s - loss: 0.0966(512, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashutosh/unix-extra1/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00000: saving model to Nvidias-check-00-0.0841.hdf5\n",
      "20008/20000 [==============================] - 77s - loss: 0.0959 - val_loss: 0.0841\n",
      "Epoch 2/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 73s - loss: 0.1332(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 70s - loss: 0.1140(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 67s - loss: 0.1093(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 64s - loss: 0.1096(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 62s - loss: 0.1050(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 60s - loss: 0.1031(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 59s - loss: 0.1029(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 57s - loss: 0.1012(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 56s - loss: 0.1026(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 54s - loss: 0.1024(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 52s - loss: 0.1007(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 50s - loss: 0.1007(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 48s - loss: 0.0978(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 46s - loss: 0.0994(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 44s - loss: 0.0994(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 42s - loss: 0.0995(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 40s - loss: 0.0988(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 39s - loss: 0.0985(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 37s - loss: 0.0977(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 35s - loss: 0.0971(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 33s - loss: 0.0964(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 31s - loss: 0.0958(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 29s - loss: 0.0954(40, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 27s - loss: 0.0958(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 25s - loss: 0.0959(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 24s - loss: 0.0940(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 22s - loss: 0.0932(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 20s - loss: 0.0953(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 18s - loss: 0.0952(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 16s - loss: 0.0944(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 15s - loss: 0.0951(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 13s - loss: 0.0948(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 11s - loss: 0.0964(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 9s - loss: 0.0960 (512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 7s - loss: 0.0963(512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 5s - loss: 0.0968(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 3s - loss: 0.0972(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 1s - loss: 0.0973(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00001: saving model to Nvidias-check-01-0.0918.hdf5\n",
      "20008/20000 [==============================] - 79s - loss: 0.0970 - val_loss: 0.0918\n",
      "Epoch 3/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 68s - loss: 0.0694(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 79s - loss: 0.1022(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 73s - loss: 0.0990(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 74s - loss: 0.0988(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 71s - loss: 0.1011(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 67s - loss: 0.0975(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 64s - loss: 0.0973(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 62s - loss: 0.0980(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 61s - loss: 0.0965(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 58s - loss: 0.0978(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 57s - loss: 0.0986(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 55s - loss: 0.0979(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 52s - loss: 0.0979(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 51s - loss: 0.0949(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 49s - loss: 0.0960(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 46s - loss: 0.0964(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 45s - loss: 0.0972(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 43s - loss: 0.0969(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 41s - loss: 0.0972(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 39s - loss: 0.0967(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 37s - loss: 0.0958(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 35s - loss: 0.0954(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 33s - loss: 0.0948(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 30s - loss: 0.0944(40, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 28s - loss: 0.0947(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 26s - loss: 0.0949(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 25s - loss: 0.0932(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 23s - loss: 0.0925(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0944(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 18s - loss: 0.0944(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 16s - loss: 0.0936(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 14s - loss: 0.0942(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 12s - loss: 0.0939(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0956(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0952 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0955(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0961(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0965(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00002: saving model to Nvidias-check-02-0.0910.hdf5\n",
      "20008/20000 [==============================] - 87s - loss: 0.0967 - val_loss: 0.0910\n",
      "Epoch 4/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 94s - loss: 0.0870(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 79s - loss: 0.0785(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 80s - loss: 0.0969(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 75s - loss: 0.0958(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 70s - loss: 0.0968(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 67s - loss: 0.0980(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 66s - loss: 0.0953(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 64s - loss: 0.0956(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 64s - loss: 0.0962(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 61s - loss: 0.0961(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 60s - loss: 0.0971(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 57s - loss: 0.0969(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 56s - loss: 0.0963(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 53s - loss: 0.0965(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 51s - loss: 0.0937(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 49s - loss: 0.0948(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 46s - loss: 0.0950(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 44s - loss: 0.0954(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 42s - loss: 0.0954(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 40s - loss: 0.0958(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 37s - loss: 0.0950(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 35s - loss: 0.0943(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 33s - loss: 0.0937(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 31s - loss: 0.0933(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 29s - loss: 0.0929(40, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 27s - loss: 0.0936(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 25s - loss: 0.0937(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 23s - loss: 0.0920(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0915(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 19s - loss: 0.0935(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 17s - loss: 0.0936(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 14s - loss: 0.0930(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 12s - loss: 0.0936(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0933(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0950(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0947(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0950(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0956(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00003: saving model to Nvidias-check-03-0.0895.hdf5\n",
      "20008/20000 [==============================] - 87s - loss: 0.0960 - val_loss: 0.0895\n",
      "Epoch 5/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 73s - loss: 0.0989(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 71s - loss: 0.0920(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 69s - loss: 0.0838(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 67s - loss: 0.0989(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 65s - loss: 0.0975(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 62s - loss: 0.0977(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 60s - loss: 0.0989(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 60s - loss: 0.0968(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 58s - loss: 0.0965(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 56s - loss: 0.0968(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 54s - loss: 0.0962(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 53s - loss: 0.0969(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 51s - loss: 0.0969(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 49s - loss: 0.0964(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 46s - loss: 0.0967(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 44s - loss: 0.0941(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 43s - loss: 0.0954(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 41s - loss: 0.0958(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 40s - loss: 0.0961(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 37s - loss: 0.0960(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 36s - loss: 0.0962(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 34s - loss: 0.0956(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 32s - loss: 0.0950(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 30s - loss: 0.0946(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 28s - loss: 0.0942(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 26s - loss: 0.0939(40, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 24s - loss: 0.0940(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 22s - loss: 0.0939(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 20s - loss: 0.0923(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 18s - loss: 0.0918(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 16s - loss: 0.0938(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 14s - loss: 0.0936(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 12s - loss: 0.0929(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0934(512, 160, 320, 3)\n",
      "17920/20000 [=========================>....] - ETA: 8s - loss: 0.0932 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0948(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0945(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0947(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00004: saving model to Nvidias-check-04-0.0826.hdf5\n",
      "20008/20000 [==============================] - 86s - loss: 0.0953 - val_loss: 0.0826\n",
      "Epoch 6/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 96s - loss: 0.1113(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 79s - loss: 0.1037(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 78s - loss: 0.0991(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 75s - loss: 0.0916(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 70s - loss: 0.1004(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 67s - loss: 0.0992(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 67s - loss: 0.0996(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 64s - loss: 0.0998(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 63s - loss: 0.0974(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 60s - loss: 0.0969(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 59s - loss: 0.0969(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 56s - loss: 0.0966(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 55s - loss: 0.0975(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 53s - loss: 0.0973(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 50s - loss: 0.0969(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 47s - loss: 0.0969(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 46s - loss: 0.0946(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 44s - loss: 0.0955(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 42s - loss: 0.0956(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 40s - loss: 0.0960(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 38s - loss: 0.0956(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 35s - loss: 0.0959(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 33s - loss: 0.0952(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 31s - loss: 0.0947(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 29s - loss: 0.0942(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 27s - loss: 0.0937(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 25s - loss: 0.0935(40, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 23s - loss: 0.0940(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0939(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 19s - loss: 0.0924(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 17s - loss: 0.0918(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 14s - loss: 0.0936(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 12s - loss: 0.0935(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0930(512, 160, 320, 3)\n",
      "17920/20000 [=========================>....] - ETA: 8s - loss: 0.0935 (512, 160, 320, 3)\n",
      "18432/20000 [==========================>...] - ETA: 6s - loss: 0.0932(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0947(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0944(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00005: saving model to Nvidias-check-05-0.0893.hdf5\n",
      "20008/20000 [==============================] - 89s - loss: 0.0946 - val_loss: 0.0893\n",
      "Epoch 7/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 69s - loss: 0.1162(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 79s - loss: 0.1130(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 73s - loss: 0.1095(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 69s - loss: 0.1042(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 71s - loss: 0.0979(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 67s - loss: 0.1037(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 67s - loss: 0.1016(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 64s - loss: 0.1016(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 64s - loss: 0.1019(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 60s - loss: 0.1000(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 58s - loss: 0.0997(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 57s - loss: 0.0997(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 55s - loss: 0.0987(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 53s - loss: 0.0998(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 50s - loss: 0.0997(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 49s - loss: 0.0989(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 46s - loss: 0.0987(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 45s - loss: 0.0963(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 42s - loss: 0.0972(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 41s - loss: 0.0972(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 38s - loss: 0.0976(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 36s - loss: 0.0974(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 34s - loss: 0.0973(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 32s - loss: 0.0966(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 30s - loss: 0.0959(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 27s - loss: 0.0955(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 25s - loss: 0.0949(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 23s - loss: 0.0945(40, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0947(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 19s - loss: 0.0946(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 17s - loss: 0.0931(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 15s - loss: 0.0927(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 12s - loss: 0.0944(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0942(512, 160, 320, 3)\n",
      "17920/20000 [=========================>....] - ETA: 8s - loss: 0.0936 (512, 160, 320, 3)\n",
      "18432/20000 [==========================>...] - ETA: 6s - loss: 0.0941(512, 160, 320, 3)\n",
      "18944/20000 [===========================>..] - ETA: 4s - loss: 0.0939(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0952(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00006: saving model to Nvidias-check-06-0.0891.hdf5\n",
      "20008/20000 [==============================] - 89s - loss: 0.0947 - val_loss: 0.0891\n",
      "Epoch 8/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 85s - loss: 0.1085(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 75s - loss: 0.1138(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 80s - loss: 0.1146(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 74s - loss: 0.1107(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 75s - loss: 0.1050(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 70s - loss: 0.0993(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 69s - loss: 0.1036(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 66s - loss: 0.1023(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 66s - loss: 0.1016(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 62s - loss: 0.1017(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 59s - loss: 0.0999(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 58s - loss: 0.0993(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 55s - loss: 0.0996(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 53s - loss: 0.0988(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 51s - loss: 0.0995(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 49s - loss: 0.0996(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 47s - loss: 0.0987(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 45s - loss: 0.0987(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 43s - loss: 0.0967(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 41s - loss: 0.0974(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 38s - loss: 0.0973(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 36s - loss: 0.0978(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 34s - loss: 0.0975(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 32s - loss: 0.0974(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 29s - loss: 0.0969(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 27s - loss: 0.0964(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 25s - loss: 0.0957(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 23s - loss: 0.0951(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0946(40, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 19s - loss: 0.0948(512, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 17s - loss: 0.0949(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 15s - loss: 0.0934(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 13s - loss: 0.0928(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0946(512, 160, 320, 3)\n",
      "17920/20000 [=========================>....] - ETA: 8s - loss: 0.0945 (512, 160, 320, 3)\n",
      "18432/20000 [==========================>...] - ETA: 6s - loss: 0.0938(512, 160, 320, 3)\n",
      "18944/20000 [===========================>..] - ETA: 4s - loss: 0.0943(512, 160, 320, 3)\n",
      "19456/20000 [============================>.] - ETA: 2s - loss: 0.0940(512, 160, 320, 3)\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0954(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00007: saving model to Nvidias-check-07-0.0940.hdf5\n",
      "20008/20000 [==============================] - 89s - loss: 0.0952 - val_loss: 0.0940\n",
      "Epoch 9/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 96s - loss: 0.0816(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 93s - loss: 0.0947(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 82s - loss: 0.1014(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 82s - loss: 0.1049(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 76s - loss: 0.1038(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 72s - loss: 0.0994(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 71s - loss: 0.0951(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 67s - loss: 0.1003(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 66s - loss: 0.0991(512, 160, 320, 3)\n",
      " 5120/20000 [======>.......................] - ETA: 63s - loss: 0.0992(512, 160, 320, 3)\n",
      " 5632/20000 [=======>......................] - ETA: 61s - loss: 0.0998(512, 160, 320, 3)\n",
      " 6144/20000 [========>.....................] - ETA: 58s - loss: 0.0987(512, 160, 320, 3)\n",
      " 6656/20000 [========>.....................] - ETA: 57s - loss: 0.0986(512, 160, 320, 3)\n",
      " 7168/20000 [=========>....................] - ETA: 54s - loss: 0.0989(512, 160, 320, 3)\n",
      " 7680/20000 [==========>...................] - ETA: 52s - loss: 0.0985(512, 160, 320, 3)\n",
      " 8192/20000 [===========>..................] - ETA: 50s - loss: 0.0996(512, 160, 320, 3)\n",
      " 8704/20000 [============>.................] - ETA: 48s - loss: 0.0994(512, 160, 320, 3)\n",
      " 9216/20000 [============>.................] - ETA: 45s - loss: 0.0989(512, 160, 320, 3)\n",
      " 9728/20000 [=============>................] - ETA: 43s - loss: 0.0988(512, 160, 320, 3)\n",
      "10240/20000 [==============>...............] - ETA: 41s - loss: 0.0968(512, 160, 320, 3)\n",
      "10752/20000 [===============>..............] - ETA: 39s - loss: 0.0974(512, 160, 320, 3)\n",
      "11264/20000 [===============>..............] - ETA: 37s - loss: 0.0972(512, 160, 320, 3)\n",
      "11776/20000 [================>.............] - ETA: 35s - loss: 0.0975(512, 160, 320, 3)\n",
      "12288/20000 [=================>............] - ETA: 32s - loss: 0.0973(512, 160, 320, 3)\n",
      "12800/20000 [==================>...........] - ETA: 30s - loss: 0.0973(512, 160, 320, 3)\n",
      "13312/20000 [==================>...........] - ETA: 28s - loss: 0.0967(512, 160, 320, 3)\n",
      "13824/20000 [===================>..........] - ETA: 26s - loss: 0.0960(512, 160, 320, 3)\n",
      "14336/20000 [====================>.........] - ETA: 24s - loss: 0.0953(512, 160, 320, 3)\n",
      "14848/20000 [=====================>........] - ETA: 21s - loss: 0.0947(512, 160, 320, 3)\n",
      "15360/20000 [======================>.......] - ETA: 19s - loss: 0.0942(40, 160, 320, 3)\n",
      "15872/20000 [======================>.......] - ETA: 17s - loss: 0.0944(512, 160, 320, 3)\n",
      "16384/20000 [=======================>......] - ETA: 15s - loss: 0.0945(512, 160, 320, 3)\n",
      "16896/20000 [========================>.....] - ETA: 13s - loss: 0.0932(512, 160, 320, 3)\n",
      "17408/20000 [=========================>....] - ETA: 10s - loss: 0.0925(512, 160, 320, 3)\n",
      "17920/20000 [=========================>....] - ETA: 8s - loss: 0.0940 (512, 160, 320, 3)\n",
      "18432/20000 [==========================>...] - ETA: 6s - loss: 0.0939(512, 160, 320, 3)\n",
      "18944/20000 [===========================>..] - ETA: 4s - loss: 0.0933(512, 160, 320, 3)\n",
      "19456/20000 [============================>.] - ETA: 2s - loss: 0.0937(512, 160, 320, 3)\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.0936(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00008: saving model to Nvidias-check-08-0.0841.hdf5\n",
      "20480/20000 [==============================] - 91s - loss: 0.0950 - val_loss: 0.0841\n",
      "Epoch 10/20\n",
      "   40/20000 [..............................] - ETA: 287s - loss: 0.0100(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      "  552/20000 [..............................] - ETA: 94s - loss: 0.0781 (512, 160, 320, 3)\n",
      " 1064/20000 [>.............................] - ETA: 94s - loss: 0.0922(512, 160, 320, 3)\n",
      " 1576/20000 [=>............................] - ETA: 83s - loss: 0.1004(512, 160, 320, 3)\n",
      " 2088/20000 [==>...........................] - ETA: 79s - loss: 0.1041(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 76s - loss: 0.1029(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 72s - loss: 0.1006(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 71s - loss: 0.0962(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 68s - loss: 0.1009(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 67s - loss: 0.0999(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 63s - loss: 0.1002(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 62s - loss: 0.1004(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 59s - loss: 0.0986(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 57s - loss: 0.0982(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 55s - loss: 0.0981(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 53s - loss: 0.0975(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 50s - loss: 0.0983(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 49s - loss: 0.0987(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 46s - loss: 0.0979(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 43s - loss: 0.0981(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0960(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 39s - loss: 0.0965(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 37s - loss: 0.0964(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 34s - loss: 0.0967(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 33s - loss: 0.0964(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0966(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0962(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 26s - loss: 0.0957(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0948(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0943(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0938(40, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0939(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0938(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0924(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0919(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0935 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0934(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0929(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0933(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00009: saving model to Nvidias-check-09-0.0900.hdf5\n",
      "20008/20000 [==============================] - 90s - loss: 0.0931 - val_loss: 0.0900\n",
      "Epoch 11/20\n",
      "(512, 160, 320, 3)\n",
      "  552/20000 [..............................] - ETA: 70s - loss: 0.1434(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 1064/20000 [>.............................] - ETA: 82s - loss: 0.1121(512, 160, 320, 3)\n",
      " 1576/20000 [=>............................] - ETA: 76s - loss: 0.1089(512, 160, 320, 3)\n",
      " 2088/20000 [==>...........................] - ETA: 72s - loss: 0.1097(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 73s - loss: 0.1090(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 73s - loss: 0.1071(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 69s - loss: 0.1038(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 68s - loss: 0.0999(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 64s - loss: 0.1033(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 63s - loss: 0.1021(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 60s - loss: 0.1020(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 57s - loss: 0.1018(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 55s - loss: 0.1001(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 53s - loss: 0.0998(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 50s - loss: 0.0996(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 49s - loss: 0.0992(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 47s - loss: 0.0998(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0997(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 42s - loss: 0.0993(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 40s - loss: 0.0992(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 38s - loss: 0.0973(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 36s - loss: 0.0979(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 34s - loss: 0.0977(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0979(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0976(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0977(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 25s - loss: 0.0973(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0967(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0960(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0955(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0952(40, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 14s - loss: 0.0953(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 12s - loss: 0.0953(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0939(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0933 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0948(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0947(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0940(512, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00010: saving model to Nvidias-check-10-0.0857.hdf5\n",
      "20008/20000 [==============================] - 88s - loss: 0.0944 - val_loss: 0.0857\n",
      "Epoch 12/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 72s - loss: 0.0817(512, 160, 320, 3)\n",
      " 1064/20000 [>.............................] - ETA: 73s - loss: 0.1117(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 1576/20000 [=>............................] - ETA: 73s - loss: 0.1014(512, 160, 320, 3)\n",
      " 2088/20000 [==>...........................] - ETA: 72s - loss: 0.1031(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 71s - loss: 0.1066(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 68s - loss: 0.1077(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 65s - loss: 0.1059(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 64s - loss: 0.1035(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 62s - loss: 0.0992(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 61s - loss: 0.1022(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 58s - loss: 0.1010(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 56s - loss: 0.1008(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 55s - loss: 0.1013(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 52s - loss: 0.0997(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 49s - loss: 0.0994(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 49s - loss: 0.0997(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 46s - loss: 0.0993(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0998(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 42s - loss: 0.0995(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0989(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 38s - loss: 0.0989(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 36s - loss: 0.0969(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 34s - loss: 0.0972(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0969(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0972(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 27s - loss: 0.0970(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 25s - loss: 0.0968(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0964(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0959(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0952(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0948(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0944(40, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 12s - loss: 0.0943(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0944(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0932 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0926(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0943(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0942(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00011: saving model to Nvidias-check-11-0.0934.hdf5\n",
      "20008/20000 [==============================] - 90s - loss: 0.0936 - val_loss: 0.0934\n",
      "Epoch 13/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 96s - loss: 0.1087(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 81s - loss: 0.0939(512, 160, 320, 3)\n",
      " 1576/20000 [=>............................] - ETA: 83s - loss: 0.1121(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 2088/20000 [==>...........................] - ETA: 77s - loss: 0.1039(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 77s - loss: 0.1052(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 72s - loss: 0.1062(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 70s - loss: 0.1071(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 68s - loss: 0.1061(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 67s - loss: 0.1032(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 63s - loss: 0.1002(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 62s - loss: 0.1028(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 59s - loss: 0.1019(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 56s - loss: 0.1017(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 55s - loss: 0.1014(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 52s - loss: 0.0998(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 50s - loss: 0.0991(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 48s - loss: 0.0992(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0989(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 43s - loss: 0.0996(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0996(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 39s - loss: 0.0991(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 36s - loss: 0.0990(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 34s - loss: 0.0972(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0978(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0976(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0978(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 26s - loss: 0.0977(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0976(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0969(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0962(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0956(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0950(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0947(40, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0950(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0951 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0938(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0933(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0948(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00012: saving model to Nvidias-check-12-0.0839.hdf5\n",
      "20008/20000 [==============================] - 90s - loss: 0.0947 - val_loss: 0.0839\n",
      "Epoch 14/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 75s - loss: 0.0702(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 71s - loss: 0.0897(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 69s - loss: 0.0871(512, 160, 320, 3)\n",
      " 2088/20000 [==>...........................] - ETA: 67s - loss: 0.1028(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 66s - loss: 0.0985(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 65s - loss: 0.0999(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 63s - loss: 0.1021(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 61s - loss: 0.1033(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 59s - loss: 0.1029(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 57s - loss: 0.1012(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 54s - loss: 0.0984(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 53s - loss: 0.1011(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 51s - loss: 0.1007(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 49s - loss: 0.1008(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 47s - loss: 0.1008(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 45s - loss: 0.0994(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 44s - loss: 0.0991(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 41s - loss: 0.0995(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 40s - loss: 0.0990(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 38s - loss: 0.0994(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 36s - loss: 0.0994(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 34s - loss: 0.0986(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 32s - loss: 0.0984(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 30s - loss: 0.0967(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 28s - loss: 0.0972(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 26s - loss: 0.0970(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 24s - loss: 0.0973(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 22s - loss: 0.0972(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 20s - loss: 0.0972(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 18s - loss: 0.0966(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 16s - loss: 0.0961(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 14s - loss: 0.0955(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 12s - loss: 0.0949(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0946(40, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0946 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0946(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0934(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0928(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00013: saving model to Nvidias-check-13-0.0868.hdf5\n",
      "20008/20000 [==============================] - 89s - loss: 0.0942 - val_loss: 0.0868\n",
      "Epoch 15/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 98s - loss: 0.0871(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 95s - loss: 0.0763(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 84s - loss: 0.0885(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 84s - loss: 0.0872(512, 160, 320, 3)\n",
      " 2600/20000 [==>...........................] - ETA: 79s - loss: 0.0986(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 78s - loss: 0.0952(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 74s - loss: 0.0965(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 71s - loss: 0.0986(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 68s - loss: 0.1003(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 65s - loss: 0.0997(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 62s - loss: 0.0982(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 59s - loss: 0.0958(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 56s - loss: 0.0980(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 54s - loss: 0.0980(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 51s - loss: 0.0982(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 49s - loss: 0.0987(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 46s - loss: 0.0978(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 44s - loss: 0.0973(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 42s - loss: 0.0976(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 39s - loss: 0.0974(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 37s - loss: 0.0979(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 35s - loss: 0.0976(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 33s - loss: 0.0970(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 31s - loss: 0.0969(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 29s - loss: 0.0953(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 27s - loss: 0.0956(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 25s - loss: 0.0954(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0957(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 20s - loss: 0.0956(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 18s - loss: 0.0957(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 16s - loss: 0.0951(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 14s - loss: 0.0946(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 12s - loss: 0.0940(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0936(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0932 (40, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0932(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0931(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0920(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00014: saving model to Nvidias-check-14-0.0816.hdf5\n",
      "20008/20000 [==============================] - 89s - loss: 0.0915 - val_loss: 0.0816\n",
      "Epoch 16/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 69s - loss: 0.1520(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 76s - loss: 0.1188(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 75s - loss: 0.1020(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 77s - loss: 0.1038(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 72s - loss: 0.0992(512, 160, 320, 3)\n",
      " 3112/20000 [===>..........................] - ETA: 72s - loss: 0.1064(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 69s - loss: 0.1028(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 68s - loss: 0.1022(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 65s - loss: 0.1035(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 64s - loss: 0.1041(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 61s - loss: 0.1031(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 58s - loss: 0.1016(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 57s - loss: 0.0996(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 54s - loss: 0.1016(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 52s - loss: 0.1007(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 50s - loss: 0.1010(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 48s - loss: 0.1013(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0997(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 43s - loss: 0.0992(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0993(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 39s - loss: 0.0986(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 37s - loss: 0.0989(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 35s - loss: 0.0988(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0983(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0982(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0966(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 26s - loss: 0.0970(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 24s - loss: 0.0969(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0970(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0969(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0969(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0963(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0958(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0953(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0946 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0943(40, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0945(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0944(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00015: saving model to Nvidias-check-15-0.0931.hdf5\n",
      "20008/20000 [==============================] - 90s - loss: 0.0934 - val_loss: 0.0931\n",
      "Epoch 17/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 98s - loss: 0.0742(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 81s - loss: 0.1129(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 75s - loss: 0.1041(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 76s - loss: 0.0967(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 72s - loss: 0.1001(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 72s - loss: 0.0969(512, 160, 320, 3)\n",
      " 3624/20000 [====>.........................] - ETA: 68s - loss: 0.1048(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 68s - loss: 0.1017(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 67s - loss: 0.1018(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 63s - loss: 0.1025(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 62s - loss: 0.1034(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 59s - loss: 0.1031(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 57s - loss: 0.1012(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 54s - loss: 0.0986(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 53s - loss: 0.1005(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 50s - loss: 0.1001(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 48s - loss: 0.0999(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 46s - loss: 0.0999(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 44s - loss: 0.0987(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0985(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 39s - loss: 0.0985(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 37s - loss: 0.0981(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 35s - loss: 0.0984(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 33s - loss: 0.0982(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0975(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0975(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 26s - loss: 0.0958(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 24s - loss: 0.0965(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0965(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0968(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0966(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0967(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0961(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 11s - loss: 0.0956(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0951 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0944(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0941(40, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0942(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00016: saving model to Nvidias-check-16-0.0884.hdf5\n",
      "20008/20000 [==============================] - 91s - loss: 0.0943 - val_loss: 0.0884\n",
      "Epoch 18/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 71s - loss: 0.0472(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 69s - loss: 0.0607(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 76s - loss: 0.0895(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 71s - loss: 0.0884(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 73s - loss: 0.0844(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 69s - loss: 0.0883(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 69s - loss: 0.0868(512, 160, 320, 3)\n",
      " 4136/20000 [=====>........................] - ETA: 65s - loss: 0.0943(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 65s - loss: 0.0931(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 62s - loss: 0.0945(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 61s - loss: 0.0964(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 58s - loss: 0.0977(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 57s - loss: 0.0979(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 54s - loss: 0.0966(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 52s - loss: 0.0947(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 49s - loss: 0.0971(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 48s - loss: 0.0963(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0965(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 43s - loss: 0.0967(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0959(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 39s - loss: 0.0957(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 37s - loss: 0.0959(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 35s - loss: 0.0958(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0964(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0964(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 28s - loss: 0.0959(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 26s - loss: 0.0960(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0947(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0951(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0950(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0953(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0952(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0953(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0949(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0945 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0941(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0936(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0934(40, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00017: saving model to Nvidias-check-17-0.0911.hdf5\n",
      "20008/20000 [==============================] - 91s - loss: 0.0935 - val_loss: 0.0911\n",
      "Epoch 19/20\n",
      "(512, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 70s - loss: 0.0951(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 81s - loss: 0.0723(512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 74s - loss: 0.0744(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 76s - loss: 0.0927(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 71s - loss: 0.0924(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 71s - loss: 0.0879(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 67s - loss: 0.0909(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 67s - loss: 0.0889(512, 160, 320, 3)\n",
      " 4648/20000 [=====>........................] - ETA: 63s - loss: 0.0950(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 63s - loss: 0.0929(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 60s - loss: 0.0937(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 57s - loss: 0.0954(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 55s - loss: 0.0965(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 53s - loss: 0.0968(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 51s - loss: 0.0963(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 49s - loss: 0.0947(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 47s - loss: 0.0968(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 45s - loss: 0.0959(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 43s - loss: 0.0962(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 41s - loss: 0.0966(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 38s - loss: 0.0957(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 36s - loss: 0.0952(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 34s - loss: 0.0956(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 32s - loss: 0.0952(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 30s - loss: 0.0959(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 27s - loss: 0.0956(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 25s - loss: 0.0952(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 23s - loss: 0.0951(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 21s - loss: 0.0937(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 19s - loss: 0.0940(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 17s - loss: 0.0940(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 15s - loss: 0.0944(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 12s - loss: 0.0941(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 10s - loss: 0.0944(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 8s - loss: 0.0938 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0935(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0931(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0927(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00018: saving model to Nvidias-check-18-0.0901.hdf5\n",
      "20008/20000 [==============================] - 90s - loss: 0.0924 - val_loss: 0.0901\n",
      "Epoch 20/20\n",
      "(40, 160, 320, 3)\n",
      "  512/20000 [..............................] - ETA: 113s - loss: 0.0969(512, 160, 320, 3)\n",
      " 1024/20000 [>.............................] - ETA: 92s - loss: 0.0967 (512, 160, 320, 3)\n",
      " 1536/20000 [=>............................] - ETA: 91s - loss: 0.0808(512, 160, 320, 3)\n",
      " 2048/20000 [==>...........................] - ETA: 84s - loss: 0.0791(512, 160, 320, 3)\n",
      " 2560/20000 [==>...........................] - ETA: 78s - loss: 0.0928(512, 160, 320, 3)\n",
      " 3072/20000 [===>..........................] - ETA: 75s - loss: 0.0922(512, 160, 320, 3)\n",
      " 3584/20000 [====>.........................] - ETA: 75s - loss: 0.0889(512, 160, 320, 3)\n",
      " 4096/20000 [=====>........................] - ETA: 71s - loss: 0.0909(512, 160, 320, 3)\n",
      " 4608/20000 [=====>........................] - ETA: 70s - loss: 0.0900(512, 160, 320, 3)\n",
      " 5160/20000 [======>.......................] - ETA: 66s - loss: 0.0956(512, 160, 320, 3)\n",
      "(512, 160, 320, 3)\n",
      " 5672/20000 [=======>......................] - ETA: 65s - loss: 0.0938(512, 160, 320, 3)\n",
      " 6184/20000 [========>.....................] - ETA: 64s - loss: 0.0950(512, 160, 320, 3)\n",
      " 6696/20000 [=========>....................] - ETA: 62s - loss: 0.0963(512, 160, 320, 3)\n",
      " 7208/20000 [=========>....................] - ETA: 59s - loss: 0.0973(512, 160, 320, 3)\n",
      " 7720/20000 [==========>...................] - ETA: 57s - loss: 0.0971(512, 160, 320, 3)\n",
      " 8232/20000 [===========>..................] - ETA: 54s - loss: 0.0963(512, 160, 320, 3)\n",
      " 8744/20000 [============>.................] - ETA: 52s - loss: 0.0948(512, 160, 320, 3)\n",
      " 9256/20000 [============>.................] - ETA: 49s - loss: 0.0968(512, 160, 320, 3)\n",
      " 9768/20000 [=============>................] - ETA: 47s - loss: 0.0962(512, 160, 320, 3)\n",
      "10280/20000 [==============>...............] - ETA: 44s - loss: 0.0964(512, 160, 320, 3)\n",
      "10792/20000 [===============>..............] - ETA: 42s - loss: 0.0967(512, 160, 320, 3)\n",
      "11304/20000 [===============>..............] - ETA: 39s - loss: 0.0959(512, 160, 320, 3)\n",
      "11816/20000 [================>.............] - ETA: 37s - loss: 0.0956(512, 160, 320, 3)\n",
      "12328/20000 [=================>............] - ETA: 35s - loss: 0.0959(512, 160, 320, 3)\n",
      "12840/20000 [==================>...........] - ETA: 32s - loss: 0.0956(512, 160, 320, 3)\n",
      "13352/20000 [===================>..........] - ETA: 30s - loss: 0.0961(512, 160, 320, 3)\n",
      "13864/20000 [===================>..........] - ETA: 27s - loss: 0.0963(512, 160, 320, 3)\n",
      "14376/20000 [====================>.........] - ETA: 25s - loss: 0.0960(512, 160, 320, 3)\n",
      "14888/20000 [=====================>........] - ETA: 23s - loss: 0.0960(512, 160, 320, 3)\n",
      "15400/20000 [======================>.......] - ETA: 20s - loss: 0.0947(512, 160, 320, 3)\n",
      "15912/20000 [======================>.......] - ETA: 18s - loss: 0.0951(512, 160, 320, 3)\n",
      "16424/20000 [=======================>......] - ETA: 16s - loss: 0.0951(512, 160, 320, 3)\n",
      "16936/20000 [========================>.....] - ETA: 13s - loss: 0.0952(512, 160, 320, 3)\n",
      "17448/20000 [=========================>....] - ETA: 11s - loss: 0.0949(512, 160, 320, 3)\n",
      "17960/20000 [=========================>....] - ETA: 9s - loss: 0.0948 (512, 160, 320, 3)\n",
      "18472/20000 [==========================>...] - ETA: 6s - loss: 0.0945(512, 160, 320, 3)\n",
      "18984/20000 [===========================>..] - ETA: 4s - loss: 0.0941(512, 160, 320, 3)\n",
      "19496/20000 [============================>.] - ETA: 2s - loss: 0.0937(512, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(3, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "(128, 160, 320, 3)\n",
      "Epoch 00019: saving model to Nvidias-check-19-0.0860.hdf5\n",
      "20008/20000 [==============================] - 96s - loss: 0.0934 - val_loss: 0.0860\n"
     ]
    }
   ],
   "source": [
    "history_object2 = model.fit_generator(training_generator2, samples_per_epoch= samples_per_epoch,\n",
    "                                     validation_data=validation_generator2,\n",
    "                                     nb_val_samples=nb_val_samples, nb_epoch=nb_epoch, verbose=1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5218538be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(history_object2.history.keys())\n",
    "plt.plot(history_object2.history['loss'])\n",
    "plt.plot(history_object2.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 162, 322, 3)   0           zeropadding2d_input_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)        (None, 82, 322, 3)    0           zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 64, 64, 3)     0           cropping2d_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 32, 32, 24)    1824        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "relu1 (Activation)               (None, 32, 32, 24)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 31, 31, 24)    0           relu1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 16, 16, 36)    21636       maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu2 (Activation)               (None, 16, 16, 36)    0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 15, 15, 36)    0           relu2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 8, 8, 48)      43248       maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3 (Activation)               (None, 8, 8, 48)      0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 7, 7, 48)      0           relu3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 7, 7, 64)      27712       maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4 (Activation)               (None, 7, 7, 64)      0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 6, 6, 64)      0           relu4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 6, 6, 64)      36928       maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5 (Activation)               (None, 6, 6, 64)      0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 5, 5, 64)      0           relu5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1600)          0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1164)          1863564     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "relu6 (Activation)               (None, 1164)          0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 100)           116500      relu6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu7 (Activation)               (None, 100)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 50)            5050        relu7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu8 (Activation)               (None, 50)            0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 10)            510         relu8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "relu9 (Activation)               (None, 10)            0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             11          relu9[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 2,116,983\n",
      "Trainable params: 2,116,983\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# go one level up to save final model\n",
    "model_json = model.to_json()\n",
    "with open(\"model_final.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "model.save(\"model_final.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### With above approach car is able to complete Track 1 but it was not able to perform well in Track 2 . During data collection we have not collected data from Track2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"IMG.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Results:-\n",
    "    please check final model file- Nvidias-check-24-0.0843.hdf5\n",
    "    video file-> IMG.mp4\n",
    "    youtube video link -> https://www.youtube.com/watch?v=x_zMxxjopok&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
